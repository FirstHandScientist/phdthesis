{ \setbeamercolor{background canvas}{bg=hl_bg}
  \setbeamercolor{normal text}{fg=hl_fg}
  \setbeamercolor{frametitle}{fg=hl_fg}
  \begin{frame}{What Have We been Talking About?}
    \usebeamercolor[fg]{normal text}
    
    \begin{tikzpicture}
      \tikzstyle{cnode} = [thick, ellipse, align=center, draw, inner sep = 6pt]
      \tikzstyle{nnode} = [thick, rectangle, rounded corners = 2pt,minimum size = 0.8cm,draw,inner sep = 6pt]
      \node[nnode] (kl) at (0,0) {KL-divergence};
      \node[nnode, right= 5cm of kl] (freeEnergy) {\begin{tabular}{c}Free energy $F_V(q(\bm{x}_U|\bm{x}_O))$ \\ (Variational/Gibbs)\end{tabular}};
      \node[nnode, below= of freeEnergy] (elbo) {\begin{tabular}{c}Evidence lower\\ bound $F(q(\bm{x}_U|\bm{x}_O))$ (ELBO)\end{tabular}};
      \node[nnode, below= of kl] (q-fun) {\begin{tabular}{c}Q-function\\ in EM\end{tabular}};
      \node[nnode, below right= of q-fun] (llk) {log-likelihood $\log{p(\bm{x}_O;\bm{\theta})}$};
      \draw[black, ->] (kl) --node [text width=5cm, black, midway, above]{\tiny
        \begin{tabular}{c}
          Diff. by a sub-partition func. \\
          $F_V(q(\bm{x}_U|\bm{x}_O)) = \mathrm{KL}(q(\bm{x}_U|\bm{x}_O) || p(\bm{x}_U|\bm{x}_O; \bm{\theta})) $ \\
          $- \log{Z(\bm{x}_O; \bm{\theta})}$
        \end{tabular}}
      (freeEnergy);
      \draw[black, ->] (freeEnergy) --node [text width=3cm, black, midway, left]{\tiny
        \begin{tabular}{c}
          Negative \\
          $F(q(\bm{x}_U|\bm{x}_O)) = -F_V(q(\bm{x}_U|\bm{x}_O))- \underbrace{\log{Z(\bm{\theta})}}_{\text{0 in DAGs}}$
        \end{tabular}}
      (elbo);

      \draw[black, ->] (kl) --node [text width=3cm, black, midway, left]{\tiny
        \begin{tabular}{c}
          $-\mathrm{KL}(q(\bm{x}_U|\bm{x}_O) || p(\bm{x}_U,\bm{x}_O; \bm{\theta})) $
        \end{tabular}}
      (elbo);

      \draw[black, ->] (elbo) --node [text width=3cm, black, midway, above]{\tiny
        \begin{tabular}{c}
          Diff. by an Entropy term\\
          $F(q(\bm{x}_U|\bm{x}_O))= \Qq + H({q(\bm{x}_U|\bm{x}_O)})$
        \end{tabular}}
      (q-fun);

      \draw[black, ->] (elbo) --node [text width=3cm, black, midway, above]{\tiny
        Below
      }
      (llk);
      \draw[black, ->] (q-fun) --node [text width=3cm, black, midway, right]{\tiny
        Below
      }
      (llk);
      
    \end{tikzpicture}
    \only<2->{
      \begin{textblock}{5}(6,6)
        \begin{tikzpicture}[scale=1.2,auto,rotate=-5,transform shape]
          \tikzstyle{cnode} = [fill=blue!20,thick, ellipse, align=center, draw, inner sep = 6pt]
          \node [cnode, text=black] (question) at (0, 0) {\begin{tabular}{c}What if lklh. \\is not tractable?\end{tabular}};
          
        \end{tikzpicture}
      \end{textblock}
    }
    % \let\thefootnote\relax\footnotetext{\tiny
    % \vskip -0.2cm
    % A bit notation abuse, $\bm{x}_U$ corresponds to unobserved variable $\bm{z}$.
    % }
    
  \end{frame}
}

{ \setbeamercolor{background canvas}{bg=hl_bg}
  \setbeamercolor{normal text}{fg=hl_fg}
  \setbeamercolor{frametitle}{fg=hl_fg}
  \begin{frame}
    \usebeamercolor[fg]{normal text}
    \begin{center}
      {
        \begin{tikzpicture}
          \tikzstyle{cnode} = [thick, draw=white, ellipse, inner sep = 2pt,  align=center]
          \tikzstyle{fnode} = [thick, draw=white, ellipse, inner sep = 10pt,  align=center]
          \tikzstyle{rnode} = [thick, rectangle, inner sep = 1.5pt,  align=left]
          \node[rnode] (inf) at (-2, 0) {\large Inference};
          \node[rnode, below = 0.6cm of inf.west, anchor=west] (abp) {$\bullet$ {$\alpha$-BP}};
          \node[rnode, below = 1.2cm of inf.west, anchor=west] (renn) {$\bullet$ RENN};
          \node[cnode, fit=(abp)(inf)(renn)] (infn) {};
          
          \node[rnode, right = 3 of inf] (lern) {\large Learning};
          \node[rnode, below = 0.4 of lern.west, anchor=west] (genmm) {$\bullet$ GenMM};
          \node[rnode, below = 0.8 of lern.west, anchor=west] (genhmm) {{$\bullet$} GenHMM};
          \node[rnode, below = 1.2 of lern.west, anchor=west] (lfree) {\textbf{{$\bullet$} EOTGM}};
          \node[cnode, fit=(lern)(genmm)(genhmm)(lfree)] (learn) {};
          \node[rnode, draw=green, fit=(lfree)] () {};

          \node[fnode, fit=(infn)(lern)] (box) {};

          
          \node[below right = 0.5 and -0.5 of infn] {{Probabilistic} Graphical Model};
          \draw[->,line width=0.2mm] (infn) to[out=15, in=165] (learn);
          \draw[->,line width=0.2mm] (learn) to[out=195, in=-15] (infn);
        \end{tikzpicture}
      }
    \end{center}
    
  \end{frame}
}

\begin{frame}{Where is the learning info. from?}
  \begin{tikzpicture}
    \tikzstyle{enode} = [thick, draw, circle, inner sep = 4pt,  align=center]
    \tikzstyle{elnode} = [thick, draw, ellipse, inner sep = 1pt,  align=center]
    \tikzstyle{nnode} = [thick, rectangle, rounded corners = 2pt,draw,inner sep = 4pt]
    \node[enode] (z) at (-4,0) {$\bm{Z}$};
    \node[enode, right=2cm of z] (x) {$\bm{X}$};
    \node[elnode] (emp) [right= 6.5cm of x] {Empirical \\samples};
    
    \draw[->] (z) -- node[nnode,fill=white] {$\bm{g}$} (x);
    \draw[<->] (x) -- node[draw, rounded corners = 2pt, fill=blue!30, align=center] (trckmetric) {
      \begin{minipage}{0.45\textwidth}
        How different they are
        \begin{itemize}[label=\textbullet]
        \item likelihood
        \item Evidence lower bound, free energy 
        \item KL-divergence, $\alpha$-divergence
        \end{itemize}
      \end{minipage}} (emp);
    \node[fill=white!30, below= of trckmetric, align=center] (ot) {
      \begin{minipage}{0.6\textwidth}
        Optimal transport (OT): moving mass from a dist. to another
        \begin{equation*}
          T(p^{\ast},p)=\min_{ \pi \in \Pi(p^{\ast},p)} \dotp{\underbrace{\pi}_{\text{marginalize to $p^{\ast},p$}}}{\underbrace{\bm{M}}_{\text{\begin{tabular}{c}cost matrix\\ sample difference \end{tabular}}}},
        \end{equation*}
        Key attributes:
        \begin{itemize}[label=\textbullet]
        \item Doesnot require tractible lklh.
        \item Learning gradient info. from sample comparison
        \item High complexity, each evaluation is sovling an optimization problem
        \end{itemize}
      \end{minipage}
    };
    \draw[->] (trckmetric) -- node[black, midway, right] {add-on} (ot);

  \end{tikzpicture}
  \only<2>{
    \begin{textblock}{5}(2,4.3)
      \begin{tikzpicture}
        \node[rounded corners = 4pt, rectangle, inner sep = 4pt, fill=gray!20, align=center, text width=10cm, label=below:{A toy example}] (ot_kl) at (0,0)
        {\includegraphics[width=1\linewidth]{images/ot_kl.pdf}};
      \end{tikzpicture}
    \end{textblock}
  }
\end{frame}


\begin{frame}{EOTGM: EOT based Generative Model}
  \begin{tikzpicture}
    \tikzstyle{enode}=[draw, circle, inner sep = 8pt,  align=center]
    \tikzstyle{elnode}=[thick, draw, rectangle, rounded corners = 2pt, inner sep = 4pt, align=center]
    \tikzstyle{nnode} = [thick,rectangle, rounded corners = 2pt,minimum size = 4.8cm,draw,inner sep = 2pt]
    \node[elnode] (ot) at (-1,0) {OT $\min\;T(p^{\ast},p)$};
    % \node[nnode] at (0,1) {$T(P,Q) = \min_{\Gamma\in\Pp(P,Q) }\EE_{(X,Y)\sim \Gamma}\left[ c(X,Y) \right]$};
    \node[elnode] (wgan) at (6,0) {\begin{tabular}{c}WGAN, Arjovsky, et al\\ {\tiny Wasserstein Generative Adversarial Network} \\ {\tiny An adversarial game} \\ {\tiny \textit{1-Lipschitz function family}}\end{tabular}};
    \node[elnode] (eot) at (3,-2) {\begin{tabular}{c} EOTGM \\ {\tiny NN generator + Sinkhorn solver} \\ {\tiny Applicable to implicit \& explicit models} \end{tabular}};
    \draw[->] (ot) -- node[midway,above] {\tiny Kantorovich duality} (wgan);
    \draw[->] (ot) -- node[midway,right] {\tiny Entropy regularization} (eot);
    \node[text width=4cm] [right=-0.1cm of eot] {\begin{equation*} \min_{p} \underbrace{\min_{\pi\in\Pp(p^{\ast},p) } \dotp{\pi}{\bm{M}} - \la H(\pi)}_{W(p^{\ast}, p)}
      \end{equation*}};   
  \end{tikzpicture}
  Alternatively scale the rows \& columns of matrix $e^{-\bm{M}/\lambda}$ (Sinkhorn \& Knopp) gives 'soft' solution
  \begin{itemize}[label=\textbullet]
  \item joint distribution $\pi^{\ast}$
  \item subgradient $\beta^{\ast}$
  \end{itemize}
  which provides the gradient information for adjusting $\bm{g}$.
  \only<2>{
    \begin{textblock}{5}(2,4.3)
      \begin{tikzpicture}
        
        \begin{scope}[scale=1.6, local bounding box=algo]
          \input{images/eot/alo_flow.tex}

        \end{scope}
        \begin{scope}[on background layer]
          \node[rounded corners = 4pt, rectangle, inner sep = 4pt, fill=gray!20, align=center, text width=10cm, label=below:{}, fit=(algo)] () {};
        \end{scope}
        
      \end{tikzpicture}
    \end{textblock}
  }

  \let\thefootnote\relax\footnotetext{\tiny
    Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport}
\end{frame}

\begin{frame}[label=current]{EOTGM and EOTGAN}
  EOTGM\\
  \begin{tikzpicture}
    \tikzstyle{enode} = [thick, draw, circle, inner sep = 4pt,  align=center]
    \tikzstyle{nnode} = [thick, rectangle, rounded corners = 2pt,draw,inner sep = 4pt]
    \tikzstyle{ecnode} = [thick, draw, ellipse, inner sep = 4pt,  align=center]

    \begin{scope}[scale=0.85, every node/.append style={transform shape}]
      \node[enode] (z) at (-4,0.15) {$\bm{Z}$};
      \node[enode, right=2cm of z] (x) {$\bm{X}$};
      \node[enode, below= of x] (hx) {$\hat{\bm{X}}$};
      \node[ecnode, below= of z] (data) {Dataset};

      \draw[->] (z) -- node[nnode,fill=white] {$\bm{g}$} (x);
      \draw[->] (data) -- (hx);
      % \node[nnode, fit=(x) (hx) ($(hx.south west) + (-0.2, -0.2)$), label={below:{\tiny Input}}, dashed] (inPair) {};

      \draw[green, ->] ($(z.north) + (0,0.6)$) -- node [black, midway, above] {\tiny generated v.s. empirical sample comparison} ($(z.north) + (5,0.6)$);
      \draw[green, <-] ($(z.north) + (0,-3.6)$) -- node [black, midway, below] {\tiny subgradient info.} ($(z.north) + (5,-3.6)$);
    \end{scope}
    \node[nnode, fit=(x) (hx), thin, label={below:{\tiny Input}}, dashed] (inPair) {};  
    \node[nnode, rotate=-90, inner sep = 8pt, fill=gray!15] (rowScale) at (1,-0.6) {\tiny ~~Row Scale~~~~};
    \node[nnode, rotate=-90, inner sep = 8pt, fill=gray!5] (colScale) at (3,-0.6) {\tiny Column Scale};

    \draw[->] (inPair) to (rowScale);
    \only<1>{
      \draw [-{Straight Barb[left]}] ($(rowScale.north) + (0, 0.1) $) to ($(colScale.south) +(0,0.1)$);
      \draw [-{Straight Barb[left]}] (colScale) -- node[black, midway, below] {\tiny iterative} (rowScale) ;
    }
    \only<2>{
      \node[nnode, rotate=-90, inner sep = 8pt, fill=gray!15] (rowScale1) at (5,-0.6) {\tiny ~~Row Scale~~~~};
      \node[nnode, rotate=-90, inner sep = 8pt, fill=gray!5] (colScale1) at (7,-0.6) {\tiny Column Scale};
      
      \foreach \x in {-0.5, -.25, 0, 0.25, 0.5}{
        \draw [->] ($(rowScale.north) + (0,\x)$) -- ($(colScale.south) +(0,\x)$);
      }
      \foreach \x in {-0.5, -.25, 0, 0.25, 0.5}{
        \draw [->] ($(rowScale1.north) + (0,\x)$) -- ($(colScale1.south) +(0,\x)$);
      }
      \draw [dashed] ($(colScale.north) + (0,0)$) -- node[black, fill=white] {......} ($(rowScale1.south) +(0,0)$);
      % under brace
      \draw [thick, decoration={ brace, mirror, raise=0.5cm }, decorate ] (rowScale.east) -- (colScale1.east) 
      node [pos=0.5,anchor=north,yshift=-0.55cm] {\tiny Unrolled concatenated feed-forward neural network}; 
    }
  \end{tikzpicture}

  
  EOTGAN: learn implicit distribution with feature mapping\\
  
  \begin{tikzpicture}
    \tikzstyle{ecnode} = [thick, draw, ellipse, inner sep = 4pt,  align=center]
    \tikzstyle{enode} = [thick, draw, circle, inner sep = 4pt,  align=center]
    \tikzstyle{nnode} = [thick, rectangle, rounded corners = 2pt,draw,inner sep = 4pt]

    \begin{scope}[scale=1, every node/.append style={transform shape}]
      \input{sections/eotgan.tex}
    \end{scope}

  \end{tikzpicture}
  
\end{frame}

\graphicspath{{../source/chapter8/}}

\begin{frame}{Numerical}
  \begin{columns}
    \column{0.5\textwidth}
    \vskip 0.5cm
    {\tiny $\bm{\rightarrow}$ Toy distribution learning (target at $4$-mixture Gaussians) using EOTGM. Real samples (red '+') and contour
      (red curve), versus generated samples (blue 'o') and contour (blue curve) by $\bm{g}$.
      \vskip 0.2cm
      $\bm{\downarrow}$
      Comparison of IS and FID (on MNIST) versus mixing ratio r.
    }
    \column{0.4\textwidth}
    
    \begin{figure}
      \centering
      \includegraphics[width=1\linewidth]{images/toy/gauss4/frame8.jpg}
    \end{figure}
  \end{columns}
  \vskip -0.4cm
  \begin{figure}[!ht]
    \captionsetup[subfigure]{justification=centering}

    \begin{subfigure}[b]{0.44\textwidth}
      \centering
      \includegraphics[width=1.1\linewidth]{images/mnist/tra_score/IS_29.pdf}\vspace{-3pt}
      % \caption{}
    \end{subfigure}
    \vspace{20pt}  
    \begin{subfigure}[b]{0.44\textwidth}
      \centering
      \includegraphics[width=1.1\linewidth]{images/mnist/tra_score/FID_29.pdf}\vspace{-3pt}
      % \caption{}
      
    \end{subfigure}
    % \vspace{-15pt}
    \captionsetup{labelformat=empty,justification=centering}
    \caption{Comparison of IS and FID (on MNIST) versus mixing ratio $r$. % (For
      % each model at a certain mixture ratio, $5$ experiments are
      % independently performed. Each solid curve with markers plots the mean of $5$
      % experiments with shaded areas denoting the range of corresponding
      % results.
    }\vspace{-1cm}
    
  \end{figure}

\end{frame}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../ppgm_slide"
%%% End:
