@ARTICLE{2017arXiv170104862A,
   author = {{Arjovsky}, M. and {Bottou}, L.},
    title = "{Towards Principled Methods for Training Generative Adversarial Networks}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1701.04862},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Computer Science - Learning},
     year = 2017,
    month = jan,
   adsurl = {http://adsabs.harvard.edu/abs/{2016arXiv1610065452017arXiv170104862A},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
}
@ARTICLE{2017arXiv170100160G,
   author = {{Goodfellow}, I.},
    title = "{NIPS 2016 Tutorial: Generative Adversarial Networks}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1701.00160},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning},
     year = 2017,
    month = dec,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170100160G},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016arXiv160602206F,
   author = {{Farnia}, F. and {Tse}, D.},
    title = "{A Minimax Approach to Supervised Learning}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1606.02206},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Computer Science - Information Theory, Computer Science - Learning},
     year = 2016,
    month = jun,
   adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160602206F},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@incollection{NIPS2015_5745,
title = {Distributionally Robust Logistic Regression},
author = {Shafieezadeh-Abadeh, Soroosh and Esfahani, Peyman Mohajerin and Kuhn, Daniel},
booktitle = {Advances in Neural Information Processing Systems 28},
editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
pages = {1576--1584},
year = {2015},
publisher = {Curran Associates, Inc.}
}
@incollection{NIPS2014_5458,
title = {Robust Classification Under Sample Selection Bias},
author = {Liu, Anqi and Ziebart, Brian},
booktitle = {Advances in Neural Information Processing Systems 27},
editor = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
pages = {37--45},
year = {2014},
publisher = {Curran Associates, Inc.}
}
@ARTICLE{2017arXiv171010016S,
   author = {{Shafieezadeh-Abadeh}, S. and {Kuhn}, D. and {Mohajerin Esfahani}, P.
	},
    title = "{Regularization via Mass Transportation}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1710.10016},
 primaryClass = "math.OC",
 keywords = {Mathematics - Optimization and Control, Computer Science - Learning, Statistics - Machine Learning},
     year = 2017,
    month = oct,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171010016S},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{2013arXiv1312.6199S,
   author = {{Szegedy}, C. and {Zaremba}, W. and {Sutskever}, I. and {Bruna}, J. and 
	{Erhan}, D. and {Goodfellow}, I. and {Fergus}, R.},
    title = "{Intriguing properties of neural networks}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1312.6199},
 primaryClass = "cs.CV",
 keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
     year = 2013,
    month = dec,
   adsurl = {http://adsabs.harvard.edu/abs/2013arXiv1312.6199S},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{2017arXiv171100970S,
   author = {{Santurkar}, S. and {Schmidt}, L. and {M{\c a}dry}, A.},
    title = "{A Classification-Based Study of Covariate Shift in GAN Distributions}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1711.00970},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
     year = 2017,
    month = nov,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171100970S},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@incollection{NIPS2016_6248,
title = {Wasserstein Training of Restricted Boltzmann Machines},
author = {Montavon, Gr\'{e}goire and M\"{u}ller, Klaus-Robert and Cuturi, Marco},
booktitle = {Advances in Neural Information Processing Systems 29},
editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
pages = {3718--3726},
year = {2016},
publisher = {Curran Associates, Inc.}
}
@Inbook{Hinton2012,
author="Hinton, Geoffrey E.",
editor="Montavon, Gr{\'e}goire
and Orr, Genevi{\`e}ve B.
and M{\"u}ller, Klaus-Robert",
title="A Practical Guide to Training Restricted Boltzmann Machines",
bookTitle="Neural Networks: Tricks of the Trade: Second Edition",
year="2012",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="599--619",
abstract="Restricted Boltzmann machines (RBMs) have been used as generative models of many different types of data. RBMs are usually trained using the contrastive divergence learning procedure. This requires a certain amount of practical experience to decide how to set the values of numerical meta-parameters. Over the last few years, the machine learning group at the University of Toronto has acquired considerable expertise at training RBMs and this guide is an attempt to share this expertise with other machine learning researchers.",
isbn="978-3-642-35289-8",
doi="10.1007/978-3-642-35289-8_32",
url="https://doi.org/10.1007/978-3-642-35289-8_32"
}
@InProceedings{2013arXiv1310.4375C,
  title = 	 {Fast Computation of Wasserstein Barycenters},
  author = 	 {Marco Cuturi and Arnaud Doucet},
  booktitle = 	 {Proceedings of the 31st International Conference on Machine Learning},
  pages = 	 {685--693},
  year = 	 {2014},
  volume = 	 {32},
  number =       {2},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Bejing, China},
  month = 	 {22--24 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v32/cuturi14.pdf},
  url = 	 {http://proceedings.mlr.press/v32/cuturi14.html},
  abstract = 	 {We present new algorithms to compute the mean of a set of N empirical probability measures under the optimal transport metric. This mean, known as the Wasserstein barycenter \citepagueh2011barycenters,rabin2012, is the measure that minimizes the sum of its Wasserstein distances to each element in that set. We argue through a simple example that Wasserstein barycenters have appealing properties that differentiate them from other barycenters proposed recently, which all build on kernel smoothing and/or Bregman divergences. Two original algorithms are proposed that require the repeated computation of primal and dual optimal solutions of transport problems. However direct implementation of these algorithms is too costly as optimal transports are notoriously computationally expensive. Extending the work of \citetcuturi2013sinkhorn, we smooth both the primal and dual of the optimal transport problem to recover fast approximations of the primal and dual optimal solutions. We apply these algorithms to the visualization of perturbed images and to a clustering problem.}
}
@incollection{2013arXiv1306.0895C,
title = {Sinkhorn Distances: Lightspeed Computation of Optimal Transport},
author = {Cuturi, Marco},
booktitle = {Advances in Neural Information Processing Systems 26},
editor = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
pages = {2292--2300},
year = {2013},
publisher = {Curran Associates, Inc.}
}
@ARTICLE{2018arXiv180607066M,
   author = {{Montufar}, G.},
    title = "{Restricted Boltzmann Machines: Introduction and Review}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1806.07066},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning, Computer Science - Information Theory, Mathematics - Probability, Mathematics - Statistics Theory, Statistics - Machine Learning},
     year = 2018,
    month = jun,
   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180607066M},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@inproceedings{2017arXiv170107875A,
  title={Wasserstein generative adversarial networks},
  author={Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
  booktitle={International Conference on Machine Learning},
  pages={214--223},
  year={2017}
}
@INPROCEEDINGS{1467314, 
author={S. Chopra and R. Hadsell and Y. LeCun}, 
booktitle={2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)}, 
title={Learning a similarity metric discriminatively, with application to face verification}, 
year={2005}, 
volume={1}, 
number={}, 
pages={539-546 vol. 1}, 
keywords={face recognition;learning (artificial intelligence);similarity metric learning;face verification;face recognition;L/sub 1/ norm;semantic distance approximation;discriminative loss function;geometric distortion;Character generation;Drives;Robustness;System testing;Spatial databases;Glass;Artificial neural networks;Support vector machines;Support vector machine classification;Face recognition}, 
doi={10.1109/CVPR.2005.202}, 
ISSN={1063-6919}, 
month={June},}
}
@incollection{2017arXiv170400028G,
title = {Improved Training of Wasserstein GANs},
author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and others},
booktitle = {Advances in Neural Information Processing Systems 30},
pages = {5767--5777},
year = {2017},
publisher = {Curran Associates, Inc.}
}
@ARTICLE{2017arXiv171105084C,
   author = {{Cao}, G. and {Yang}, Y. and {Lei}, J. and others},
    title = "{TripletGAN: Training Generative Model with Triplet Loss}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1711.05084},
 keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
     year = 2017,
    month = nov,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171105084C},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@incollection{NIPS2014_5423,
title = {Generative Adversarial Nets},
author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
booktitle = {Advances in Neural Information Processing Systems 27},
pages = {2672--2680},
year = {2014},
publisher = {Curran Associates, Inc.}
}
@book{villani2003topics,
  title={Topics in optimal transportation},
  author={Villani, C{\'e}dric},
  number={58},
  year={2003},
  publisher={American Mathematical Soc.}
}
@INPROCEEDINGS{7298682, 
author={F. Schroff and D. Kalenichenko and J. Philbin}, 
booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={FaceNet: A unified embedding for face recognition and clustering}, 
year={2015}, 
volume={}, 
number={}, 
pages={815-823}, 
keywords={convolution;data mining;face recognition;image matching;neural nets;optimisation;pattern clustering;FaceNet embedding;face recognition;face clustering;deep convolutional network;embedding optimization;face patch matching;online triplet mining method;Face;Face recognition;Training;Accuracy;Artificial neural networks;Standards;Principal component analysis}, 
doi={10.1109/CVPR.2015.7298682}, 
ISSN={1063-6919}, 
month={June},}
@ARTICLE{2018arXiv180607755X,
   author = {{Xu}, Q. and {Huang}, G. and {Yuan}, Y. and {Guo}, C. and {Sun}, Y. and 
	{Wu}, F. and {Weinberger}, K.},
    title = "{An empirical study on evaluation metrics of generative adversarial networks}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1806.07755},
 keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
     year = 2018,
    month = jun,
   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180607755X},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{2016arXiv161006545L,
   author = {{Lopez-Paz}, D. and {Oquab}, M.},
    title = {Revisiting Classifier Two-Sample Tests},
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1610.06545},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning},
     year = 2016,
    month = oct,
   adsurl = {http://adsabs.harvard.edu/abs/2016arXiv161006545L},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{ponce2011computer,
  title={Computer vision: a modern approach},
  author={Ponce, Jean and Forsyth, David and Willow, Equipe-projet and others},
  journal={Computer},
  volume={16},
  number={11},
  year={2011}
}
@incollection{NIPS2016_6125,
title = {Improved Techniques for Training GANs},
author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and others},
booktitle = {Advances in Neural Information Processing Systems 29},
pages = {2234--2242},
year = {2016},
publisher = {Curran Associates, Inc.}
}
@ARTICLE{2018arXiv180101973B,
   author = {{Barratt}, S. and {Sharma}, R.},
    title = "{A Note on the Inception Score}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1801.01973},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
     year = 2018,
    month = jan,
   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180101973B},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@INPROCEEDINGS{1640964, 
author={R. Hadsell and S. Chopra and Y. LeCun}, 
booktitle={2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)}, 
title={Dimensionality Reduction by Learning an Invariant Mapping}, 
year={2006}, 
volume={2}, 
number={}, 
pages={1735-1742}, 
keywords={Extraterrestrial measurements;Image generation;Biology;Geoscience;Astronomy;Service robots;Manufacturing industries;Image analysis;Feature extraction;Data visualization}, 
doi={10.1109/CVPR.2006.100}, 
ISSN={1063-6919}, 
month={June},}

@ARTICLE{2015arXiv151106434R,
   author = {{Radford}, A. and {Metz}, L. and {Chintala}, S.},
    title = "{Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1511.06434},
 keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
     year = 2015,
    month = nov,
   adsurl = {http://adsabs.harvard.edu/abs/2015arXiv151106434R},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{ClaiciCS18,
  author    = {Sebastian Claici and
               Edward Chien and
               Justin Solomon},
  title     = {Stochastic Wasserstein Barycenters},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning,
               {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July
               10-15, 2018},
  pages     = {998--1007},
  year      = {2018},
  timestamp = {Fri, 13 Jul 2018 14:58:25 +0200},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@ARTICLE{2016arXiv161006519S,
   author = {{Schmitzer}, B.},
    title = "{Stabilized Sparse Scaling Algorithms for Entropy Regularized Transport Problems}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1610.06519},
 primaryClass = "math.OC",
 keywords = {Mathematics - Optimization and Control, Computer Science - Computational Engineering, Finance, and Science, Mathematics - Numerical Analysis},
     year = 2016,
    month = oct,
   adsurl = {http://adsabs.harvard.edu/abs/2016arXiv161006519S},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@InProceedings{pmlr-v84-genevay18a,
  title = 	 {Learning Generative Models with Sinkhorn Divergences},
  author = 	 {Aude Genevay and Gabriel Peyre and Marco Cuturi},
  booktitle = 	 {Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1608--1617},
  year = 	 {2018},
  volume = 	 {84},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Playa Blanca, Lanzarote, Canary Islands},
  month = 	 {09--11 Apr},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v84/genevay18a/genevay18a.pdf},
  url = 	 {http://proceedings.mlr.press/v84/genevay18a.html},
  abstract = 	 {The ability to compare two degenerate probability distributions, that is two distributions supported on low-dimensional manifolds in much higher-dimensional spaces, is a crucial factor in the estimation of generative mod- els.It is therefore no surprise that optimal transport (OT) metrics and their ability to handle measures with non-overlapping sup- ports have emerged as a promising tool. Yet, training generative machines using OT raises formidable computational and statistical challenges, because of (i) the computational bur- den of evaluating OT losses, (ii) their instability and lack of smoothness, (iii) the difficulty to estimate them, as well as their gradients, in high dimension. This paper presents the first tractable method to train large scale generative models using an OT-based loss called Sinkhorn loss which tackles these three issues by relying on two key ideas: (a) entropic smoothing, which turns the original OT loss into a differentiable and more robust quantity that can be computed using Sinkhorn fixed point iterations; (b) algorithmic (automatic) differentiation of these iterations with seam- less GPU execution. Additionally, Entropic smoothing generates a family of losses interpolating between Wasserstein (OT) and Energy distance/Maximum Mean Discrepancy (MMD) losses, thus allowing to find a sweet spot leveraging the geometry of OT on the one hand, and the favorable high-dimensional sample complexity of MMD, which comes with un- biased gradient estimates. The resulting computational architecture complements nicely standard deep network generative models by a stack of extra layers implementing the loss function.}
}
@incollection{2017arXiv170608500H,
title = {GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium},
author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and others},
booktitle = {Advances in Neural Information Processing Systems 30},
pages = {6626--6637},
year = {2017},
publisher = {Curran Associates, Inc.}
}
@inproceedings{2018arXiv180205957M,
title={Spectral Normalization for Generative Adversarial Networks},
author={Takeru Miyato and Toshiki Kataoka and Masanori Koyama and Yuichi Yoshida},
booktitle={International Conference on Learning Representations},
year={2018}
}
@ARTICLE{2017arXiv170208398M,
   author = {{Mroueh}, Y. and {Sercu}, T. and {Goel}, V.},
    title = "{McGan: Mean and Covariance Feature Matching GAN}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1702.08398},
 keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
     year = 2017,
    month = feb,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170208398M},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{2017arXiv170502894L,
   author = {{Lim}, J.~H. and {Ye}, J.~C.},
    title = "{Geometric GAN}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1705.02894},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
     year = 2017,
    month = may,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170502894L},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018arXiv180508318Z,
   author = {{Zhang}, H. and {Goodfellow}, I. and {Metaxas}, D. and {Odena}, A.
	},
    title = "{Self-Attention Generative Adversarial Networks}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1805.08318},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
     year = 2018,
    month = may,
   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180508318Z},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@misc{pytorch,
  title = {Pytorch Autograd},
  howpublished = {\url{https://pytorch.org/docs/stable/autograd.html}},
}
@misc{tensorflow,
  title = {Tensorflow Automatic differentiation},
  howpublished = {\url{https://pytorch.org/docs/stable/autograd.html}},
}
@ARTICLE{2018arXiv180300567P,
   author = {{Peyr{\'e}}, G. and {Cuturi}, M.},
    title = "{Computational Optimal Transport}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1803.00567},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning},
     year = 2018,
    month = mar,
   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180300567P},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@INPROCEEDINGS{5459199, 
author={O. Pele and M. Werman}, 
booktitle={2009 IEEE 12th International Conference on Computer Vision}, 
title={Fast and robust Earth Mover's Distances}, 
year={2009}, 
volume={}, 
number={}, 
pages={460-467}, 
keywords={edge detection;thresholded ground distances;flow-network;histograms;outlier noise;quantization effects;robust earth mover's distances;Robustness;Earth;Histograms;Costs;Humans;Image retrieval;Image edge detection;Quantization;Computer vision;Image databases}, 
doi={10.1109/ICCV.2009.5459199}, 
ISSN={2380-7504}, 
month={Sept},}
@ARTICLE{2017arXiv171101558T,
   author = {{Tolstikhin}, I. and {Bousquet}, O. and {Gelly}, S. and {Schoelkopf}, B.
	},
    title = "{Wasserstein Auto-Encoders}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1711.01558},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
     year = 2017,
    month = nov,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171101558T},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{2017arXiv170104722M,
   author = {{Mescheder}, L. and {Nowozin}, S. and {Geiger}, A.},
    title = "{Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1701.04722},
 keywords = {Computer Science - Machine Learning},
     year = 2017,
    month = jan,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170104722M},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{2017arXiv170102386T,
   author = {{Tolstikhin}, I. and {Gelly}, S. and {Bousquet}, O. and {Simon-Gabriel}, C.-J. and 
	{Sch{\"o}lkopf}, B.},
    title = "{AdaGAN: Boosting Generative Models}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1701.02386},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
     year = 2017,
    month = jan,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170102386T},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{2018arXiv180601879W,
   author = {{Weed}, J.},
    title = "{An explicit analysis of the entropic penalty in linear programming}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1806.01879},
 primaryClass = "math.OC",
 keywords = {Mathematics - Optimization and Control, Computer Science - Machine Learning, Statistics - Machine Learning},
     year = 2018,
    month = jun,
   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180601879W},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{2018arXiv180600880K,
   author = {{Khayatkhoei}, M. and {Elgammal}, A. and {Singh}, M.},
    title = "{Disconnected Manifold Learning for Generative Adversarial Networks}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1806.00880},
 keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
     year = 2018,
    month = jun,
   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180600880K},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{2018arXiv181106763L,
   author = {{Liu}, D. and {Th{\`a}nh Vu}, M. and {Chatterjee}, S. and {Rasmussen}, L.~K.
	},
    title = "{Entropy-regularized Optimal Transport Generative Models}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1811.06763},
 keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
     year = 2018,
    month = nov,
   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv181106763L},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{2018arXiv180703039K,
   author = {{Kingma}, D.~P. and {Dhariwal}, P.},
    title = "{Glow: Generative Flow with Invertible 1x1 Convolutions}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1807.03039},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
     year = 2018,
    month = jul,
   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180703039K},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{2016arXiv160508803D,
   author = {{Dinh}, L. and {Sohl-Dickstein}, J. and {Bengio}, S.},
    title = "{Density estimation using Real NVP}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1605.08803},
 keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
     year = 2016,
    month = may,
   adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160508803D},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@incollection{NIPS2016_6399,
title = {InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets},
author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
booktitle = {Advances in Neural Information Processing Systems 29},
editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
pages = {2172--2180},
year = {2016},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/6399-infogan-interpretable-representation-learning-by-information-maximizing-generative-adversarial-nets.pdf}
}
@article{bang2018icml,
  author    = {Duhyeon Bang and
               Hyunjung Shim},
  title     = {Improved Training of Generative Adversarial Networks Using Representative
               Features},
  journal   = {CoRR},
  volume    = {abs/1801.09195},
  year      = {2018},
  archivePrefix = {arXiv},
  eprint    = {1801.09195},
  timestamp = {Mon, 13 Aug 2018 16:47:52 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1801-09195},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@incollection{eitan2018nips_gmm,
title = {On GANs and GMMs},
author = {Richardson, Eitan and Weiss, Yair},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {5852--5863},
year = {2018},
publisher = {Curran Associates, Inc.}
}
@article{grover2017aaai_boost,
  author    = {Aditya Grover and
               Stefano Ermon},
  title     = {Boosted Generative Models},
  journal   = {CoRR},
  volume    = {abs/1702.08484},
  year      = {2017},
  archivePrefix = {arXiv},
  eprint    = {1702.08484},
  timestamp = {Mon, 13 Aug 2018 16:46:31 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/GroverE17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/DinhKB14,
  author    = {Laurent Dinh and
               David Krueger and
               Yoshua Bengio},
  title     = {{NICE:} Non-linear Independent Components Estimation},
  journal   = {CoRR},
  volume    = {abs/1410.8516},
  year      = {2014},
  archivePrefix = {arXiv},
  eprint    = {1410.8516},
  timestamp = {Mon, 13 Aug 2018 16:48:00 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/DinhKB14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/GhoshKNTD17,
  author    = {Arnab Ghosh and
               Viveka Kulharia and
               Vinay P. Namboodiri and
               Philip H. S. Torr and
               Puneet Kumar Dokania},
  title     = {Multi-Agent Diverse Generative Adversarial Networks},
  journal   = {CoRR},
  volume    = {abs/1704.02906},
  year      = {2017},
  archivePrefix = {arXiv},
  eprint    = {1704.02906},
  timestamp = {Mon, 13 Aug 2018 16:47:13 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/GhoshKNTD17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{hoang2018mgan,
title={{MGAN}: Training Generative Adversarial Nets with Multiple Generators},
author={Quan Hoang and Tu Dinh Nguyen and Trung Le and Dinh Phung},
booktitle={International Conference on Learning Representations},
year={2018}
}
@inproceedings{
donahue2017adversarial,
title={Adversarial Feature Learning},
author={Jeff Donahue and Philipp Krähenbühl and Trevor Darrell},
booktitle={International Conference on Learning Representations},
year={2017},
}
@inproceedings{
dumoulin2017adversarially,
title={Adversarial Learned Inference},
author={Vincent Dumoulin and Ishmael Belghazi and Ben Poole and Olivier Mastropietro and Alex Lamb and Martin Arjovsky and Aaron Courville},
booktitle={International Conference on Learning Representations},
year={2017},
}
@incollection{dustin2017hierarchical,
title = {Hierarchical Implicit Models and Likelihood-Free Variational Inference},
author = {Tran, Dustin and Ranganath, Rajesh and Blei, David},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {5523--5533},
year = {2017},
publisher = {Curran Associates, Inc.}
}
@inproceedings{
salimans2018improving,
title={Improving {GAN}s Using Optimal Transport},
author={Tim Salimans and Han Zhang and Alec Radford and Dimitris Metaxas},
booktitle={International Conference on Learning Representations},
year={2018}
}
@INPROCEEDINGS{ledig2017photo, 
author={C. Ledig and L. Theis and F. Huszár and J. Caballero and A. Cunningham and A. Acosta and A. Aitken and A. Tejani and J. Totz and Z. Wang and W. Shi}, 
booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network}, 
year={2017}, 
volume={}, 
number={}, 
pages={105-114}, 
keywords={feedforward neural nets;image reconstruction;image resolution;image sampling;image texture;realistic images;upscaling factors;perceptual loss function;adversarial loss;content loss;natural image manifold;discriminator network;perceptual similarity;deep residual network;photo-realistic textures;heavily downsampled images;SRGAN;high-resolution images;photo-realistic single image super-resolution;generative adversarial network;deeper convolutional neural networks;super-resolution methods;objective function;mean squared reconstruction error;signal-to-noise ratios;photo-realistic natural images;Image resolution;Signal resolution;Gallium nitride;Image reconstruction;Manifolds;Training;Network architecture}, 
doi={10.1109/CVPR.2017.19}, 
ISSN={1063-6919}, 
month={July},}
@book{Bishop:2006:PRM:1162264,
 author = {Bishop, Christopher M.},
 title = {Pattern Recognition and Machine Learning (Information Science and Statistics)},
 year = {2006},
 isbn = {0387310738},
 publisher = {Springer-Verlag},
 address = {Berlin, Heidelberg},
} 
@ARTICLE{ma2011bayesian, 
author={Z. Ma and A. Leijon}, 
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
title={Bayesian Estimation of Beta Mixture Models with Variational Inference}, 
year={2011}, 
volume={33}, 
number={11}, 
pages={2160-2173}, 
keywords={approximation theory;Bayes methods;expectation-maximisation algorithm;inference mechanisms;parameter estimation;beta mixture models;Bayesian parameter estimation;posterior distribution;variational inference framework;extended factorized approximation method;model complexity;expectation maximization algorithm;Approximation methods;Bayesian methods;Data models;Numerical models;Maximum likelihood estimation;Probability density function;Bayesian estimation;maximum likelihood estimation;beta distribution;mixture modeling;variational inference;factorized approximation.;Algorithms;Bayes Theorem;Computer Simulation;Engineering;Humans;Likelihood Functions;Pattern Recognition, Automated;Skin Pigmentation}, 
doi={10.1109/TPAMI.2011.63}, 
ISSN={0162-8828}, 
month={Nov},}
@article{dempster1977maximum,
  title={Maximum likelihood from incomplete data via the EM algorithm},
  author={Dempster, Arthur P and Laird, Nan M and Rubin, Donald B},
  journal={Journal of the royal statistical society. Series B (methodological)},
  pages={1--38},
  year={1977},
  publisher={JSTOR}
}
@article{DBLP:journals/corr/KingmaB14,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  title     = {Adam: {A} Method for Stochastic Optimization},
  journal   = {CoRR},
  volume    = {abs/1412.6980},
  year      = {2014},
  archivePrefix = {arXiv},
  eprint    = {1412.6980},
  timestamp = {Mon, 13 Aug 2018 16:47:35 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/KingmaB14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{kurutach2018learning_plan,
  author    = {Thanard Kurutach and
               Aviv Tamar and
               Ge Yang and
               Stuart J. Russell and
               Pieter Abbeel},
  title     = {Learning Plannable Representations with Causal InfoGAN},
  journal   = {CoRR},
  volume    = {abs/1807.09341},
  year      = {2018},
  url       = {http://arxiv.org/abs/1807.09341},
  archivePrefix = {arXiv},
  eprint    = {1807.09341},
  timestamp = {Mon, 13 Aug 2018 16:48:44 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1807-09341},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@TECHREPORT{Zhu95informationgeometric,
    author = {Huaiyu Zhu and Richard Rohwer},
    title = {Information Geometric Measurements of Generalisation},
    institution = {},
    year = {1995}
}
@techreport{divergence-measures-and-message-passing,
author = {Minka, Tom},
title = {Divergence Measures and Message Passing},
year = {2005},
month = {January},
abstract = {

This paper presents a unifying view of message-passing algorithms, as methods to approximate a complex Bayesian network by a simpler network with minimum information divergence. In this view, the difference between mean-field methods and belief propagation is not the amount of structure they model, but only the measure of loss they minimize (`exclusive' versus `inclusive' Kullback-Leibler divergence). In each case, message-passing arises by minimizing a localized version of the divergence, local to each factor. By examining these divergence measures, we can intuit the types of solution they prefer (symmetry-breaking, for example) and their suitability for different tasks. Furthermore, by considering a wider variety of divergence measures (such as alpha-divergences), we can achieve different complexity and performance goals.


},
url = {https://www.microsoft.com/en-us/research/publication/divergence-measures-and-message-passing/},
pages = {17},
number = {MSR-TR-2005-173},
}
@INPROCEEDINGS{pseudo_priorBP2010, 
author={J. {Goldberger} and A. {Leshem}}, 
booktitle={2010 IEEE Information Theory Workshop on Information Theory (ITW 2010, Cairo)}, 
title={Pseudo Prior Belief Propagation for densely connected discrete graphs}, 
year={2010}, 
volume={}, 
number={}, 
pages={1-5}, 
keywords={maximum likelihood estimation;mean square error methods;MIMO communication;pseudo prior belief propagation;densely connected discrete graphs;linear least squares problem;maximum likelihood;minimum mean square error detection;MIMO detection problem;computational complexity;Belief propagation;MIMO;Receiving antennas;Vectors;Least squares methods;Transmitting antennas;Maximum likelihood detection;Mean square error methods;Graphical models;Application software}, 
doi={10.1109/ITWKSPS.2010.5503198}, 
ISSN={}, 
month={Jan},}
@INPROCEEDINGS{zhang2013denoise, 
author={R. {Zhang} and C. A. {Bouman} and J. {Thibault} and K. D. {Sauer}}, 
booktitle={2013 IEEE Global Conference on Signal and Information Processing}, 
title={Gaussian mixture Markov random field for image denoising and reconstruction}, 
year={2013}, 
volume={}, 
number={}, 
pages={1089-1092}, 
keywords={Gaussian processes;image denoising;image reconstruction;Markov processes;maximum likelihood estimation;mixture models;optimisation;quadratic optimization;MAP estimates;image patches;global image model;tomographic reconstruction;inverse problem;GM-MRF;image reconstruction;image denoising;Markov random field;Gaussian mixture;Computational modeling;Image reconstruction;PSNR;Noise reduction;Optimization;Computed tomography;Materials;Markov random fields;Gaussian mixture;patch-based methods;image model;prior model}, 
doi={10.1109/GlobalSIP.2013.6737083}, 
ISSN={}, 
month={Dec},}
@ARTICLE{cespedes2014ep, 
author={J. {Céspedes} and P. M. {Olmos} and M. {Sánchez-Fernández} and F. {Perez-Cruz}}, 
journal={IEEE Transactions on Communications}, 
title={Expectation Propagation Detection for High-Order High-Dimensional MIMO Systems}, 
year={2014}, 
volume={62}, 
number={8}, 
pages={2840-2849}, 
keywords={antenna arrays;computational complexity;iterative methods;maximum likelihood detection;MIMO communication;quadrature amplitude modulation;expectation propagation detection;high-order high-dimensional MIMO systems;communication system;multiple-input multiple-output constellation;high-order QAM constellation;spectral efficiency maximization;antenna number;low-complexity MIMO receivers;MIMO receiver efficiency;symbol detection;maximum likelihood detection;sphere-decoding method;transmitter-receiver number;low-complexity high-accuracy MIMO symbol detector;expectation propagation algorithm;EP algorithm;iterative approximation;polynomial-time;transmitted symbol posterior distribution;EP MIMO detector;symbol error rate reduction;computational complexity reduction;MIMO;Detectors;Approximation methods;Vectors;Signal to noise ratio;Computational complexity;High-dimensional MIMO communication systems;high-order QAM;low complexity;expectation propagation}, 
doi={10.1109/TCOMM.2014.2332349}, 
ISSN={0090-6778}, 
month={Aug},}
@ARTICLE{kschischang2001factor_graph, 
author={F. R. {Kschischang} and B. J. {Frey} and H. -. {Loeliger}}, 
journal={IEEE Transactions on Information Theory}, 
title={Factor graphs and the sum-product algorithm}, 
year={2001}, 
volume={47}, 
number={2}, 
pages={498-519}, 
keywords={graph theory;message passing;functional analysis;artificial intelligence;signal processing;digital communication;iterative decoding;Viterbi decoding;belief networks;Kalman filters;fast Fourier transforms;hidden Markov models;turbo codes;sum-product algorithm;factor graphs;global functions;local functions;factorization;bipartite graph;generic message-passing algorithm;computational rule;marginal functions;global function;artificial intelligence;signal processing;digital communications;forward/backward algorithm;Viterbi algorithm;iterative turbo decoding algorithm;belief propagation algorithm;Bayesian networks;Kalman filter;fast Fourier transform;FFT algorithms;HMM;Graph theory}, 
doi={10.1109/18.910572}, 
ISSN={0018-9448}, 
month={Feb},}
@article{10.2307/25651244,
 ISSN = {10618600},
 URL = {http://www.jstor.org/stable/25651244},
 abstract = {Hidden Markov random fields represent a complex hierarchical model, where the hidden latent process is an undirected graphical structure. Performing inference for such models is difficult primarily because the likelihood of the hidden states is often unavailable. The main contribution of this article is to present approximate methods to calculate the likelihood for large lattices based on exact methods for smaller lattices. We introduce approximate likelihood methods by relaxing some of the dependencies in the latent model, and also by extending tractable approximations to the likelihood, the so-called pseudolikelihood approximations, for a large lattice partitioned into smaller sublattices. Results are presented based on simulated data as well as inference for the temporal-spatial structure of the interaction between up-and down-regulated states within the mitochondrial chromosome of the Plasmodium falciparum organism. Supplemental material for this article is available online.},
 author = {N. Friel and A. N. Pettitt and R. Reeves and E. Wit},
 journal = {Journal of Computational and Graphical Statistics},
 number = {2},
 pages = {243--261},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd., Institute of Mathematical Statistics, Interface Foundation of America]},
 title = {Bayesian Inference in Hidden Markov Random Fields for Binary Data Defined on Large Lattices},
 volume = {18},
 year = {2009}
}
@inproceedings{DBLP:journals/corr/KingmaW13,
  author    = {Diederik P. Kingma and
               Max Welling},
  title     = {Auto-Encoding Variational Bayes},
  booktitle = {2nd International Conference on Learning Representations, {ICLR} 2014,
               Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
  year      = {2014},
  timestamp = {Thu, 04 Apr 2019 13:20:07 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/KingmaW13},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@Article{erdos1960,
  Title                    = {On the evolution of random graphs},
  Author                   = {Erdos, Paul and R{\'e}nyi, Alfr{\'e}d},
  Journal                  = {Publ. Math. Inst. Hung. Acad. Sci},
  Year                     = {1960},
  Number                   = {1},
  Pages                    = {17--60},
  Volume                   = {5}
}
@book{James:2014:ISL:2517747,
 author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
 title = {An Introduction to Statistical Learning: With Applications in R},
 year = {2014},
 isbn = {1461471370, 9781461471370},
 publisher = {Springer Publishing Company, Incorporated},
} 
@inproceedings{Minka:2001:EPA:647235.720257,
 author = {Minka, Thomas P.},
 title = {Expectation Propagation for Approximate Bayesian Inference},
 booktitle = {Proceedings of the 17th Conference in Uncertainty in Artificial Intelligence},
 series = {UAI '01},
 year = {2001},
 isbn = {1-55860-800-1},
 pages = {362--369},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=647235.720257},
 acmid = {720257},
 publisher = {Morgan Kaufmann Publishers Inc.},
 address = {San Francisco, CA, USA},
} 
@article{Ihler:2005:LBP:1046920.1088703,
 author = {Ihler, Alexander T. and Fischer III, John W. and Willsky, Alan S.},
 title = {Loopy Belief Propagation: Convergence and Effects of Message Errors},
 journal = {J. Mach. Learn. Res.},
 issue_date = {12/1/2005},
 volume = {6},
 month = dec,
 year = {2005},
 issn = {1532-4435},
 pages = {905--936},
 numpages = {32},
 url = {http://dl.acm.org/citation.cfm?id=1046920.1088703},
 acmid = {1088703},
 publisher = {JMLR.org},
} 
@inproceedings{Yedidia:2000:GBP:3008751.3008848,
 author = {Yedidia, Jonathan S. and Freeman, William T. and Weiss, Yair},
 title = {Generalized Belief Propagation},
 booktitle = {Proceedings of the 13th International Conference on Neural Information Processing Systems},
 series = {NIPS'00},
 year = {2000},
 location = {Denver, CO},
 pages = {668--674},
 numpages = {7},
 acmid = {3008848},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 
@inproceedings{Wiegerinck:2002:FBP:2968618.2968673,
 author = {Wiegerinck, Wim and Heskes, Tom},
 title = {Fractional Belief Propagation},
 booktitle = {Proceedings of the 15th International Conference on Neural Information Processing Systems},
 series = {NIPS'02},
 year = {2002},
 pages = {438--445},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=2968618.2968673},
 acmid = {2968673},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 
@phdthesis{Minka:2001:FAA:935427,
 author = {Minka, Thomas P.},
 title = {A Family of Algorithms for Approximate Bayesian Inference},
 year = {2001},
 note = {AAI0803033},
 publisher = {Massachusetts Institute of Technology},
 address = {Cambridge, MA, USA},
} 
@incollection{yingzhen2015sep,
title = {Stochastic Expectation Propagation},
author = {Li, Yingzhen and Hern\'{a}ndez-Lobato, Jos\'{e} Miguel and Turner, Richard E},
booktitle = {Advances in Neural Information Processing Systems 28},
editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
pages = {2323--2331},
year = {2015},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5760-stochastic-expectation-propagation.pdf}
}
@inproceedings{Lin:2015:DLM:2969239.2969280,
 author = {Lin, Guosheng and Shen, Chunhua and Reid, Ian and Hengel, Anton van den},
 title = {Deeply Learning the Messages in Message Passing Inference},
 booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
 series = {NIPS'15},
 year = {2015},
 location = {Montreal, Canada},
 pages = {361--369},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=2969239.2969280},
 acmid = {2969280},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
}
@article{yoon2019inferenceGraph,
  author    = {KiJung Yoon and
               Renjie Liao and
               Yuwen Xiong and
               Lisa Zhang and
               Ethan Fetaya and
               Raquel Urtasun and
               Richard S. Zemel and
               Xaq Pitkow},
  title     = {Inference in Probabilistic Graphical Models by Graph Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1803.07710},
  year      = {2018},
  archivePrefix = {arXiv},
  eprint    = {1803.07710},
  timestamp = {Mon, 13 Aug 2018 16:48:16 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1803-07710},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{du2017convergenceBP,
 author = {Du, Jian and Ma, Shaodan and Wu, Yik-Chung and Kar, Soummya and Moura, Jos{\'e} M. F.},
 title = {Convergence Analysis of Distributed Inference with Vector-valued Gaussian Belief Propagation},
 journal = {J. Mach. Learn. Res.},
 issue_date = {January 2017},
 volume = {18},
 number = {1},
 month = jan,
 year = {2017},
 issn = {1532-4435},
 pages = {6302--6339},
 numpages = {38},
 url = {http://dl.acm.org/citation.cfm?id=3122009.3242029},
 acmid = {3242029},
 publisher = {JMLR.org},
 keywords = {Markov random field, graphical model, large-scale networks, linear gaussian model, walk-summability},
}
@article{Opper:2000:GPC:1121900.1121911,
 author = {Opper, Manfred and Winther, Ole},
 title = {Gaussian Processes for Classification: Mean-Field Algorithms},
 journal = {Neural Comput.},
 issue_date = {November 2000},
 volume = {12},
 number = {11},
 month = nov,
 year = {2000},
 issn = {0899-7667},
 pages = {2655--2684},
 numpages = {30},
 url = {http://dx.doi.org/10.1162/089976600300014881},
 doi = {10.1162/089976600300014881},
 acmid = {1121911},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
}
@INPROCEEDINGS{jeon2015optimality, 
author={C. {Jeon} and R. {Ghods} and A. {Maleki} and C. {Studer}}, 
booktitle={2015 IEEE International Symposium on Information Theory (ISIT)}, 
title={Optimality of large MIMO detection via approximate message passing}, 
year={2015}, 
volume={}, 
number={}, 
pages={1227-1231}, 
keywords={message passing;MIMO communication;multiuser detection;large MIMO detection;approximate message passing;multiple-input multiple-output communication systems;wireless link;computational complexity;sub-optimal detection algorithms;novel data-detection method;AMP;Noise;MIMO;Phase shift keying;Message passing;Algorithm design and analysis;Compressed sensing;Quadrature amplitude modulation}, 
doi={10.1109/ISIT.2015.7282651}, 
ISSN={2157-8117}, 
month={June},}
@article{Pretti2005damping,
        doi = {10.1088/1742-5468/2005/11/p11008},
        year = 2005,
        month = {nov},
        publisher = {{IOP} Publishing},
        volume = {2005},
        number = {11},
        pages = {P11008--P11008},
        author = {Marco Pretti},
        title = {A message-passing algorithm with damping},
        journal = {Journal of Statistical Mechanics: Theory and Experiment},
        abstract = {We propose a modified belief propagation algorithm, with over-relaxed dynamics. Such an
algorithm turns out to be generally more stable and faster than ordinary belief
propagation. We characterize the performance of the algorithm, employed as a tool for
combinatorial optimization, on the random satisfiability problem. Moreover, we trace a
connection with a recently proposed double-loop algorithm for minimizing Bethe and
Kikuchi free energies.
}}
@ARTICLE{roosta2008reweighed_sum_product,
author={T. G. {Roosta} and M. J. {Wainwright} and S. S. {Sastry}},
journal={IEEE Transactions on Signal Processing},
title={Convergence Analysis of Reweighted Sum-Product Algorithms},
year={2008},
volume={56},
number={9},
pages={4293-4305},
keywords={convergence;graph theory;Markov processes;signal processing;reweighted sum-product algorithm;signal processing;approximate marginalization;graph-dependent weight;geometric convergence rate;approximate message-passing algorithms;Markov random fields;Convergence;Algorithm design and analysis;Sum product algorithm;Signal processing algorithms;Markov random fields;Signal design;Random variables;Smoothing methods;Noise reduction;Stability;Approximate marginalization;belief propagation;convergence analysis;graphical models;Markov random fields;sum-product algorithm},
doi={10.1109/TSP.2008.924136},
ISSN={1053-587X},
month={Sep.},}
@ARTICLE{nima2013stochasticBP,
author={N. {Noorshams} and M. J. {Wainwright}},
journal={IEEE Transactions on Information Theory},
title={Stochastic Belief Propagation: A Low-Complexity Alternative to the Sum-Product Algorithm},
year={2013},
volume={59},
number={4},
pages={1981-2000},
keywords={communication complexity;matrix algebra;mean square error methods;message passing;trees (mathematics);vectors;stochastic belief propagation;low-complexity alternative;sum-product algorithm;SBP;message-passing method;marginal distributions;graphical models;BP message updates;discrete variables;pairwise interactions;matrix-vector product;state dimensions;computational complexity;communication complexity;tree-structured graph;contractivity condition;nonasymptotic upper bounds;normalized mean-squared error decays;Vectors;Graphical models;Belief propagation;Random variables;Stochastic processes;Computational complexity;Graphical models;low-complexity belief propagation (BP);randomized algorithms;stochastic approximation;sum-product algorithm},
doi={10.1109/TIT.2012.2231464},
ISSN={},
month={April},}
@BOOK{wainwright2008graphical,
author={M. J. {Wainwright} and M. I. {Jordan}},
booktitle={Graphical Models, Exponential Families, and Variational Inference},
title={Graphical Models, Exponential Families, and Variational Inference},
year={2008},
volume={},
number={},
pages={},
keywords={ARTIFICIAL INTELLIGENCE;MACHINE LEARNING},
doi={},
ISSN={},
publisher={now},
isbn={},
url={https://ieeexplore.ieee.org/document/8187302},}
@ARTICLE{yedida2005constucting,
author={J. S. {Yedidia} and W. T. {Freeman} and Y. {Weiss}},
journal={IEEE Transactions on Information Theory},
title={Constructing free-energy approximations and generalized belief propagation algorithms},
year={2005},
volume={51},
number={7},
pages={2282-2312},
keywords={inference mechanisms;graph theory;message passing;belief networks;backpropagation;inference problem;factor graphs;Bethe approximation;free energy approximation;generalized belief propagation;GBP algorithm;junction graph method;cluster variation method;region graph method;Kikuchi free energy;message passing;sum-product algorithm;Belief propagation;Inference algorithms;Computer vision;Approximation algorithms;Clustering algorithms;Computer errors;Codes;Artificial intelligence;Physics computing;Probability;Belief propagation (BP);Bethe free energy;cluster variation method;generalized belief propagation (GBP);Kikuchi free energy;message passing;sum–product algorithm},
doi={10.1109/TIT.2005.850085},
ISSN={1557-9654},
month={July},}
@InProceedings{welling2005structured,
author = {Welling, Max and Minka, Tom and Teh, Yee Whye},
title = {Structured Region Graphs: Morphing EP into GBP},
booktitle = {UAI},
year = {2005},
month = {January},
abstract = {GBP and EP are two successful algorithms for approximate probabilistic inference, which are based on different approximation strategies. An open problem in both algorithms has been how to choose an appropriate approximation structure. We introduce “structured region graphs,” a formalism which marries these two strategies, reveals a deep connection between them, and suggests how to choose good approximation structures. In this formalism, each region has an internal structure which defines an exponential family, whose sufficient statistics must be matched by the parent region.},
edition = {UAI},
}
@inproceedings{gelfand2012generalized,
author = {Gelfand, Andrew E. and Welling, Max},
title = {Generalized Belief Propagation on Tree Robust Structured Region Graphs},
year = {2012},
isbn = {9780974903989},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
booktitle = {Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence},
pages = {296–305},
numpages = {10},
location = {Catalina Island, CA},
series = {UAI’12}
}
@article{morita1991cluster,
    author = {Morita, Tohru},
    title = "{Cluster Variation Method for Non-Uniform Ising and Heisenberg Models and Spin-Pair Correlation Function}",
    journal = {Progress of Theoretical Physics},
    volume = {85},
    number = {2},
    pages = {243-255},
    year = {1991},
    month = {02},
    abstract = "{A general discussion on applying the cluster variation method to non-uniform Ising and Heisenberg models is given. In particular, a general formula is provided for obtaining the linear response to increases of the external fields and the parameters of interactions for the uniform Ising and Heisenberg models in the cluster variation method. As an illustration, it is shown that the expressions of the spin-pair correlation function in the pair and the square approximation are readily derived by the formula, for the Ising models on the square and simple cubic lattices.}",
    issn = {0033-068X},
    doi = {10.1143/ptp/85.2.243},
    eprint = {https://academic.oup.com/ptp/article-pdf/85/2/243/5170743/85-2-243.pdf},
}
@article{PhysRev.81.988,
  title = {A Theory of Cooperative Phenomena},
  author = {Kikuchi, Ryoichi},
  journal = {Phys. Rev.},
  volume = {81},
  issue = {6},
  pages = {988--1003},
  numpages = {0},
  year = {1951},
  month = {Mar},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRev.81.988}
}
@inproceedings{srikumar-etal-2012-amortizing,
    title = "On Amortizing Inference Cost for Structured Prediction",
    author = "Srikumar, Vivek  and
      Kundu, Gourab  and
      Roth, Dan",
    booktitle = "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning",
    month = jul,
    year = "2012",
    address = "Jeju Island, Korea",
    publisher = "Association for Computational Linguistics",
    pages = "1114--1124",
}
@incollection{NIPS2019_9687,
title = {Amortized Bethe Free Energy Minimization for Learning MRFs},
author = {Wiseman, Sam and Kim, Yoon},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {15520--15531},
year = {2019},
publisher = {Curran Associates, Inc.},
}
@inproceedings{akbayrak2019reparameterization,
title = "Reparameterization gradient message passing",
abstract = "In this paper we consider efficient message passing based inference in a factor graph representation of a probabilistic model. Current message passing methods, such as belief propagation, variational message passing or expectation propagation, rely on analytically pre-computed message update rules. In practical models, it is often not feasible to analytically derive all update rules for all factors in the graph and as a result, efficient message passing-based inference cannot proceed. In related research on (non-message passing-based) inference, a “reparameterization trick” has lead to a considerable extension of the class of models for which automated inference is possible. In this paper, we introduce Reparameterization Gradient Message Passing (RGMP), which is a new message passing method based on the reparameterization gradient. In most models, the large majority of messages can be analytically derived and we resort to RGMP only when necessary. We will argue that this kind of hybrid message passing leads naturally to low-variance gradients.",
author = "Semih Akbayrak and {de Vries}, Bert",
year = "2019",
month = "9",
doi = "10.23919/EUSIPCO.2019.8902930",
language = "English",
booktitle = "EUSIPCO 2019 - 27th European Signal Processing Conference",
publisher = "Institute of Electrical and Electronics Engineers",
address = "United States",
}
@incollection{AshishNIPS2017_7181,
title = {Attention is All you Need},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {5998--6008},
year = {2017},
publisher = {Curran Associates, Inc.},
}
@incollection{wallach2019combining,
title = {Combining Generative and Discriminative Models for Hybrid Inference},
author = {Garcia Satorras, Victor and Akata, Zeynep and Welling, Max},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {13802--13812},
year = {2019},
publisher = {Curran Associates, Inc.},
}
@inproceedings{heess2013learning,
author = {Heess, Nicolas and Tarlow, Daniel and Winn, John},
title = {Learning to Pass Expectation Propagation Messages},
year = {2013},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3219–3227},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS’13}
}
@inproceedings{jitkrittum2015kernel,
  author    = {Wittawat Jitkrittum and
               Arthur Gretton and
               Nicolas Heess and
               S. M. Ali Eslami and
               Balaji Lakshminarayanan and
               Dino Sejdinovic and
               Zolt{\'{a}}n Szab{\'{o}}},
  title     = {Kernel-Based Just-In-Time Learning for Passing Expectation Propagation
               Messages},
  booktitle = {Proceedings of the Thirty-First Conference on Uncertainty in Artificial
               Intelligence, {UAI} 2015, July 12-16, 2015, Amsterdam, The Netherlands},
  pages     = {405--414},
  year      = {2015},
  timestamp = {Mon, 13 Nov 2017 12:25:45 +0100},
  biburl    = {https://dblp.org/rec/bib/conf/uai/JitkrittumGHELS15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@incollection{li2018graphical,
title = {Graphical Generative Adversarial Networks},
author = {LI, Chongxuan and Welling, Max and Zhu, Jun and Zhang, Bo},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {6069--6080},
year = {2018},
publisher = {Curran Associates, Inc.}
}
@incollection{johansonNIPS2016_6379,
title = {Composing graphical models with neural networks for structured representations and fast inference},
author = {Johnson, Matthew J and Duvenaud, David K and Wiltschko, Alex and Adams, Ryan P and Datta, Sandeep R},
booktitle = {Advances in Neural Information Processing Systems 29},
editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
pages = {2946--2954},
year = {2016},
publisher = {Curran Associates, Inc.},
}
@inproceedings{qu2019gmnn,
title={GMNN: Graph Markov Neural Networks},
author={Qu, Meng and Bengio, Yoshua and Tang, Jian},
booktitle={International Conference on Machine Learning},
pages={5241--5250},
year={2019}
}
@inproceedings{welling2001belief,
author = {Welling, Max and Teh, Yee Whye},
title = {Belief Optimization for Binary Networks: A Stable Alternative to Loopy Belief Propagation},
year = {2001},
isbn = {1558608001},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Proceedings of the 17th Conference in Uncertainty in Artificial Intelligence},
pages = {554–561},
numpages = {8},
series = {UAI ’01}
}
@inproceedings{kuleshov2017neural_variational,
author = {Kuleshov, Volodymyr and Ermon, Stefano},
title = {Neural Variational Inference and Learning in Undirected Graphical Models},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {6737–6746},
numpages = {10},
location = {Long Beach, California, USA},
series = {NIPS’17}
}
@incollection{NIPS2017_7136,
title = {Hierarchical Implicit Models and Likelihood-Free Variational Inference},
author = {Tran, Dustin and Ranganath, Rajesh and Blei, David},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {5523--5533},
year = {2017},
publisher = {Curran Associates, Inc.},
}
@InProceedings{pmlr-v70-gilmer17a,
  title = 	 {Neural Message Passing for Quantum Chemistry},
  author = 	 {Justin Gilmer and Samuel S. Schoenholz and Patrick F. Riley and Oriol Vinyals and George E. Dahl},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1263--1272},
  year = 	 {2017},
  editor = 	 {Doina Precup and Yee Whye Teh},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {International Convention Centre, Sydney, Australia},
  month = 	 {06--11 Aug},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/gilmer17a/gilmer17a.pdf},
  abstract = 	 {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.}
}
@inproceedings{Pearl1982reverend,
author = {Pearl, Judea},
title = {Reverend Bayes on Inference Engines: A Distributed Hierarchical Approach},
year = {1982},
publisher = {AAAI Press},
booktitle = {Proceedings of the Second AAAI Conference on Artificial Intelligence},
pages = {133–136},
numpages = {4},
location = {Pittsburgh, Pennsylvania},
series = {AAAI’82}
}
@article{DBLP:journals/corr/abs-1207-4158,
  author    = {Max Welling},
  title     = {On the Choice of Regions for Generalized Belief Propagation},
  journal   = {CoRR},
  volume    = {abs/1207.4158},
  year      = {2012},
  archivePrefix = {arXiv},
  eprint    = {1207.4158},
  timestamp = {Mon, 13 Aug 2018 16:48:07 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1207-4158},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@ARTICLE{mooij2007sufficient,
author={J. M. {Mooij} and H. J. {Kappen}},
journal={IEEE Transactions on Information Theory},
title={Sufficient Conditions for Convergence of the Sum–Product Algorithm},
year={2007},
volume={53},
number={12},
pages={4422-4437},
keywords={belief networks;computational complexity;convergence;graph theory;convergence;sum-product algorithm;loopy belief propagation;parallel synchronous;computational complexity;arbitrary factor graphs;Sufficient conditions;Convergence;Graphical models;Iterative algorithms;Belief propagation;Inference algorithms;Sum product algorithm;Computational complexity;Polynomials;Message passing;Contraction;convergence;factor graphs;graphical models;loopy belief propagation;marginalization;message passing;sum–product algorithm},
doi={10.1109/TIT.2007.909166},
ISSN={1557-9654},
month={Dec},
}
@inbook{yedidia2003understanding,
author = {Yedidia, Jonathan and Freeman, William and Weiss, Yair},
year = {2003},
month = {01},
pages = {239-269},
title = {Understanding belief propagation and its generalizations},
volume = {8},
isbn = {1558608117},
journal = {Exploring Artificial Intelligence in the New Millenium}
}
@book{opper2001advanced,
  title={Advanced Mean Field Methods: Theory and Practice},
  author={Opper, M. and Saad, D.},
  isbn={9780262150545},
  lccn={00053322},
  series={Neural information processing series},
  year={2001},
  publisher={MIT Press}
}
@book{koller2009pgm,
author = {Koller, Daphne and Friedman, Nir},
title = {Probabilistic Graphical Models: Principles and Techniques - Adaptive Computation and Machine Learning},
year = {2009},
isbn = {0262013193},
publisher = {The MIT Press}
}
@book{kullback1959,
  added-at = {2008-09-16T23:39:07.000+0200},
  address = {New York},
  author = {Kullback, Solomon},
  biburl = {https://www.bibsonomy.org/bibtex/28d0af9cdd06af73190b01cc1e04da70b/brian.mingus},
  booktitle = {Information Theory and Statistics},
  description = {CCNLab BibTeX},
  interhash = {03b56ca50da39d05c8832fb6f814ddda},
  intrahash = {8d0af9cdd06af73190b01cc1e04da70b},
  keywords = {stats},
  publisher = {Wiley},
  timestamp = {2008-09-16T23:40:28.000+0200},
  title = {Information Theory and Statistics},
  year = 1959
}
@article{kullback1951,
author = "Kullback, S. and Leibler, R. A.",
doi = "10.1214/aoms/1177729694",
fjournal = "The Annals of Mathematical Statistics",
journal = "Ann. Math. Statist.",
month = "03",
number = "1",
pages = "79--86",
publisher = "The Institute of Mathematical Statistics",
title = "On Information and Sufficiency",
url = "https://doi.org/10.1214/aoms/1177729694",
volume = "22",
year = "1951"
}
@article{wainwright06estimating,
  author    = {Martin J. Wainwright},
  title     = {Estimating the "Wrong" Graphical Model: Benefits in the Computation-Limited
               Setting},
  journal   = {J. Mach. Learn. Res.},
  volume    = {7},
  pages     = {1829--1859},
  year      = {2006},
  url       = {http://jmlr.org/papers/v7/wainwright06a.html},
  timestamp = {Wed, 10 Jul 2019 15:28:18 +0200},
  biburl    = {https://dblp.org/rec/journals/jmlr/Wainwright06.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{lu2019blockBP,
  author    = {You Lu and
               Zhiyuan Liu and
               Bert Huang},
  title     = {Block Belief Propagation for Parameter Learning in Markov Random Fields},
  journal   = {CoRR},
  volume    = {abs/1811.04064},
  year      = {2018},
  url       = {http://arxiv.org/abs/1811.04064},
  archivePrefix = {arXiv},
  eprint    = {1811.04064},
  timestamp = {Wed, 24 Jul 2019 11:25:28 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1811-04064.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{sutton2012piecewise,
  author    = {Charles A. Sutton and
               Andrew McCallum},
  title     = {Piecewise Training for Undirected Models},
  journal   = {CoRR},
  volume    = {abs/1207.1409},
  year      = {2012},
  url       = {http://arxiv.org/abs/1207.1409},
  archivePrefix = {arXiv},
  eprint    = {1207.1409},
  timestamp = {Mon, 13 Aug 2018 16:48:13 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1207-1409.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@InProceedings{lin_2016_CVPR,
author = {Lin, Guosheng and Shen, Chunhua and van den Hengel, Anton and Reid, Ian},
title = {Efficient Piecewise Training of Deep Structured Models for Semantic Segmentation},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2016}
}
@misc{lazarogredilla2019learning,
    title={Learning undirected models via query training},
    author={Miguel Lazaro-Gredilla and Wolfgang Lehrach and Dileep George},
    year={2019},
    eprint={1912.02893},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@article{DEMP1977em,
  added-at = {2013-07-02T15:06:42.000+0200},
  author = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
  biburl = {https://www.bibsonomy.org/bibtex/220a2bf528cbc6504f573625ecb78362e/mjobst},
  interhash = {6a3c3e7e36b05f7855a57eab65f93593},
  intrahash = {20a2bf528cbc6504f573625ecb78362e},
  journal = {Journal of the Royal Statistical Society: Series B},
  keywords = {},
  owner = {plankensteiner},
  pages = {1-38},
  timestamp = {2013-07-02T15:06:42.000+0200},
  title = {Maximum likelihood from incomplete data via the {EM} algorithm},
  url = {http://web.mit.edu/6.435/www/Dempster77.pdf},
  volume = 39,
  year = 1977
}
@misc{neath2012convergence,
    title={On Convergence Properties of the Monte Carlo EM Algorithm},
    author={Ronald C. Neath},
    year={2012},
    eprint={1206.4768},
    archivePrefix={arXiv},
    primaryClass={math.ST}
}
@article{kuleshov2017NVIL,
  author    = {Volodymyr Kuleshov and
               Stefano Ermon},
  title     = {Neural Variational Inference and Learning in Undirected Graphical
               Models},
  journal   = {CoRR},
  volume    = {abs/1711.02679},
  year      = {2017},
  url       = {http://arxiv.org/abs/1711.02679},
  archivePrefix = {arXiv},
  eprint    = {1711.02679},
  timestamp = {Mon, 13 Aug 2018 16:48:22 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1711-02679.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@incollection{goodfellow2014gan,
title = {Generative Adversarial Nets},
author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
booktitle = {Advances in Neural Information Processing Systems 27},
editor = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
pages = {2672--2680},
year = {2014},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf}
}
@incollection{deco1995high-order,
title = {Higher Order Statistical Decorrelation without Information Loss},
author = {Gustavo Deco and Wilfried Brauer},
booktitle = {Advances in Neural Information Processing Systems 7},
editor = {G. Tesauro and D. S. Touretzky and T. K. Leen},
pages = {247--254},
year = {1995},
publisher = {MIT Press},
url = {http://papers.nips.cc/paper/901-higher-order-statistical-decorrelation-without-information-loss.pdf}
}
@incollection{ricky2018ODE,
title = {Neural Ordinary Differential Equations},
author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {6571--6583},
year = {2018},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7892-neural-ordinary-differential-equations.pdf}
}
@book{santambrogio2015optimal,
  title={Optimal Transport for Applied Mathematicians: Calculus of Variations, PDEs, and Modeling},
  author={Santambrogio, F.},
  isbn={9783319208282},
  series={Progress in Nonlinear Differential Equations and Their Applications},
  url={https://books.google.se/books?id=UOHHCgAAQBAJ},
  year={2015},
  publisher={Springer International Publishing}
}
@article{pearl1986b,
  author    = {Pearl, J},
  title     = {Fusion, Propagation, and Structuring in Belief Networks},
  publisher   = {Elsevier Science Publisher Ltd.},
  volume    = {29},
  number    = {3},
  issn = {0004-3702},
  year      = {1986},
  journal   = {Artif. Intell.},
  monthe = sep,
  pages ={241-288},
  numpages = {48}
}
@article{Chernoff1952measure,
  added-at = {2008-10-07T16:03:39.000+0200},
  author = {Chernoff, H.},
  biburl = {https://www.bibsonomy.org/bibtex/2b075bfad5c209bbe83bafdb4f95ec19b/brefeld},
  interhash = {36d092a346b0714b53bd0e00212f42f0},
  intrahash = {b075bfad5c209bbe83bafdb4f95ec19b},
  journal = {Annals of Mathematical Statistics},
  keywords = {imported},
  pages = {409-507},
  timestamp = {2008-10-07T16:03:43.000+0200},
  title = {A measure of asymptotic efficiency for tests of a hypothesis based on the sums of observations},
  volume = 23,
  year = 1952
}
@inproceedings{renyi1961entropy,
address = "Berkeley, Calif.",
author = "R\'enyi, Alfr\'ed",
booktitle = "Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics",
pages = "547--561",
publisher = "University of California Press",
title = "On Measures of Entropy and Information",
url = "https://projecteuclid.org/euclid.bsmsp/1200512181",
year = "1961"
}
@article{amari1982differential,
author = "Amari, Shun-Ichi",
doi = "10.1214/aos/1176345779",
fjournal = "The Annals of Statistics",
journal = "Ann. Statist.",
month = "06",
number = "2",
pages = "357--385",
publisher = "The Institute of Mathematical Statistics",
title = "Differential Geometry of Curved Exponential Families-Curvatures and Information Loss",
url = "https://doi.org/10.1214/aos/1176345779",
volume = "10",
year = "1982"
}
@paper{ghosh2016assumed,
	author = {Soumya Ghosh and Francesco Delle Fave and Jonathan Yedidia},
	title = {Assumed Density Filtering Methods for Learning Bayesian Neural Networks},
	conference = {AAAI Conference on Artificial Intelligence},
	year = {2016},
	keywords = {},
	abstract = {Buoyed by the success of deep multilayer neural networks, there is renewed interest in scalable learning of Bayesian neural networks. Here, we study algorithms that utilize recent advances in Bayesian inference to efficiently learn distributions over network weights. In particular, we focus on recently proposed assumed density filtering based methods for learning Bayesian neural networks -- Expectation and Probabilistic backpropagation. Apart from scaling to large datasets, these techniques seamlessly deal with non-differentiable activation functions and provide parameter (learning rate, momentum) free learning. In this paper, we first rigorously compare the two algorithms and in the process develop several extensions, including a version of EBP for continuous regression problems and a PBP variant for binary classification. Next, we extend both algorithms to deal with multiclass classification and count regression problems. On a variety of diverse real world benchmarks, we find our extensions to be effective, achieving results competitive with the state-of-the-art.},

	url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12391}
}
@inbook{opper1999bayesian,
author = {Opper, Manfred},
title = {A Bayesian Approach to Online Learning},
year = {1999},
publisher = {Cambridge University Press},
address = {USA},
booktitle = {On-Line Learning in Neural Networks},
numpages = {16}
}
