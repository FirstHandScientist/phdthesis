@ARTICLE{2017arXiv170104862A,
   author = {{Arjovsky}, M. and {Bottou}, L.},
    title = "{Towards Principled Methods for Training Generative Adversarial Networks}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1701.04862},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Computer Science - Learning},
     year = 2017,
    month = jan,
   adsurl = {http://adsabs.harvard.edu/abs/{2016arXiv1610065452017arXiv170104862A},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
}
@ARTICLE{2017arXiv170100160G,
   author = {{Goodfellow}, I.},
    title = "{NIPS 2016 Tutorial: Generative Adversarial Networks}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1701.00160},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning},
     year = 2017,
    month = dec,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170100160G},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016arXiv160602206F,
   author = {{Farnia}, F. and {Tse}, D.},
    title = "{A Minimax Approach to Supervised Learning}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1606.02206},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Computer Science - Information Theory, Computer Science - Learning},
     year = 2016,
    month = jun,
   adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160602206F},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@incollection{NIPS2015_5745,
title = {Distributionally Robust Logistic Regression},
author = {Shafieezadeh-Abadeh, Soroosh and Esfahani, Peyman Mohajerin and Kuhn, Daniel},
booktitle = {Advances in Neural Information Processing Systems 28},
editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
pages = {1576--1584},
year = {2015},
publisher = {Curran Associates, Inc.}
}
@incollection{NIPS2014_5458,
title = {Robust Classification Under Sample Selection Bias},
author = {Liu, Anqi and Ziebart, Brian},
booktitle = {Advances in Neural Information Processing Systems 27},
editor = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
pages = {37--45},
year = {2014},
publisher = {Curran Associates, Inc.}
}
@ARTICLE{2017arXiv171010016S,
   author = {{Shafieezadeh-Abadeh}, S. and {Kuhn}, D. and {Mohajerin Esfahani}, P.
	},
    title = "{Regularization via Mass Transportation}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1710.10016},
 primaryClass = "math.OC",
 keywords = {Mathematics - Optimization and Control, Computer Science - Learning, Statistics - Machine Learning},
     year = 2017,
    month = oct,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171010016S},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{2013arXiv1312.6199S,
   author = {{Szegedy}, C. and {Zaremba}, W. and {Sutskever}, I. and {Bruna}, J. and 
	{Erhan}, D. and {Goodfellow}, I. and {Fergus}, R.},
    title = "{Intriguing properties of neural networks}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1312.6199},
 primaryClass = "cs.CV",
 keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
     year = 2013,
    month = dec,
   adsurl = {http://adsabs.harvard.edu/abs/2013arXiv1312.6199S},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{2017arXiv171100970S,
   author = {{Santurkar}, S. and {Schmidt}, L. and {M{\c a}dry}, A.},
    title = "{A Classification-Based Study of Covariate Shift in GAN Distributions}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1711.00970},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
     year = 2017,
    month = nov,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171100970S},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@incollection{NIPS2016_6248,
title = {Wasserstein Training of Restricted Boltzmann Machines},
author = {Montavon, Gr\'{e}goire and M\"{u}ller, Klaus-Robert and Cuturi, Marco},
booktitle = {Advances in Neural Information Processing Systems 29},
editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
pages = {3718--3726},
year = {2016},
publisher = {Curran Associates, Inc.}
}
@Inbook{Hinton2012,
author="Hinton, Geoffrey E.",
editor="Montavon, Gr{\'e}goire
and Orr, Genevi{\`e}ve B.
and M{\"u}ller, Klaus-Robert",
title="A Practical Guide to Training Restricted Boltzmann Machines",
bookTitle="Neural Networks: Tricks of the Trade: Second Edition",
year="2012",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="599--619",
abstract="Restricted Boltzmann machines (RBMs) have been used as generative models of many different types of data. RBMs are usually trained using the contrastive divergence learning procedure. This requires a certain amount of practical experience to decide how to set the values of numerical meta-parameters. Over the last few years, the machine learning group at the University of Toronto has acquired considerable expertise at training RBMs and this guide is an attempt to share this expertise with other machine learning researchers.",
isbn="978-3-642-35289-8",
doi="10.1007/978-3-642-35289-8_32",
url="https://doi.org/10.1007/978-3-642-35289-8_32"
}
@InProceedings{2013arXiv1310.4375C,
  title = 	 {Fast Computation of Wasserstein Barycenters},
  author = 	 {Marco Cuturi and Arnaud Doucet},
  booktitle = 	 {Proceedings of the 31st International Conference on Machine Learning},
  pages = 	 {685--693},
  year = 	 {2014},
  volume = 	 {32},
  number =       {2},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Bejing, China},
  month = 	 {22--24 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v32/cuturi14.pdf},
  url = 	 {http://proceedings.mlr.press/v32/cuturi14.html},
  abstract = 	 {We present new algorithms to compute the mean of a set of N empirical probability measures under the optimal transport metric. This mean, known as the Wasserstein barycenter \citepagueh2011barycenters,rabin2012, is the measure that minimizes the sum of its Wasserstein distances to each element in that set. We argue through a simple example that Wasserstein barycenters have appealing properties that differentiate them from other barycenters proposed recently, which all build on kernel smoothing and/or Bregman divergences. Two original algorithms are proposed that require the repeated computation of primal and dual optimal solutions of transport problems. However direct implementation of these algorithms is too costly as optimal transports are notoriously computationally expensive. Extending the work of \citetcuturi2013sinkhorn, we smooth both the primal and dual of the optimal transport problem to recover fast approximations of the primal and dual optimal solutions. We apply these algorithms to the visualization of perturbed images and to a clustering problem.}
}
@incollection{2013arXiv1306.0895C,
title = {Sinkhorn Distances: Lightspeed Computation of Optimal Transport},
author = {Cuturi, Marco},
booktitle = {Advances in Neural Information Processing Systems 26},
editor = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
pages = {2292--2300},
year = {2013},
publisher = {Curran Associates, Inc.}
}
@ARTICLE{2018arXiv180607066M,
   author = {{Montufar}, G.},
    title = "{Restricted Boltzmann Machines: Introduction and Review}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1806.07066},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning, Computer Science - Information Theory, Mathematics - Probability, Mathematics - Statistics Theory, Statistics - Machine Learning},
     year = 2018,
    month = jun,
   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180607066M},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@inproceedings{2017arXiv170107875A,
  title={Wasserstein generative adversarial networks},
  author={Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
  booktitle={International Conference on Machine Learning},
  pages={214--223},
  year={2017}
}
@INPROCEEDINGS{1467314, 
author={S. Chopra and R. Hadsell and Y. LeCun}, 
booktitle={2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)}, 
title={Learning a similarity metric discriminatively, with application to face verification}, 
year={2005}, 
volume={1}, 
number={}, 
pages={539-546 vol. 1}, 
keywords={face recognition;learning (artificial intelligence);similarity metric learning;face verification;face recognition;L/sub 1/ norm;semantic distance approximation;discriminative loss function;geometric distortion;Character generation;Drives;Robustness;System testing;Spatial databases;Glass;Artificial neural networks;Support vector machines;Support vector machine classification;Face recognition}, 
doi={10.1109/CVPR.2005.202}, 
ISSN={1063-6919}, 
month={June},}
}
@incollection{2017arXiv170400028G,
title = {Improved Training of Wasserstein GANs},
author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and others},
booktitle = {Advances in Neural Information Processing Systems 30},
pages = {5767--5777},
year = {2017},
publisher = {Curran Associates, Inc.}
}
@ARTICLE{2017arXiv171105084C,
   author = {{Cao}, G. and {Yang}, Y. and {Lei}, J. and others},
    title = "{TripletGAN: Training Generative Model with Triplet Loss}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1711.05084},
 keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
     year = 2017,
    month = nov,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171105084C},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@incollection{NIPS2014_5423,
title = {Generative Adversarial Nets},
author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
booktitle = {Advances in Neural Information Processing Systems 27},
pages = {2672--2680},
year = {2014},
publisher = {Curran Associates, Inc.}
}
@book{villani2003topics,
  title={Topics in optimal transportation},
  author={Villani, C{\'e}dric},
  number={58},
  year={2003},
  publisher={American Mathematical Soc.}
}
@INPROCEEDINGS{7298682, 
author={F. Schroff and D. Kalenichenko and J. Philbin}, 
booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={FaceNet: A unified embedding for face recognition and clustering}, 
year={2015}, 
volume={}, 
number={}, 
pages={815-823}, 
keywords={convolution;data mining;face recognition;image matching;neural nets;optimisation;pattern clustering;FaceNet embedding;face recognition;face clustering;deep convolutional network;embedding optimization;face patch matching;online triplet mining method;Face;Face recognition;Training;Accuracy;Artificial neural networks;Standards;Principal component analysis}, 
doi={10.1109/CVPR.2015.7298682}, 
ISSN={1063-6919}, 
month={June},}
@ARTICLE{2018arXiv180607755X,
   author = {{Xu}, Q. and {Huang}, G. and {Yuan}, Y. and {Guo}, C. and {Sun}, Y. and 
	{Wu}, F. and {Weinberger}, K.},
    title = "{An empirical study on evaluation metrics of generative adversarial networks}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1806.07755},
 keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
     year = 2018,
    month = jun,
   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180607755X},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{2016arXiv161006545L,
   author = {{Lopez-Paz}, D. and {Oquab}, M.},
    title = {Revisiting Classifier Two-Sample Tests},
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1610.06545},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning},
     year = 2016,
    month = oct,
   adsurl = {http://adsabs.harvard.edu/abs/2016arXiv161006545L},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{ponce2011computer,
  title={Computer vision: a modern approach},
  author={Ponce, Jean and Forsyth, David and Willow, Equipe-projet and others},
  journal={Computer},
  volume={16},
  number={11},
  year={2011}
}
@incollection{NIPS2016_6125,
title = {Improved Techniques for Training GANs},
author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and others},
booktitle = {Advances in Neural Information Processing Systems 29},
pages = {2234--2242},
year = {2016},
publisher = {Curran Associates, Inc.}
}
@ARTICLE{2018arXiv180101973B,
   author = {{Barratt}, S. and {Sharma}, R.},
    title = "{A Note on the Inception Score}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1801.01973},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
     year = 2018,
    month = jan,
   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180101973B},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@INPROCEEDINGS{1640964, 
author={R. Hadsell and S. Chopra and Y. LeCun}, 
booktitle={2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)}, 
title={Dimensionality Reduction by Learning an Invariant Mapping}, 
year={2006}, 
volume={2}, 
number={}, 
pages={1735-1742}, 
keywords={Extraterrestrial measurements;Image generation;Biology;Geoscience;Astronomy;Service robots;Manufacturing industries;Image analysis;Feature extraction;Data visualization}, 
doi={10.1109/CVPR.2006.100}, 
ISSN={1063-6919}, 
month={June},}

@ARTICLE{2015arXiv151106434R,
   author = {{Radford}, A. and {Metz}, L. and {Chintala}, S.},
    title = "{Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1511.06434},
 keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
     year = 2015,
    month = nov,
   adsurl = {http://adsabs.harvard.edu/abs/2015arXiv151106434R},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{ClaiciCS18,
  author    = {Sebastian Claici and
               Edward Chien and
               Justin Solomon},
  title     = {Stochastic Wasserstein Barycenters},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning,
               {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July
               10-15, 2018},
  pages     = {998--1007},
  year      = {2018},
  timestamp = {Fri, 13 Jul 2018 14:58:25 +0200},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@ARTICLE{2016arXiv161006519S,
   author = {{Schmitzer}, B.},
    title = "{Stabilized Sparse Scaling Algorithms for Entropy Regularized Transport Problems}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1610.06519},
 primaryClass = "math.OC",
 keywords = {Mathematics - Optimization and Control, Computer Science - Computational Engineering, Finance, and Science, Mathematics - Numerical Analysis},
     year = 2016,
    month = oct,
   adsurl = {http://adsabs.harvard.edu/abs/2016arXiv161006519S},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@InProceedings{pmlr-v84-genevay18a,
  title = 	 {Learning Generative Models with Sinkhorn Divergences},
  author = 	 {Aude Genevay and Gabriel Peyre and Marco Cuturi},
  booktitle = 	 {Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1608--1617},
  year = 	 {2018},
  volume = 	 {84},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Playa Blanca, Lanzarote, Canary Islands},
  month = 	 {09--11 Apr},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v84/genevay18a/genevay18a.pdf},
  url = 	 {http://proceedings.mlr.press/v84/genevay18a.html},
  abstract = 	 {The ability to compare two degenerate probability distributions, that is two distributions supported on low-dimensional manifolds in much higher-dimensional spaces, is a crucial factor in the estimation of generative mod- els.It is therefore no surprise that optimal transport (OT) metrics and their ability to handle measures with non-overlapping sup- ports have emerged as a promising tool. Yet, training generative machines using OT raises formidable computational and statistical challenges, because of (i) the computational bur- den of evaluating OT losses, (ii) their instability and lack of smoothness, (iii) the difficulty to estimate them, as well as their gradients, in high dimension. This paper presents the first tractable method to train large scale generative models using an OT-based loss called Sinkhorn loss which tackles these three issues by relying on two key ideas: (a) entropic smoothing, which turns the original OT loss into a differentiable and more robust quantity that can be computed using Sinkhorn fixed point iterations; (b) algorithmic (automatic) differentiation of these iterations with seam- less GPU execution. Additionally, Entropic smoothing generates a family of losses interpolating between Wasserstein (OT) and Energy distance/Maximum Mean Discrepancy (MMD) losses, thus allowing to find a sweet spot leveraging the geometry of OT on the one hand, and the favorable high-dimensional sample complexity of MMD, which comes with un- biased gradient estimates. The resulting computational architecture complements nicely standard deep network generative models by a stack of extra layers implementing the loss function.}
}
@incollection{2017arXiv170608500H,
title = {GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium},
author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and others},
booktitle = {Advances in Neural Information Processing Systems 30},
pages = {6626--6637},
year = {2017},
publisher = {Curran Associates, Inc.}
}
@inproceedings{2018arXiv180205957M,
title={Spectral Normalization for Generative Adversarial Networks},
author={Takeru Miyato and Toshiki Kataoka and Masanori Koyama and Yuichi Yoshida},
booktitle={International Conference on Learning Representations},
year={2018}
}
@ARTICLE{2017arXiv170208398M,
   author = {{Mroueh}, Y. and {Sercu}, T. and {Goel}, V.},
    title = "{McGan: Mean and Covariance Feature Matching GAN}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1702.08398},
 keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
     year = 2017,
    month = feb,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170208398M},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{2017arXiv170502894L,
   author = {{Lim}, J.~H. and {Ye}, J.~C.},
    title = "{Geometric GAN}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1705.02894},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
     year = 2017,
    month = may,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170502894L},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018arXiv180508318Z,
   author = {{Zhang}, H. and {Goodfellow}, I. and {Metaxas}, D. and {Odena}, A.
	},
    title = "{Self-Attention Generative Adversarial Networks}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1805.08318},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
     year = 2018,
    month = may,
   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180508318Z},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@incollection{pytorch,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and others},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
}
@inproceedings{tensorflow,
  title={Tensorflow: A system for large-scale machine learning},
  author={Abadi, Mart{\'\i}n and Barham, Paul and Chen, Jianmin and others},
  booktitle={12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16)},
  pages={265--283},
  year={2016}
}
@ARTICLE{2018arXiv180300567P,
   author = {{Peyr{\'e}}, G. and {Cuturi}, M.},
    title = "{Computational Optimal Transport}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1803.00567},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning},
     year = 2018,
    month = mar,
   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180300567P},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@INPROCEEDINGS{5459199, 
author={O. Pele and M. Werman}, 
booktitle={2009 IEEE 12th International Conference on Computer Vision}, 
title={Fast and robust Earth Mover's Distances}, 
year={2009}, 
volume={}, 
number={}, 
pages={460-467}, 
keywords={edge detection;thresholded ground distances;flow-network;histograms;outlier noise;quantization effects;robust earth mover's distances;Robustness;Earth;Histograms;Costs;Humans;Image retrieval;Image edge detection;Quantization;Computer vision;Image databases}, 
doi={10.1109/ICCV.2009.5459199}, 
ISSN={2380-7504}, 
month={Sept},}
@ARTICLE{2017arXiv171101558T,
   author = {{Tolstikhin}, I. and {Bousquet}, O. and {Gelly}, S. and {Schoelkopf}, B.
	},
    title = "{Wasserstein Auto-Encoders}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1711.01558},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
     year = 2017,
    month = nov,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171101558T},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{2017arXiv170104722M,
   author = {{Mescheder}, L. and {Nowozin}, S. and {Geiger}, A.},
    title = "{Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1701.04722},
 keywords = {Computer Science - Machine Learning},
     year = 2017,
    month = jan,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170104722M},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{2017arXiv170102386T,
   author = {{Tolstikhin}, I. and {Gelly}, S. and {Bousquet}, O. and {Simon-Gabriel}, C.-J. and 
	{Sch{\"o}lkopf}, B.},
    title = "{AdaGAN: Boosting Generative Models}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1701.02386},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
     year = 2017,
    month = jan,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170102386T},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{2018arXiv180601879W,
   author = {{Weed}, J.},
    title = "{An explicit analysis of the entropic penalty in linear programming}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1806.01879},
 primaryClass = "math.OC",
 keywords = {Mathematics - Optimization and Control, Computer Science - Machine Learning, Statistics - Machine Learning},
     year = 2018,
    month = jun,
   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180601879W},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{2018arXiv180600880K,
   author = {{Khayatkhoei}, M. and {Elgammal}, A. and {Singh}, M.},
    title = "{Disconnected Manifold Learning for Generative Adversarial Networks}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1806.00880},
 keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
     year = 2018,
    month = jun,
   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180600880K},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{2018arXiv181106763L,
   author = {{Liu}, D. and {Th{\`a}nh Vu}, M. and {Chatterjee}, S. and {Rasmussen}, L.~K.
	},
    title = "{Entropy-regularized Optimal Transport Generative Models}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1811.06763},
 keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
     year = 2018,
    month = nov,
   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv181106763L},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{2018arXiv180703039K,
   author = {{Kingma}, D.~P. and {Dhariwal}, P.},
    title = "{Glow: Generative Flow with Invertible 1x1 Convolutions}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1807.03039},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
     year = 2018,
    month = jul,
   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180703039K},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{2016arXiv160508803D,
   author = {{Dinh}, L. and {Sohl-Dickstein}, J. and {Bengio}, S.},
    title = "{Density estimation using Real NVP}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1605.08803},
 keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
     year = 2016,
    month = may,
   adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160508803D},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@incollection{NIPS2016_6399,
title = {InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets},
author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
booktitle = {Advances in Neural Information Processing Systems 29},
editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
pages = {2172--2180},
year = {2016},
publisher = {Curran Associates, Inc.}
}
@article{bang2018icml,
  author    = {Duhyeon Bang and
               Hyunjung Shim},
  title     = {Improved Training of Generative Adversarial Networks Using Representative
               Features},
  journal   = {CoRR},
  volume    = {abs/1801.09195},
  year      = {2018},
  archivePrefix = {arXiv},
  eprint    = {1801.09195},
  timestamp = {Mon, 13 Aug 2018 16:47:52 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1801-09195},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@incollection{eitan2018nips_gmm,
title = {On GANs and GMMs},
author = {Richardson, Eitan and Weiss, Yair},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {5852--5863},
year = {2018},
publisher = {Curran Associates, Inc.}
}
@article{grover2017aaai_boost,
  author    = {Aditya Grover and
               Stefano Ermon},
  title     = {Boosted Generative Models},
  journal   = {CoRR},
  volume    = {abs/1702.08484},
  year      = {2017},
  archivePrefix = {arXiv},
  eprint    = {1702.08484},
  timestamp = {Mon, 13 Aug 2018 16:46:31 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/GroverE17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/DinhKB14,
  author    = {Laurent Dinh and
               David Krueger and
               Yoshua Bengio},
  title     = {{NICE:} Non-linear Independent Components Estimation},
  journal   = {CoRR},
  volume    = {abs/1410.8516},
  year      = {2014},
  archivePrefix = {arXiv},
  eprint    = {1410.8516},
  timestamp = {Mon, 13 Aug 2018 16:48:00 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/DinhKB14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/GhoshKNTD17,
  author    = {Arnab Ghosh and
               Viveka Kulharia and
               Vinay P. Namboodiri and
               Philip H. S. Torr and
               Puneet Kumar Dokania},
  title     = {Multi-Agent Diverse Generative Adversarial Networks},
  journal   = {CoRR},
  volume    = {abs/1704.02906},
  year      = {2017},
  archivePrefix = {arXiv},
  eprint    = {1704.02906},
  timestamp = {Mon, 13 Aug 2018 16:47:13 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/GhoshKNTD17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{hoang2018mgan,
title={{MGAN}: Training Generative Adversarial Nets with Multiple Generators},
author={Quan Hoang and Tu Dinh Nguyen and Trung Le and Dinh Phung},
booktitle={International Conference on Learning Representations},
year={2018}
}
@inproceedings{
donahue2017adversarial,
title={Adversarial Feature Learning},
author={Jeff Donahue and Philipp Krähenbühl and Trevor Darrell},
booktitle={International Conference on Learning Representations},
year={2017},
}
@inproceedings{
dumoulin2017adversarially,
title={Adversarial Learned Inference},
author={Vincent Dumoulin and Ishmael Belghazi and Ben Poole and Olivier Mastropietro and Alex Lamb and Martin Arjovsky and Aaron Courville},
booktitle={International Conference on Learning Representations},
year={2017},
}
@incollection{dustin2017hierarchical,
title = {Hierarchical Implicit Models and Likelihood-Free Variational Inference},
author = {Tran, Dustin and Ranganath, Rajesh and Blei, David},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {5523--5533},
year = {2017},
publisher = {Curran Associates, Inc.}
}
@inproceedings{
salimans2018improving,
title={Improving {GAN}s Using Optimal Transport},
author={Tim Salimans and Han Zhang and Alec Radford and Dimitris Metaxas},
booktitle={International Conference on Learning Representations},
year={2018}
}
@INPROCEEDINGS{ledig2017photo, 
author={C. Ledig and L. Theis and F. Huszár and J. Caballero and A. Cunningham and A. Acosta and A. Aitken and A. Tejani and J. Totz and Z. Wang and W. Shi}, 
booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network}, 
year={2017}, 
volume={}, 
number={}, 
pages={105-114}, 
keywords={feedforward neural nets;image reconstruction;image resolution;image sampling;image texture;realistic images;upscaling factors;perceptual loss function;adversarial loss;content loss;natural image manifold;discriminator network;perceptual similarity;deep residual network;photo-realistic textures;heavily downsampled images;SRGAN;high-resolution images;photo-realistic single image super-resolution;generative adversarial network;deeper convolutional neural networks;super-resolution methods;objective function;mean squared reconstruction error;signal-to-noise ratios;photo-realistic natural images;Image resolution;Signal resolution;Gallium nitride;Image reconstruction;Manifolds;Training;Network architecture}, 
doi={10.1109/CVPR.2017.19}, 
ISSN={1063-6919}, 
month={July},}
@book{Bishop:2006:PRM:1162264,
 author = {Bishop, Christopher M.},
 title = {Pattern Recognition and Machine Learning (Information Science and Statistics)},
 year = {2006},
 isbn = {0387310738},
 publisher = {Springer-Verlag},
 address = {Berlin, Heidelberg},
} 
@ARTICLE{ma2011bayesian, 
author={Z. Ma and A. Leijon}, 
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
title={Bayesian Estimation of Beta Mixture Models with Variational Inference}, 
year={2011}, 
volume={33}, 
number={11}, 
pages={2160-2173}, 
keywords={approximation theory;Bayes methods;expectation-maximisation algorithm;inference mechanisms;parameter estimation;beta mixture models;Bayesian parameter estimation;posterior distribution;variational inference framework;extended factorized approximation method;model complexity;expectation maximization algorithm;Approximation methods;Bayesian methods;Data models;Numerical models;Maximum likelihood estimation;Probability density function;Bayesian estimation;maximum likelihood estimation;beta distribution;mixture modeling;variational inference;factorized approximation.;Algorithms;Bayes Theorem;Computer Simulation;Engineering;Humans;Likelihood Functions;Pattern Recognition, Automated;Skin Pigmentation}, 
doi={10.1109/TPAMI.2011.63}, 
ISSN={0162-8828}, 
month={Nov},}
@article{dempster1977maximum,
  title={Maximum likelihood from incomplete data via the EM algorithm},
  author={Dempster, Arthur P and Laird, Nan M and Rubin, Donald B},
  journal={Journal of the royal statistical society. Series B (methodological)},
  pages={1--38},
  year={1977},
  publisher={JSTOR}
}
@article{DBLP:journals/corr/KingmaB14,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  title     = {Adam: {A} Method for Stochastic Optimization},
  journal   = {CoRR},
  volume    = {abs/1412.6980},
  year      = {2014},
  archivePrefix = {arXiv},
  eprint    = {1412.6980},
  timestamp = {Mon, 13 Aug 2018 16:47:35 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/KingmaB14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{kurutach2018learning_plan,
  author    = {Thanard Kurutach and
               Aviv Tamar and
               Ge Yang and
               Stuart J. Russell and
               Pieter Abbeel},
  title     = {Learning Plannable Representations with Causal InfoGAN},
  journal   = {CoRR},
  volume    = {abs/1807.09341},
  year      = {2018},
  url       = {http://arxiv.org/abs/1807.09341},
  archivePrefix = {arXiv},
  eprint    = {1807.09341},
  timestamp = {Mon, 13 Aug 2018 16:48:44 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1807-09341},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@TECHREPORT{Zhu95informationgeometric,
    author = {Huaiyu Zhu and Richard Rohwer},
    title = {Information Geometric Measurements of Generalisation},
    institution = {},
    year = {1995}
}
@techreport{divergence-measures-and-message-passing,
author = {Minka, Tom},
title = {Divergence Measures and Message Passing},
year = {2005},
month = {January},
abstract = {

This paper presents a unifying view of message-passing algorithms, as methods to approximate a complex Bayesian network by a simpler network with minimum information divergence. In this view, the difference between mean-field methods and belief propagation is not the amount of structure they model, but only the measure of loss they minimize (`exclusive' versus `inclusive' Kullback-Leibler divergence). In each case, message-passing arises by minimizing a localized version of the divergence, local to each factor. By examining these divergence measures, we can intuit the types of solution they prefer (symmetry-breaking, for example) and their suitability for different tasks. Furthermore, by considering a wider variety of divergence measures (such as alpha-divergences), we can achieve different complexity and performance goals.


},
url = {https://www.microsoft.com/en-us/research/publication/divergence-measures-and-message-passing/},
pages = {17},
number = {MSR-TR-2005-173},
}
@INPROCEEDINGS{pseudo_priorBP2010, 
author={J. {Goldberger} and A. {Leshem}}, 
booktitle={2010 IEEE Information Theory Workshop on Information Theory (ITW 2010, Cairo)}, 
title={Pseudo Prior Belief Propagation for densely connected discrete graphs}, 
year={2010}, 
volume={}, 
number={}, 
pages={1-5}, 
keywords={maximum likelihood estimation;mean square error methods;MIMO communication;pseudo prior belief propagation;densely connected discrete graphs;linear least squares problem;maximum likelihood;minimum mean square error detection;MIMO detection problem;computational complexity;Belief propagation;MIMO;Receiving antennas;Vectors;Least squares methods;Transmitting antennas;Maximum likelihood detection;Mean square error methods;Graphical models;Application software}, 
doi={10.1109/ITWKSPS.2010.5503198}, 
ISSN={}, 
month={Jan},}
@INPROCEEDINGS{zhang2013denoise, 
author={R. {Zhang} and C. A. {Bouman} and J. {Thibault} and K. D. {Sauer}}, 
booktitle={2013 IEEE Global Conference on Signal and Information Processing}, 
title={Gaussian mixture Markov random field for image denoising and reconstruction}, 
year={2013}, 
volume={}, 
number={}, 
pages={1089-1092}, 
keywords={Gaussian processes;image denoising;image reconstruction;Markov processes;maximum likelihood estimation;mixture models;optimisation;quadratic optimization;MAP estimates;image patches;global image model;tomographic reconstruction;inverse problem;GM-MRF;image reconstruction;image denoising;Markov random field;Gaussian mixture;Computational modeling;Image reconstruction;PSNR;Noise reduction;Optimization;Computed tomography;Materials;Markov random fields;Gaussian mixture;patch-based methods;image model;prior model}, 
doi={10.1109/GlobalSIP.2013.6737083}, 
ISSN={}, 
month={Dec},}
@ARTICLE{cespedes2014ep, 
author={J. {Céspedes} and P. M. {Olmos} and M. {Sánchez-Fernández} and F. {Perez-Cruz}}, 
journal={IEEE Transactions on Communications}, 
title={Expectation Propagation Detection for High-Order High-Dimensional MIMO Systems}, 
year={2014}, 
volume={62}, 
number={8}, 
pages={2840-2849}, 
keywords={antenna arrays;computational complexity;iterative methods;maximum likelihood detection;MIMO communication;quadrature amplitude modulation;expectation propagation detection;high-order high-dimensional MIMO systems;communication system;multiple-input multiple-output constellation;high-order QAM constellation;spectral efficiency maximization;antenna number;low-complexity MIMO receivers;MIMO receiver efficiency;symbol detection;maximum likelihood detection;sphere-decoding method;transmitter-receiver number;low-complexity high-accuracy MIMO symbol detector;expectation propagation algorithm;EP algorithm;iterative approximation;polynomial-time;transmitted symbol posterior distribution;EP MIMO detector;symbol error rate reduction;computational complexity reduction;MIMO;Detectors;Approximation methods;Vectors;Signal to noise ratio;Computational complexity;High-dimensional MIMO communication systems;high-order QAM;low complexity;expectation propagation}, 
doi={10.1109/TCOMM.2014.2332349}, 
ISSN={0090-6778}, 
month={Aug},}
@ARTICLE{kschischang2001factor_graph, 
author={F. R. {Kschischang} and B. J. {Frey} and H. -. {Loeliger}}, 
journal={IEEE Transactions on Information Theory}, 
title={Factor graphs and the sum-product algorithm}, 
year={2001}, 
volume={47}, 
number={2}, 
pages={498-519}, 
keywords={graph theory;message passing;functional analysis;artificial intelligence;signal processing;digital communication;iterative decoding;Viterbi decoding;belief networks;Kalman filters;fast Fourier transforms;hidden Markov models;turbo codes;sum-product algorithm;factor graphs;global functions;local functions;factorization;bipartite graph;generic message-passing algorithm;computational rule;marginal functions;global function;artificial intelligence;signal processing;digital communications;forward/backward algorithm;Viterbi algorithm;iterative turbo decoding algorithm;belief propagation algorithm;Bayesian networks;Kalman filter;fast Fourier transform;FFT algorithms;HMM;Graph theory}, 
doi={10.1109/18.910572}, 
ISSN={0018-9448}, 
month={Feb},}
@article{10.2307/25651244,
 ISSN = {10618600},
 URL = {http://www.jstor.org/stable/25651244},
 abstract = {Hidden Markov random fields represent a complex hierarchical model, where the hidden latent process is an undirected graphical structure. Performing inference for such models is difficult primarily because the likelihood of the hidden states is often unavailable. The main contribution of this article is to present approximate methods to calculate the likelihood for large lattices based on exact methods for smaller lattices. We introduce approximate likelihood methods by relaxing some of the dependencies in the latent model, and also by extending tractable approximations to the likelihood, the so-called pseudolikelihood approximations, for a large lattice partitioned into smaller sublattices. Results are presented based on simulated data as well as inference for the temporal-spatial structure of the interaction between up-and down-regulated states within the mitochondrial chromosome of the Plasmodium falciparum organism. Supplemental material for this article is available online.},
 author = {N. Friel and A. N. Pettitt and R. Reeves and E. Wit},
 journal = {Journal of Computational and Graphical Statistics},
 number = {2},
 pages = {243--261},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd., Institute of Mathematical Statistics, Interface Foundation of America]},
 title = {Bayesian Inference in Hidden Markov Random Fields for Binary Data Defined on Large Lattices},
 volume = {18},
 year = {2009}
}
@inproceedings{DBLP:journals/corr/KingmaW13,
  author    = {Diederik P. Kingma and
               Max Welling},
  title     = {Auto-Encoding Variational Bayes},
  booktitle = {2nd International Conference on Learning Representations, {ICLR} 2014,
               Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
  year      = {2014},
  timestamp = {Thu, 04 Apr 2019 13:20:07 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/KingmaW13},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@Article{erdos1960,
  Title                    = {On the evolution of random graphs},
  Author                   = {Erdos, Paul and R{\'e}nyi, Alfr{\'e}d},
  Journal                  = {Publ. Math. Inst. Hung. Acad. Sci},
  Year                     = {1960},
  Number                   = {1},
  Pages                    = {17--60},
  Volume                   = {5}
}
@book{James:2014:ISL:2517747,
 author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
 title = {An Introduction to Statistical Learning: With Applications in R},
 year = {2014},
 isbn = {1461471370, 9781461471370},
 publisher = {Springer Publishing Company, Incorporated},
} 
@inproceedings{Minka:2001:EPA:647235.720257,
 author = {Minka, Thomas P.},
 title = {Expectation Propagation for Approximate Bayesian Inference},
 booktitle = {Proceedings of the 17th Conference in Uncertainty in Artificial Intelligence},
 series = {UAI '01},
 year = {2001},
 isbn = {1-55860-800-1},
 pages = {362--369},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=647235.720257},
 acmid = {720257},
 publisher = {Morgan Kaufmann Publishers Inc.},
 address = {San Francisco, CA, USA},
} 
@article{Ihler:2005:LBP:1046920.1088703,
 author = {Ihler, Alexander T. and Fischer III, John W. and Willsky, Alan S.},
 title = {Loopy Belief Propagation: Convergence and Effects of Message Errors},
 journal = {J. Mach. Learn. Res.},
 issue_date = {12/1/2005},
 volume = {6},
 month = dec,
 year = {2005},
 issn = {1532-4435},
 pages = {905--936},
 numpages = {32},
 url = {http://dl.acm.org/citation.cfm?id=1046920.1088703},
 acmid = {1088703},
 publisher = {JMLR.org},
} 
@inproceedings{Yedidia:2000:GBP:3008751.3008848,
 author = {Yedidia, Jonathan S. and Freeman, William T. and Weiss, Yair},
 title = {Generalized Belief Propagation},
 booktitle = {Proceedings of the 13th International Conference on Neural Information Processing Systems},
 series = {NIPS'00},
 year = {2000},
 location = {Denver, CO},
 pages = {668--674},
 numpages = {7},
 acmid = {3008848},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 
@inproceedings{Wiegerinck:2002:FBP:2968618.2968673,
 author = {Wiegerinck, Wim and Heskes, Tom},
 title = {Fractional Belief Propagation},
 booktitle = {Proceedings of the 15th International Conference on Neural Information Processing Systems},
 series = {NIPS'02},
 year = {2002},
 pages = {438--445},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=2968618.2968673},
 acmid = {2968673},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 
@phdthesis{Minka:2001:FAA:935427,
 author = {Minka, Thomas P.},
 title = {A Family of Algorithms for Approximate Bayesian Inference},
 year = {2001},
 note = {AAI0803033},
 publisher = {Massachusetts Institute of Technology},
 address = {MIT, MA, USA},
} 
@incollection{yingzhen2015sep,
title = {Stochastic Expectation Propagation},
author = {Li, Yingzhen and Hern\'{a}ndez-Lobato, Jos\'{e} Miguel and Turner, Richard E},
booktitle = {Advances in Neural Information Processing Systems 28},
editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
pages = {2323--2331},
year = {2015},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5760-stochastic-expectation-propagation.pdf}
}
@inproceedings{Lin:2015:DLM:2969239.2969280,
 author = {Lin, Guosheng and Shen, Chunhua and Reid, Ian and Hengel, Anton van den},
 title = {Deeply Learning the Messages in Message Passing Inference},
 booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
 series = {NIPS'15},
 year = {2015},
 location = {Montreal, Canada},
 pages = {361--369},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=2969239.2969280},
 acmid = {2969280},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
}
@article{yoon2019inferenceGraph,
  author    = {KiJung Yoon and
               Renjie Liao and
               Yuwen Xiong and
               Lisa Zhang and
               Ethan Fetaya and
               Raquel Urtasun and
               Richard S. Zemel and
               Xaq Pitkow},
  title     = {Inference in Probabilistic Graphical Models by Graph Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1803.07710},
  year      = {2018},
  archivePrefix = {arXiv},
  eprint    = {1803.07710},
  timestamp = {Mon, 13 Aug 2018 16:48:16 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1803-07710},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{du2017convergenceBP,
 author = {Du, Jian and Ma, Shaodan and Wu, Yik-Chung and Kar, Soummya and Moura, Jos{\'e} M. F.},
 title = {Convergence Analysis of Distributed Inference with Vector-valued Gaussian Belief Propagation},
 journal = {J. Mach. Learn. Res.},
 issue_date = {January 2017},
 volume = {18},
 number = {1},
 month = jan,
 year = {2017},
 issn = {1532-4435},
 pages = {6302--6339},
 numpages = {38},
 url = {http://dl.acm.org/citation.cfm?id=3122009.3242029},
 acmid = {3242029},
 publisher = {JMLR.org},
 keywords = {Markov random field, graphical model, large-scale networks, linear gaussian model, walk-summability},
}
@article{Opper:2000:GPC:1121900.1121911,
 author = {Opper, Manfred and Winther, Ole},
 title = {Gaussian Processes for Classification: Mean-Field Algorithms},
 journal = {Neural Comput.},
 issue_date = {November 2000},
 volume = {12},
 number = {11},
 month = nov,
 year = {2000},
 issn = {0899-7667},
 pages = {2655--2684},
 numpages = {30},
 url = {http://dx.doi.org/10.1162/089976600300014881},
 doi = {10.1162/089976600300014881},
 acmid = {1121911},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
}
@INPROCEEDINGS{jeon2015optimality, 
author={C. {Jeon} and R. {Ghods} and A. {Maleki} and C. {Studer}}, 
booktitle={2015 IEEE International Symposium on Information Theory (ISIT)}, 
title={Optimality of large MIMO detection via approximate message passing}, 
year={2015}, 
volume={}, 
number={}, 
pages={1227-1231}, 
keywords={message passing;MIMO communication;multiuser detection;large MIMO detection;approximate message passing;multiple-input multiple-output communication systems;wireless link;computational complexity;sub-optimal detection algorithms;novel data-detection method;AMP;Noise;MIMO;Phase shift keying;Message passing;Algorithm design and analysis;Compressed sensing;Quadrature amplitude modulation}, 
doi={10.1109/ISIT.2015.7282651}, 
ISSN={2157-8117}, 
month={June},}
@article{Pretti2005damping,
        doi = {10.1088/1742-5468/2005/11/p11008},
        year = 2005,
        month = {nov},
        publisher = {{IOP} Publishing},
        volume = {2005},
        number = {11},
        pages = {P11008--P11008},
        author = {Marco Pretti},
        title = {A message-passing algorithm with damping},
        journal = {Journal of Statistical Mechanics: Theory and Experiment},
        abstract = {We propose a modified belief propagation algorithm, with over-relaxed dynamics. Such an
algorithm turns out to be generally more stable and faster than ordinary belief
propagation. We characterize the performance of the algorithm, employed as a tool for
combinatorial optimization, on the random satisfiability problem. Moreover, we trace a
connection with a recently proposed double-loop algorithm for minimizing Bethe and
Kikuchi free energies.
}}
@ARTICLE{roosta2008reweighed_sum_product,
author={T. G. {Roosta} and M. J. {Wainwright} and S. S. {Sastry}},
journal={IEEE Transactions on Signal Processing},
title={Convergence Analysis of Reweighted Sum-Product Algorithms},
year={2008},
volume={56},
number={9},
pages={4293-4305},
keywords={convergence;graph theory;Markov processes;signal processing;reweighted sum-product algorithm;signal processing;approximate marginalization;graph-dependent weight;geometric convergence rate;approximate message-passing algorithms;Markov random fields;Convergence;Algorithm design and analysis;Sum product algorithm;Signal processing algorithms;Markov random fields;Signal design;Random variables;Smoothing methods;Noise reduction;Stability;Approximate marginalization;belief propagation;convergence analysis;graphical models;Markov random fields;sum-product algorithm},
doi={10.1109/TSP.2008.924136},
ISSN={1053-587X},
month={Sep.},}
@ARTICLE{nima2013stochasticBP,
author={N. {Noorshams} and M. J. {Wainwright}},
journal={IEEE Transactions on Information Theory},
title={Stochastic Belief Propagation: A Low-Complexity Alternative to the Sum-Product Algorithm},
year={2013},
volume={59},
number={4},
pages={1981-2000},
keywords={communication complexity;matrix algebra;mean square error methods;message passing;trees (mathematics);vectors;stochastic belief propagation;low-complexity alternative;sum-product algorithm;SBP;message-passing method;marginal distributions;graphical models;BP message updates;discrete variables;pairwise interactions;matrix-vector product;state dimensions;computational complexity;communication complexity;tree-structured graph;contractivity condition;nonasymptotic upper bounds;normalized mean-squared error decays;Vectors;Graphical models;Belief propagation;Random variables;Stochastic processes;Computational complexity;Graphical models;low-complexity belief propagation (BP);randomized algorithms;stochastic approximation;sum-product algorithm},
doi={10.1109/TIT.2012.2231464},
ISSN={},
month={April},}
@BOOK{wainwright2008graphical,
author={M. J. {Wainwright} and M. I. {Jordan}},
booktitle={Graphical Models, Exponential Families, and Variational Inference},
title={Graphical Models, Exponential Families, and Variational Inference},
year={2008},
volume={},
number={},
pages={},
keywords={ARTIFICIAL INTELLIGENCE;MACHINE LEARNING},
doi={},
ISSN={},
publisher={now},
isbn={},
url={https://ieeexplore.ieee.org/document/8187302},}
@ARTICLE{yedida2005constucting,
author={J. S. {Yedidia} and W. T. {Freeman} and Y. {Weiss}},
journal={IEEE Transactions on Information Theory},
title={Constructing free-energy approximations and generalized belief propagation algorithms},
year={2005},
volume={51},
number={7},
pages={2282-2312},
keywords={inference mechanisms;graph theory;message passing;belief networks;backpropagation;inference problem;factor graphs;Bethe approximation;free energy approximation;generalized belief propagation;GBP algorithm;junction graph method;cluster variation method;region graph method;Kikuchi free energy;message passing;sum-product algorithm;Belief propagation;Inference algorithms;Computer vision;Approximation algorithms;Clustering algorithms;Computer errors;Codes;Artificial intelligence;Physics computing;Probability;Belief propagation (BP);Bethe free energy;cluster variation method;generalized belief propagation (GBP);Kikuchi free energy;message passing;sum–product algorithm},
doi={10.1109/TIT.2005.850085},
ISSN={1557-9654},
month={July},}
@InProceedings{welling2005structured,
author = {Welling, Max and Minka, Tom and Teh, Yee Whye},
title = {Structured Region Graphs: Morphing EP into GBP},
booktitle = {UAI},
year = {2005},
month = {January},
abstract = {GBP and EP are two successful algorithms for approximate probabilistic inference, which are based on different approximation strategies. An open problem in both algorithms has been how to choose an appropriate approximation structure. We introduce “structured region graphs,” a formalism which marries these two strategies, reveals a deep connection between them, and suggests how to choose good approximation structures. In this formalism, each region has an internal structure which defines an exponential family, whose sufficient statistics must be matched by the parent region.},
edition = {UAI},
}
@inproceedings{gelfand2012generalized,
author = {Gelfand, Andrew E. and Welling, Max},
title = {Generalized Belief Propagation on Tree Robust Structured Region Graphs},
year = {2012},
isbn = {9780974903989},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
booktitle = {Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence},
pages = {296-305},
numpages = {10},
location = {Catalina Island, CA},
series = {UAI'12}
}
@article{morita1991cluster,
    author = {Morita, Tohru},
    title = "{Cluster Variation Method for Non-Uniform Ising and Heisenberg Models and Spin-Pair Correlation Function}",
    journal = {Progress of Theoretical Physics},
    volume = {85},
    number = {2},
    pages = {243-255},
    year = {1991},
    month = {02},
    abstract = "{A general discussion on applying the cluster variation method to non-uniform Ising and Heisenberg models is given. In particular, a general formula is provided for obtaining the linear response to increases of the external fields and the parameters of interactions for the uniform Ising and Heisenberg models in the cluster variation method. As an illustration, it is shown that the expressions of the spin-pair correlation function in the pair and the square approximation are readily derived by the formula, for the Ising models on the square and simple cubic lattices.}",
    issn = {0033-068X},
    doi = {10.1143/ptp/85.2.243},
    eprint = {https://academic.oup.com/ptp/article-pdf/85/2/243/5170743/85-2-243.pdf},
}
@article{PhysRev.81.988,
  title = {A Theory of Cooperative Phenomena},
  author = {Kikuchi, Ryoichi},
  journal = {Phys. Rev.},
  volume = {81},
  issue = {6},
  pages = {988--1003},
  numpages = {0},
  year = {1951},
  month = {Mar},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRev.81.988}
}
@inproceedings{srikumar-etal-2012-amortizing,
    title = "On Amortizing Inference Cost for Structured Prediction",
    author = "Srikumar, Vivek  and
      Kundu, Gourab  and
      Roth, Dan",
    booktitle = "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning",
    month = jul,
    year = "2012",
    address = "Jeju Island, Korea",
    publisher = "Association for Computational Linguistics",
    pages = "1114--1124",
}
@incollection{NIPS2019_9687,
title = {Amortized Bethe Free Energy Minimization for Learning MRFs},
author = {Wiseman, Sam and Kim, Yoon},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {15520--15531},
year = {2019},
publisher = {Curran Associates, Inc.},
}
@inproceedings{akbayrak2019reparameterization,
title = "Reparameterization gradient message passing",
abstract = "In this paper we consider efficient message passing based inference in a factor graph representation of a probabilistic model. Current message passing methods, such as belief propagation, variational message passing or expectation propagation, rely on analytically pre-computed message update rules. In practical models, it is often not feasible to analytically derive all update rules for all factors in the graph and as a result, efficient message passing-based inference cannot proceed. In related research on (non-message passing-based) inference, a “reparameterization trick” has lead to a considerable extension of the class of models for which automated inference is possible. In this paper, we introduce Reparameterization Gradient Message Passing (RGMP), which is a new message passing method based on the reparameterization gradient. In most models, the large majority of messages can be analytically derived and we resort to RGMP only when necessary. We will argue that this kind of hybrid message passing leads naturally to low-variance gradients.",
author = "Semih Akbayrak and {de Vries}, Bert",
year = "2019",
month = "9",
doi = "10.23919/EUSIPCO.2019.8902930",
language = "English",
booktitle = "EUSIPCO 2019 - 27th European Signal Processing Conference",
publisher = "Institute of Electrical and Electronics Engineers",
address = "United States",
}
@incollection{AshishNIPS2017_7181,
title = {Attention is All you Need},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {5998--6008},
year = {2017},
publisher = {Curran Associates, Inc.},
}
@incollection{wallach2019combining,
title = {Combining Generative and Discriminative Models for Hybrid Inference},
author = {Garcia Satorras, Victor and Akata, Zeynep and Welling, Max},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {13802--13812},
year = {2019},
publisher = {Curran Associates, Inc.},
}
@inproceedings{heess2013learning,
author = {Heess, Nicolas and Tarlow, Daniel and Winn, John},
title = {Learning to Pass Expectation Propagation Messages},
year = {2013},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3219-3227},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'13}
}
@inproceedings{jitkrittum2015kernel,
  author    = {Wittawat Jitkrittum and
               Arthur Gretton and
               Nicolas Heess and
               S. M. Ali Eslami and
               Balaji Lakshminarayanan and
               Dino Sejdinovic and
               Zolt{\'{a}}n Szab{\'{o}}},
  title     = {Kernel-Based Just-In-Time Learning for Passing Expectation Propagation
               Messages},
  booktitle = {Proceedings of the Thirty-First Conference on Uncertainty in Artificial
               Intelligence, {UAI} 2015, July 12-16, 2015, Amsterdam, The Netherlands},
  pages     = {405--414},
  year      = {2015},
  timestamp = {Mon, 13 Nov 2017 12:25:45 +0100},
  biburl    = {https://dblp.org/rec/bib/conf/uai/JitkrittumGHELS15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@incollection{li2018graphical,
title = {Graphical Generative Adversarial Networks},
author = {LI, Chongxuan and Welling, Max and Zhu, Jun and Zhang, Bo},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {6069--6080},
year = {2018},
publisher = {Curran Associates, Inc.}
}
@incollection{johansonNIPS2016_6379,
title = {Composing graphical models with neural networks for structured representations and fast inference},
author = {Johnson, Matthew J and Duvenaud, David K and Wiltschko, Alex and Adams, Ryan P and Datta, Sandeep R},
booktitle = {Advances in Neural Information Processing Systems 29},
editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
pages = {2946--2954},
year = {2016},
publisher = {Curran Associates, Inc.},
}
@inproceedings{qu2019gmnn,
title={GMNN: Graph Markov Neural Networks},
author={Qu, Meng and Bengio, Yoshua and Tang, Jian},
booktitle={International Conference on Machine Learning},
pages={5241--5250},
year={2019}
}
@inproceedings{welling2001belief,
author = {Welling, Max and Teh, Yee Whye},
title = {Belief Optimization for Binary Networks: A Stable Alternative to Loopy Belief Propagation},
year = {2001},
isbn = {1558608001},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Proceedings of the 17th Conference in Uncertainty in Artificial Intelligence},
pages = {554-561},
numpages = {8},
series = {UAI'01}
}
@inproceedings{kuleshov2017neural_variational,
author = {Kuleshov, Volodymyr and Ermon, Stefano},
title = {Neural Variational Inference and Learning in Undirected Graphical Models},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {6737-6746},
numpages = {10},
location = {Long Beach, California, USA},
series = {NIPS'17}
}
@InProceedings{pmlr-v70-gilmer17a,
  title = 	 {Neural Message Passing for Quantum Chemistry},
  author = 	 {Justin Gilmer and Samuel S. Schoenholz and Patrick F. Riley and Oriol Vinyals and George E. Dahl},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1263--1272},
  year = 	 {2017},
  editor = 	 {Doina Precup and Yee Whye Teh},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {International Convention Centre, Sydney, Australia},
  month = 	 {06--11 Aug},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/gilmer17a/gilmer17a.pdf},
  abstract = 	 {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.}
}
@inproceedings{Pearl1982reverend,
author = {Pearl, Judea},
title = {Reverend Bayes on Inference Engines: A Distributed Hierarchical Approach},
year = {1982},
publisher = {AAAI Press},
booktitle = {Proceedings of the Second AAAI Conference on Artificial Intelligence},
pages = {133-136},
numpages = {4},
location = {Pittsburgh, Pennsylvania},
series = {AAAI'82}
}
@article{DBLP:journals/corr/abs-1207-4158,
  author    = {Max Welling},
  title     = {On the Choice of Regions for Generalized Belief Propagation},
  journal   = {CoRR},
  volume    = {abs/1207.4158},
  year      = {2012},
  archivePrefix = {arXiv},
  eprint    = {1207.4158},
  timestamp = {Mon, 13 Aug 2018 16:48:07 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1207-4158},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@ARTICLE{mooij2007sufficient,
author={J. M. {Mooij} and H. J. {Kappen}},
journal={IEEE Transactions on Information Theory},
title={Sufficient Conditions for Convergence of the Sum-Product Algorithm},
year={2007},
volume={53},
number={12},
pages={4422-4437},
keywords={belief networks;computational complexity;convergence;graph theory;convergence;sum-product algorithm;loopy belief propagation;parallel synchronous;computational complexity;arbitrary factor graphs;Sufficient conditions;Convergence;Graphical models;Iterative algorithms;Belief propagation;Inference algorithms;Sum product algorithm;Computational complexity;Polynomials;Message passing;Contraction;convergence;factor graphs;graphical models;loopy belief propagation;marginalization;message passing;sum–product algorithm},
doi={10.1109/TIT.2007.909166},
ISSN={1557-9654},
month={Dec},
}
@inbook{yedidia2003understanding,
author = {Yedidia, Jonathan and Freeman, William and Weiss, Yair},
year = {2003},
month = {01},
pages = {239-269},
title = {Understanding belief propagation and its generalizations},
volume = {8},
isbn = {1558608117},
journal = {Exploring Artificial Intelligence in the New Millenium}
}
@book{opper2001advanced,
  title={Advanced Mean Field Methods: Theory and Practice},
  author={Opper, M. and Saad, D.},
  isbn={9780262150545},
  lccn={00053322},
  series={Neural information processing series},
  year={2001},
  publisher={MIT Press}
}
@book{koller2009pgm,
author = {Koller, Daphne and Friedman, Nir},
title = {Probabilistic Graphical Models: Principles and Techniques - Adaptive Computation and Machine Learning},
year = {2009},
isbn = {0262013193},
publisher = {The MIT Press}
}
@book{kullback1959,
  added-at = {2008-09-16T23:39:07.000+0200},
  address = {New York},
  author = {Kullback, Solomon},
  biburl = {https://www.bibsonomy.org/bibtex/28d0af9cdd06af73190b01cc1e04da70b/brian.mingus},
  booktitle = {Information Theory and Statistics},
  description = {CCNLab BibTeX},
  interhash = {03b56ca50da39d05c8832fb6f814ddda},
  intrahash = {8d0af9cdd06af73190b01cc1e04da70b},
  keywords = {stats},
  publisher = {Wiley},
  timestamp = {2008-09-16T23:40:28.000+0200},
  title = {Information Theory and Statistics},
  year = 1959
}
@article{kullback1951,
author = "Kullback, S. and Leibler, R. A.",
doi = "10.1214/aoms/1177729694",
fjournal = "The Annals of Mathematical Statistics",
journal = "Ann. Math. Statist.",
month = "03",
number = "1",
pages = "79--86",
publisher = "The Institute of Mathematical Statistics",
title = "On Information and Sufficiency",
url = "https://doi.org/10.1214/aoms/1177729694",
volume = "22",
year = "1951"
}
@article{wainwright06estimating,
  author    = {Martin J. Wainwright},
  title     = {Estimating the "Wrong" Graphical Model: Benefits in the Computation-Limited
               Setting},
  journal   = {J. Mach. Learn. Res.},
  volume    = {7},
  pages     = {1829--1859},
  year      = {2006},
  url       = {http://jmlr.org/papers/v7/wainwright06a.html},
  timestamp = {Wed, 10 Jul 2019 15:28:18 +0200},
  biburl    = {https://dblp.org/rec/journals/jmlr/Wainwright06.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{lu2019blockBP,
  author    = {You Lu and
               Zhiyuan Liu and
               Bert Huang},
  title     = {Block Belief Propagation for Parameter Learning in Markov Random Fields},
  journal   = {CoRR},
  volume    = {abs/1811.04064},
  year      = {2018},
  url       = {http://arxiv.org/abs/1811.04064},
  archivePrefix = {arXiv},
  eprint    = {1811.04064},
  timestamp = {Wed, 24 Jul 2019 11:25:28 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1811-04064.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{sutton2012piecewise,
  author    = {Charles A. Sutton and
               Andrew McCallum},
  title     = {Piecewise Training for Undirected Models},
  journal   = {CoRR},
  volume    = {abs/1207.1409},
  year      = {2012},
  url       = {http://arxiv.org/abs/1207.1409},
  archivePrefix = {arXiv},
  eprint    = {1207.1409},
  timestamp = {Mon, 13 Aug 2018 16:48:13 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1207-1409.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@InProceedings{lin_2016_CVPR,
author = {Lin, Guosheng and Shen, Chunhua and van den Hengel, Anton and Reid, Ian},
title = {Efficient Piecewise Training of Deep Structured Models for Semantic Segmentation},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2016}
}
@article{lazarogredilla2019learning,
    title={Learning undirected models via query training},
    author={Miguel Lazaro-Gredilla and Wolfgang Lehrach and Dileep George},
    year={2019},
    eprint={1912.02893},
    journal={arXiv},
    primaryClass={cs.LG}
}
@article{neath2012convergence,
    title={On Convergence Properties of the Monte Carlo EM Algorithm},
    author={Ronald C. Neath},
    year={2012},
    eprint={1206.4768},
    journal={arXiv},
    primaryClass={math.ST}
}
@incollection{goodfellow2014gan,
title = {Generative Adversarial Nets},
author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
booktitle = {Advances in Neural Information Processing Systems 27},
editor = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
pages = {2672--2680},
year = {2014},
publisher = {Curran Associates, Inc.}
}
@incollection{deco1995high-order,
title = {Higher Order Statistical Decorrelation without Information Loss},
author = {Gustavo Deco and Wilfried Brauer},
booktitle = {Advances in Neural Information Processing Systems 7},
editor = {G. Tesauro and D. S. Touretzky and T. K. Leen},
pages = {247--254},
year = {1995},
publisher = {MIT Press}
}
@incollection{ricky2018ODE,
title = {Neural Ordinary Differential Equations},
author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {6571--6583},
year = {2018},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7892-neural-ordinary-differential-equations.pdf}
}
@book{santambrogio2015optimal,
  title={Optimal Transport for Applied Mathematicians: Calculus of Variations, PDEs, and Modeling},
  author={Santambrogio, F.},
  isbn={9783319208282},
  series={Progress in Nonlinear Differential Equations and Their Applications},
  url={https://books.google.se/books?id=UOHHCgAAQBAJ},
  year={2015},
  publisher={Springer International Publishing}
}
@article{pearl1986b,
  author    = {Pearl, J},
  title     = {Fusion, Propagation, and Structuring in Belief Networks},
  publisher   = {Elsevier Science Publisher Ltd.},
  volume    = {29},
  number    = {3},
  issn = {0004-3702},
  year      = {1986},
  journal   = {Artif. Intell.},
  monthe = sep,
  pages ={241-288},
  numpages = {48}
}
@article{Chernoff1952measure,
  added-at = {2008-10-07T16:03:39.000+0200},
  author = {Chernoff, H.},
  biburl = {https://www.bibsonomy.org/bibtex/2b075bfad5c209bbe83bafdb4f95ec19b/brefeld},
  interhash = {36d092a346b0714b53bd0e00212f42f0},
  intrahash = {b075bfad5c209bbe83bafdb4f95ec19b},
  journal = {Annals of Mathematical Statistics},
  keywords = {imported},
  pages = {409-507},
  timestamp = {2008-10-07T16:03:43.000+0200},
  title = {A measure of asymptotic efficiency for tests of a hypothesis based on the sums of observations},
  volume = 23,
  year = 1952
}
@inproceedings{renyi1961entropy,
address = "Berkeley, Calif.",
author = "R\'enyi, Alfr\'ed",
booktitle = "Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics",
pages = "547--561",
publisher = "University of California Press",
title = "On Measures of Entropy and Information",
url = "https://projecteuclid.org/euclid.bsmsp/1200512181",
year = "1961"
}
@article{amari1982differential,
author = "Amari, Shun-Ichi",
doi = "10.1214/aos/1176345779",
fjournal = "The Annals of Statistics",
journal = "Ann. Statist.",
month = "06",
number = "2",
pages = "357--385",
publisher = "The Institute of Mathematical Statistics",
title = "Differential Geometry of Curved Exponential Families-Curvatures and Information Loss",
url = "https://doi.org/10.1214/aos/1176345779",
volume = "10",
year = "1982"
}
@inproceedings{ghosh2016assumed,
	author = {Soumya Ghosh and Francesco Delle Fave and Jonathan Yedidia},
	title = {Assumed Density Filtering Methods for Learning Bayesian Neural Networks},
	conference = {AAAI Conference on Artificial Intelligence},
	year = {2016},
	keywords = {},
	abstract = {Buoyed by the success of deep multilayer neural networks, there is renewed interest in scalable learning of Bayesian neural networks. Here, we study algorithms that utilize recent advances in Bayesian inference to efficiently learn distributions over network weights. In particular, we focus on recently proposed assumed density filtering based methods for learning Bayesian neural networks -- Expectation and Probabilistic backpropagation. Apart from scaling to large datasets, these techniques seamlessly deal with non-differentiable activation functions and provide parameter (learning rate, momentum) free learning. In this paper, we first rigorously compare the two algorithms and in the process develop several extensions, including a version of EBP for continuous regression problems and a PBP variant for binary classification. Next, we extend both algorithms to deal with multiclass classification and count regression problems. On a variety of diverse real world benchmarks, we find our extensions to be effective, achieving results competitive with the state-of-the-art.}
}
@inbook{opper1999bayesian,
author = {Opper, Manfred},
title = {A Bayesian Approach to Online Learning},
year = {1999},
publisher = {Cambridge University Press},
address = {USA},
booktitle = {On-Line Learning in Neural Networks},
numpages = {16}
}
@article{mooij2012sufficient-conditions,
  author    = {Joris M. Mooij and
               Hilbert J. Kappen},
  title     = {Sufficient conditions for convergence of Loopy Belief Propagation},
  journal   = {CoRR},
  volume    = {abs/1207.1405},
  year      = {2012},
  url       = {http://arxiv.org/abs/1207.1405},
  archivePrefix = {arXiv},
  eprint    = {1207.1405},
  timestamp = {Mon, 13 Aug 2018 16:47:21 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1207-1405},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@incollection{frederic2019fast,
title = {Fast Convergence of Belief Propagation to Global Optima: Beyond Correlation Decay},
author = {Koehler, Frederic},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {8329--8339},
year = {2019},
publisher = {Curran Associates, Inc.},
}
@article{heskes2004uniqueness,
author = {Heskes, Tom},
title = {On the Uniqueness of Loopy Belief Propagation Fixed Points},
year = {2004},
issue_date = {November 2004},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {16},
number = {11},
issn = {0899-7667},
url = {https://doi.org/10.1162/0899766041941943},
doi = {10.1162/0899766041941943},
journal = {Neural Comput.},
month = nov,
pages = {2379-2413},
numpages = {35}
}
@article{weiss2000correctness,
author = {Weiss, Yair},
title = {Correctness of Local Probability Propagation in Graphical Models with Loops},
year = {2000},
issue_date = {January 2000},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {12},
number = {1},
issn = {0899-7667},
url = {https://doi.org/10.1162/089976600300015880},
doi = {10.1162/089976600300015880},
journal = {Neural Comput.},
month = jan,
pages = {1-41},
numpages = {41}
}
@article{malioutov2006walk-sums,
author = {Malioutov, Dmitry M. and Johnson, Jason K. and Willsky, Alan S.},
title = {Walk-Sums and Belief Propagation in Gaussian Graphical Models},
year = {2006},
issue_date = {December 2006},
publisher = {JMLR.org},
volume = {7},
issn = {1532-4435},
journal = {J. Mach. Learn. Res.},
month = dec,
pages = {2031-2064},
numpages = {34}
}
@article{kingma2019vae,
url = {http://dx.doi.org/10.1561/2200000056},
year = {2019},
volume = {12},
journal = {Foundations and Trends® in Machine Learning},
title = {An Introduction to Variational Autoencoders},
doi = {10.1561/2200000056},
issn = {1935-8237},
number = {4},
pages = {307-392},
author = {Diederik P. Kingma and Max Welling}
}
@techreport{eric10-708ipf,
author = {Eric P. Xing},
title = {Learning in Fully Observed Markov Networks},
year = {2014},
url = {https://www.cs.cmu.edu/~epxing/Class/10708-14/scribe_notes/scribe_note_lecture8.pdf},
}
@incollection{kingma2016IVF,
title = {Improved Variational Inference with Inverse Autoregressive Flow},
author = {Kingma, Durk P and Salimans, Tim and Jozefowicz, Rafal and Chen, Xi and Sutskever, Ilya and Welling, Max},
booktitle = {Advances in Neural Information Processing Systems 29},
pages = {4743--4751},
year = {2016},
publisher = {Curran Associates, Inc.}
}
@article{rezende2015variational,
    title={Variational Inference with Normalizing Flows},
    author={Danilo Jimenez Rezende and Shakir Mohamed},
    year={2015},
    eprint={1505.05770},
    journal={arXiv},
    primaryClass={stat.ML}
}
@ARTICLE{bellegarda1990tiedmixture, 
author={J. R. {Bellegarda} and D. {Nahamoo}}, 
journal={IEEE Transactions on Acoustics, Speech, and Signal Processing}, 
title={Tied mixture continuous parameter modeling for speech recognition}, 
year={1990}, 
volume={38}, 
number={12}, 
pages={2033-2045}, 
keywords={acoustic signal processing;speech analysis and processing;speech recognition;discrete modelling;continuous parameter modeling;acoustic-modeling problem;automatic speech recognition;acoustic feature vectors;speech waveform;front-end signal processing;speech recognizer;multivariate probability distribution;time-synchronous labeling acoustic processor;vector quantization;multinomial probability distribution;vector quantizer;hidden Markov models;tied mixtures;multivariate densities;vocabulary;office correspondence tasks;Speech recognition;Automatic speech recognition;Acoustic waves;Speech processing;Probability distribution;Acoustic signal processing;Face recognition;Labeling;Vector quantization;Hidden Markov models}, 
doi={10.1109/29.61531}, 
ISSN={0096-3518}, 
month={Dec},}
@inproceedings{Kimball:1993:UTD:1075671.1075694,
 author = {Kimball, Owen and Ostendorf, Mari},
 title = {On the Use of Tied-mixture Distributions},
 booktitle = {Proceedings of the Workshop on Human Language Technology},
 series = {HLT '93},
 year = {1993},
 isbn = {1-55860-324-7},
 location = {Princeton, New Jersey},
 pages = {102--107},
 numpages = {6},
 url = {https://doi.org/10.3115/1075671.1075694},
 doi = {10.3115/1075671.1075694},
 acmid = {1075694},
 publisher = {Association for Computational Linguistics},
 address = {Stroudsburg, PA, USA},
}
@Article{Gupta2013,
author="Gupta, Sunil Kumar
and Phung, Dinh
and Adams, Brett
and Venkatesh, Svetha",
title="Regularized nonnegative shared subspace learning",
journal="Data Mining and Knowledge Discovery",
year="2013",
month="Jan",
day="01",
volume="26",
number="1",
pages="57--97",
abstract="Joint modeling of related data sources has the potential to improve various data mining tasks such as transfer learning, multitask clustering, information retrieval etc. However, diversity among various data sources might outweigh the advantages of the joint modeling, and thus may result in performance degradations. To this end, we propose a regularized shared subspace learning framework, which can exploit the mutual strengths of related data sources while being immune to the effects of the variabilities of each source. This is achieved by further imposing a mutual orthogonality constraint on the constituent subspaces which segregates the common patterns from the source specific patterns, and thus, avoids performance degradations. Our approach is rooted in nonnegative matrix factorization and extends it further to enable joint analysis of related data sources. Experiments performed using three real world data sets for both retrieval and clustering applications demonstrate the benefits of regularization and validate the effectiveness of the model. Our proposed solution provides a formal framework appropriate for jointly analyzing related data sources and therefore, it is applicable to a wider context in data mining.",
issn="1573-756X",
doi="10.1007/s10618-011-0244-8",
url="https://doi.org/10.1007/s10618-011-0244-8"
}
@ARTICLE{sundman2016design, 
author={D. {Sundman} and S. {Chatterjee} and M. {Skoglund}}, 
journal={IEEE Transactions on Signal Processing}, 
title={Design and Analysis of a Greedy Pursuit for Distributed Compressed Sensing}, 
year={2016}, 
volume={64}, 
number={11}, 
pages={2803-2818}, 
keywords={compressed sensing;convergence of numerical methods;signal reconstruction;signal sampling;greedy pursuit analysis;distributed compressed sensing scenario;sparse signal correlated measurement;distributed parallel pursuit algorithm;partial common support-set estimation;quality reconstruction performance;restricted isometry property based theoretical analysis;RIP based theoretical analysis;information exchange quality;DIPP algorithm;scaled additive measurement noise power;scaling coefficient;information processing quality parameter;signal-to-measurement-noise ratio;network-connectivity condition;undersampling;Signal processing algorithms;Sensors;Estimation;Correlation;Convergence;Matching pursuit algorithms;Algorithm design and analysis;Compressed sensing;restricted isometry property;distributed estimation}, 
doi={10.1109/TSP.2016.2523462}, 
ISSN={1053-587X}, 
month={June},}
@article{SUNDMAN2014298,
title = "Distributed greedy pursuit algorithms",
journal = "Signal Processing",
volume = "105",
pages = "298 - 315",
year = "2014",
issn = "0165-1684",
doi = "https://doi.org/10.1016/j.sigpro.2014.05.027",
url = "http://www.sciencedirect.com/science/article/pii/S016516841400245X",
author = "Dennis Sundman and Saikat Chatterjee and Mikael Skoglund",
keywords = "Greedy algorithms, Compressed sensing, Distributed compressed sensing",
abstract = "For compressed sensing over arbitrarily connected networks, we consider the problem of estimating underlying sparse signals in a distributed manner. We introduce a new signal model that helps to describe inter-signal correlation among connected nodes. Based on this signal model along with a brief survey of existing greedy algorithms, we develop distributed greedy algorithms with low communication overhead. Incorporating appropriate modifications, we design two new distributed algorithms where the local algorithms are based on appropriately modified existing orthogonal matching pursuit and subspace pursuit. Further, by combining advantages of these two local algorithms, we design a new greedy algorithm that is well suited for a distributed scenario. By extensive simulations we demonstrate that the new algorithms in a sparsely connected network provide good performance, close to the performance of a centralized greedy solution."
}
@ARTICLE{tang2016extreme, 
author={J. {Tang} and C. {Deng} and G. {Huang}}, 
journal={IEEE Transactions on Neural Networks and Learning Systems}, 
title={Extreme Learning Machine for Multilayer Perceptron}, 
year={2016}, 
volume={27}, 
number={4}, 
pages={809-821}, 
keywords={feedforward neural nets;learning (artificial intelligence);multilayer perceptrons;pattern classification;extreme learning machine;multilayer perceptron;learning algorithm;generalized single hidden layer feedforward neural networks;feature learning;ELM-based hierarchical learning framework;self-taught feature extraction;supervised feature classification;random initialized hidden weights;unsupervised multilayer encoding;ELM-based sparse autoencoder;feature representations;ELM random feature mapping;hierarchically encoded outputs;decision making;learning speed;greedy layerwise training;deep learning;DL;hierarchical learning methods;computer vision;Feature extraction;Training;Nonhomogeneous media;Optimization;Least squares approximations;Artificial neural networks;Deep learning (DL);deep neural network (DNN);extreme learning machine (ELM);multilayer perceptron (MLP);random feature mapping.;Deep learning (DL);deep neural network (DNN);extreme learning machine (ELM);multilayer perceptron (MLP);random feature mapping}, 
doi={10.1109/TNNLS.2015.2424995}, 
ISSN={2162-237X}, 
month={April},}
@ARTICLE{jiang2013k-svd, 
author={Z. {Jiang} and Z. {Lin} and L. S. {Davis}}, 
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
title={Label Consistent K-SVD: Learning a Discriminative Dictionary for Recognition}, 
year={2013}, 
volume={35}, 
number={11}, 
pages={2651-2664}, 
keywords={face recognition;image classification;image coding;learning (artificial intelligence);object recognition;singular value decomposition;label consistent K-SVD algorithm;discriminative dictionary learning;sparse coding;label information association;label consistency constraint;discriminative sparse-code error constraint;linear classifier;incremental dictionary learning algorithm;face category recognition;action category recognition;scene category recognition;object category recognition;k-means clustering;singular value decomposition;Dictionaries;Linear programming;Classification algorithms;Training;Algorithm design and analysis;Image reconstruction;Testing;Discriminative dictionary learning;incremental dictionary learning;supervised learning;label consistent K-SVD;discriminative sparse-code error;Algorithms;Biometry;Discriminant Analysis;Face;Humans;Image Interpretation, Computer-Assisted;Pattern Recognition, Automated;Support Vector Machines}, 
doi={10.1109/TPAMI.2013.88}, 
ISSN={0162-8828}, 
month={Nov},}
@InProceedings{pmlr-v5-salakhutdinov09a,
  title =        {Deep Boltzmann Machines},
  author =       {Ruslan Salakhutdinov and Geoffrey Hinton},
  booktitle =    {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
  pages =        {448--455},
  year =         {2009},
  volume =       {5},
  series =       {Proceedings of Machine Learning Research},
  address =      {Florida USA},
  month =        {16--18 Apr},
  publisher =    {PMLR},
  pdf =          {http://proceedings.mlr.press/v5/salakhutdinov09a/salakhutdinov09a.pdf},
  abstract =     {We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode, and data-independent expectations are approximated using persistent Markov chains. The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer “pre-training” phase that allows variational inference to be initialized by a single bottom-up pass. We present results on the MNIST and NORB datasets showing that deep Boltzmann machines learn good generative models and perform well on handwritten digit and visual object recognition tasks.}
}
@incollection{karimi2019incrementalEM,
title = {On the Global Convergence of (Fast) Incremental Expectation Maximization Methods},
author = {Karimi, Belhal and Wai, Hoi-To and Moulines, Eric and Lavielle, Marc},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {2837--2847},
year = {2019},
publisher = {Curran Associates, Inc.},
}
@incollection{chen2018emvr,
title = {Stochastic Expectation Maximization with Variance Reduction},
author = {Chen, Jianfei and Zhu, Jun and Teh, Yee Whye and Zhang, Tong},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {7967--7977},
year = {2018},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/8021-stochastic-expectation-maximization-with-variance-reduction.pdf}
}
@article{cappe2009sem,
   title={On-line expectation-maximization algorithm for latent data models},
   volume={71},
   ISSN={1467-9868},
   url={http://dx.doi.org/10.1111/j.1467-9868.2009.00698.x},
   DOI={10.1111/j.1467-9868.2009.00698.x},
   number={3},
   journal={Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
   publisher={Wiley},
   author={Cappe, Olivier and Moulines, Eric},
   year={2009},
   month={Jun},
   pages={593-613}
}
@article{charles2012crf,
author = {Sutton, Charles and McCallum, Andrew},
title = {An Introduction to Conditional Random Fields},
year = {2012},
issue_date = {April 2012},
publisher = {Now Publishers Inc.},
address = {Hanover, MA, USA},
volume = {4},
number = {4},
issn = {1935-8237},
url = {https://doi.org/10.1561/2200000013},
doi = {10.1561/2200000013},
journal = {Found. Trends Mach. Learn.},
month = apr,
pages = {267-373},
numpages = {107}
}
@InProceedings{lasserre2006principled,
author = {Lasserre, Julia A. and Bishop, Christopher and Minka, Tom},
title = {Principled Hybrids of Generative and Discriminative Models},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition},
year = {2006},
month = {June},
abstract = {When labelled training data is plentiful, discriminative techniques are widely used since they give excellent generalization performance. However, for large-scale applications such as object recognition, hand labelling of data is expensive, and there is much interest in semi-supervised techniques based on generative models in which the majority of the training data is unlabelled. Although the generalization performance of generative models can often be improved by ‘training them discriminatively’, they can then no longer make use of unlabelled data. In an attempt to gain the benefit of both generative and discriminative approaches, heuristic procedure have been proposed [2, 3] which interpolate between these two extremes by taking a convex combination of the generative and discriminative objective functions. In this paper we adopt a new perspective which says that there is only one correct way to train a given model, and that a ‘discriminatively trained’ generative model is fundamentally a new model [7]. From this viewpoint, generative and discriminative models correspond to specific choices for the prior over  parameters. As well as giving a principled interpretation of ‘discriminative training’, this approach opens door to very general ways of interpolating between generative and discriminative extremes through alternative choices of prior. We illustrate this framework using both synthetic data and a practical example in the domain of multi-class object recognition. Our results show that, when the supply of labelled training data is limited, the optimum performance corresponds to a balance between the purely generative and the purely discriminative.},
publisher = {IEEE Computer Society},
pages = {87-94},
volume = {1},
edition = {IEEE Conference on Computer Vision and Pattern Recognition},
}
@incollection{Perdigao11,
author = {Carla Lopes and Fernando Perdigao},
title = {Phoneme Recognition on the TIMIT Database},
booktitle = {Speech Technologies},
publisher = {IntechOpen},
address = {Rijeka},
year = {2011},
editor = {Ivo Ipsic},
chapter = {14},
doi = {10.5772/17600},
url = {https://doi.org/10.5772/17600}
}
@article{griffinHeartRateCharacteristics2005,
  title = {Heart {{Rate Characteristics}}: {{Novel Physiomarkers}} to {{Predict Neonatal Infection}} and {{Death}}},
  volume = {116},
  issn = {0031-4005, 1098-4275},
  shorttitle = {Heart {{Rate Characteristics}}},
  abstract = {Methods. We monitored 1022 infants at 2 tertiary care NICUs, 458 of whom were very low birth weight. We calculated an HRC index from validated regression models relating mathematical features of heart rate time series and histograms to episodes of illness. We calculated the risks for adverse events of sepsis, UTI, and death for infants stratified by HRC measurements.
Results. Compared with infants with low-risk HRC measurements, infants with high-risk HRC measurements had 5- to 6-fold increased risk for an adverse event in the next day and 3-fold increased risk in the next week. Laboratory tests that were relevant to infection added information to HRC measurements. Infants with both high-risk HRC and abnormal laboratory tests had 6- to 7-fold increased risk for an adverse event in the next day compared with infants who had neither.
Conclusion. HRC are noninvasively monitored physiomarkers that identify infants in the NICU who are at high risk for sepsis, UTI, and death. Pediatrics 2005;116: 1070\textendash{}1074; infant, mortality prediction, sepsis, urinary tract infections, physiomarker.},
  language = {en},
  number = {5},
  journal = {PEDIATRICS},
  doi = {10.1542/peds.2004-2461},
  author = {Griffin, M. P.},
  month = nov,
  year = {2005},
  pages = {1070-1074}
}
@article{hicksHeartRateObservation2013,
  title = {HeRO monitoring in the NICU: sepsis detection and beyond},
  language = {en},
  journal={Infant},
  author = {Hicks, Jamie Fletcher and Fairchild, Karen},
  year = {2013},
  pages = {5}
}
@article{gurMathematicalAlgorithmDetection2014,
  title = {A Mathematical Algorithm for Detection of {{Late}}-Onset {{Sepsis}} in Very-Low Birth Weight Infants: {{A}} Preliminary Diagnostic Test Evaluation},
  volume = {51},
  issn = {0019-6061, 0974-7559},
  shorttitle = {A Mathematical Algorithm for Detection of {{Late}}-Onset {{Sepsis}} in Very-Low Birth Weight Infants},
  abstract = {Objective: To study the diagnostic ability of RALIS (computerized mathematical algorithm and continuous monitoring device) to detect late onset sepsis among very low birth weight preterm neonates. Methods: Randomly chosen 24 very low birth weight infants with proven sepsis were compared to 22 infants without sepsis. The clinical parameters were retrospectively collected from the medical records. The ability of RALIS to detect late onset sepsis was calculated. Results: RALIS positively identified 23 of the 24 infants with sepsis (sensitivity 95.8\%). It indicated sepsis alert median 2.0 days earlier than clinical suspicion. A false positive alert was indicated in 23\% (5/22) infants. The specificity, and positive and negative predictive ability of RALIS were 77.3\%. 82.1\% and 94.4\%, respectively. Conclusions: RALIS may aid in the early diagnosis of late onset sepsis in very low birth weight preterm infants.},
  language = {en},
  number = {8},
  journal = {Indian Pediatr},
  doi = {10.1007/s13312-014-0469-x},
  author = {Gur, Ilan and Markel, Gal and Nave, Yaron and Vainshtein, Igor and Eisenkraft, Arik and Riskin, Arieh},
  month = aug,
  year = {2014},
  pages = {647-650}
}
@article{sullivanEarlyPulseOximetry2018,
  title = {Early {{Pulse Oximetry Data Improves Prediction}} of {{Death}} and {{Adverse Outcomes}} in a {{Two}}-{{Center Cohort}} of {{Very Low Birth Weight Infants}}},
  volume = {35},
  issn = {0735-1631, 1098-8785},
  abstract = {Background We previously showed, in a single-center study, that early heart rate (HR) characteristics predicted later adverse outcomes in very low birth weight (VLBW) infants. We sought to improve predictive models by adding oxygenation data and testing in a second neonatal intensive care unit (NICU).
Methods HR and oxygen saturation (SpO2) from the first 12 hours and first 7 days after birth were analyzed for 778 VLBW infants at two NICUs. Using multivariate logistic regression, clinical predictive scores were developed for death, severe intraventricular hemorrhage (sIVH), bronchopulmonary dysplasia (BPD), treated retinopathy of prematurity (tROP), late-onset septicemia (LOS), and necrotizing enterocolitis (NEC). Ten HR-SpO2 measures were analyzed, with first 12 hours data used for predicting death or sIVH and first 7 days for the other outcomes. HR-SpO2 models were combined with clinical models to develop a pulse oximetry predictive score (POPS). Net reclassification improvement (NRI) compared performance of POPS with the clinical predictive score.
Results Models using clinical or pulse oximetry variables alone performed well for each outcome. POPS performed better than clinical variables for predicting death, sIVH, and BPD (NRI {$>$} 0.5, p {$<$} 0.01), but not tROP, LOS, or NEC.
Conclusion Analysis of early HR-SpO2 characteristics adds to clinical risk factors to predict later adverse outcomes in VLBW infants.},
  language = {en},
  number = {13},
  journal = {Amer J Perinatol},
  doi = {10.1055/s-0038-1654712},
  author = {Sullivan, B. and {Wallman-Stokes}, A. and Isler, J. and Sahni, R. and Moorman, J. and Fairchild, K. and Lake, D.},
  month = nov,
  year = {2018},
  pages = {1331-1338}
}
@INPROCEEDINGS{honore2020icassp,
  author={A. {Honoré} and D. {Liu} and D. {Forsberg} and K. {Coste} and E. {Herlenius} and S. {Chatterjee} and M. {Skoglund}},
  booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Hidden Markov Models for Sepsis Detection in Preterm Infants}, 
  year={2020},
  volume={},
  number={},
  pages={1130-1134},}
@ARTICLE{hinton2012deepSpeech, 
author={G. {Hinton} and L. {Deng} and D. {Yu} and G. E. {Dahl} and A. {Mohamed} and N. {Jaitly} and A. {Senior} and V. {Vanhoucke} and P. {Nguyen} and T. N. {Sainath} and B. {Kingsbury}}, 
journal={IEEE Signal Processing Magazine}, 
title={Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups}, 
year={2012}, 
volume={29}, 
number={6}, 
pages={82-97}, 
keywords={feedforward neural nets;Gaussian processes;hidden Markov models;speech recognition;deep neural networks;acoustic modeling;speech recognition;hidden Markov models;temporal variability;Gaussian mixture models;feed-forward neural network;posterior probabilities;HMM states;Automatic speech recognition;Speech recognition;Hidden Markov models;Training;Gaussian processes;Acoustics;Neural networks;Data models}, 
doi={10.1109/MSP.2012.2205597}, 
ISSN={1053-5888}, 
month={Nov},}
@INPROCEEDINGS{li2013hybrid, 
author={L. {Li} and Y. {Zhao} and D. {Jiang} and Y. {Zhang} and F. {Wang} and I. {Gonzalez} and E. {Valentin} and H. {Sahli}}, 
booktitle={2013 Humaine Association Conference on Affective Computing and Intelligent Interaction}, 
title={Hybrid Deep Neural Network--Hidden Markov Model (DNN-HMM) Based Speech Emotion Recognition}, 
year={2013}, 
volume={}, 
number={}, 
pages={312-317}, 
keywords={Boltzmann machines;emotion recognition;hidden Markov models;speech recognition;unsupervised learning;hybrid deep neural network-hidden Markov model based speech emotion recognition;DNN-HMM;acoustic models;Gaussian mixture model based HMM;GMM-HMM;restricted Boltzmann Machine;unsupervised pre-training;RBM;discriminative pre-training;multilayer perceptrons HMM;Berlin database;eNTERFACE 05 database;Hidden Markov models;Emotion recognition;Training;Speech recognition;Speech;Databases;Neural networks}, 
doi={10.1109/ACII.2013.58}, 
ISSN={2156-8111}, 
month={Sep.},}
@inproceedings{Miao2013ImprovingLC,
  title={Improving low-resource CD-DNN-HMM using dropout and multilingual DNN training},
  author={Yajie Miao and Florian Metze},
  booktitle={INTERSPEECH},
  year={2013}
}
@article{liu2019lstmHmmHyb,
  author    = {Larkin Liu and
               Yu{-}Chung Lin and
               Joshua Reid},
  title     = {Improving the Performance of the {LSTM} and {HMM} Models via Hybridization},
  journal   = {CoRR},
  volume    = {abs/1907.04670},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.04670},
  archivePrefix = {arXiv},
  eprint    = {1907.04670},
  timestamp = {Wed, 17 Jul 2019 10:27:36 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1907-04670},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@ARTICLE{rabiner1986intro_hmm,
  author={L. {Rabiner} and B. {Juang}},
  journal={IEEE ASSP Magazine}, 
  title={An introduction to hidden Markov models}, 
  year={1986},
  volume={3},
  number={1},
  pages={4-16},}
@ARTICLE{rabiner1989tutorialhmm,
  author={L. R. {Rabiner}},
  journal={Proceedings of the IEEE}, 
  title={A tutorial on hidden Markov models and selected applications in speech recognition}, 
  year={1989},
  volume={77},
  number={2},
  pages={257-286},}
@article{dean1989reasoning,
author = {Dean, Thomas and Kanazawa, Keiji},
title = {A Model for Reasoning about Persistence and Causation},
year = {1989},
issue_date = {Aug. 1990},
publisher = {Blackwell Publishers, Inc.},
address = {USA},
volume = {5},
number = {3},
issn = {0824-7935},
url = {https://doi.org/10.1111/j.1467-8640.1989.tb00324.x},
doi = {10.1111/j.1467-8640.1989.tb00324.x},
journal = {Comput. Intell.},
month = dec,
pages = {142-150},
numpages = {9}
}
@article{chung2016hmRNN,
  author    = {Junyoung Chung and
               Sungjin Ahn and
               Yoshua Bengio},
  title     = {Hierarchical Multiscale Recurrent Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1609.01704},
  year      = {2016},
  url       = {http://arxiv.org/abs/1609.01704},
  archivePrefix = {arXiv},
  eprint    = {1609.01704},
  timestamp = {Mon, 13 Aug 2018 16:46:29 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/ChungAB16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{kim2019variational,
  title={Variational Temporal Abstraction},
  author={Kim, Taesup and Ahn, Sungjin and Bengio, Yoshua},
  booktitle={Advances in Neural Information Processing Systems},
  pages={11566--11575},
  year={2019}
}
@article{fine1998hierarchicalHMM,
author = {Fine, Shai and Singer, Yoram and Tishby, Naftali},
title = {The Hierarchical Hidden Markov Model: Analysis and Applications},
year = {1998},
issue_date = {July 1998},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {32},
number = {1},
issn = {0885-6125},
url = {https://doi.org/10.1023/A:1007469218079},
doi = {10.1023/A:1007469218079},
journal = {Mach. Learn.},
month = jul,
pages = {41-62},
numpages = {22},
keywords = {temporal pattern recognition, hidden variable models, cursive handwriting, statistical models}
}
@INPROCEEDINGS{ding2018reinforcementhmm, 
author={W. {Ding} and S. {Li} and H. {Qian} and Y. {Chen}}, 
booktitle={2018 IEEE International Conference on Robotics and Biomimetics (ROBIO)}, 
title={Hierarchical Reinforcement Learning Framework Towards Multi-Agent Navigation}, 
year={2018}, 
volume={}, 
number={}, 
pages={237-242}, 
keywords={adaptive control;collision avoidance;hidden Markov models;learning (artificial intelligence);multi-agent systems;navigation algorithm;multiagent environment;hierarchical framework;HMM;high-level architecture;agents perception;adaptive control action;low-level architecture;differential target-driven system;collision avoidance DRL system;hierarchical structure;collision avoidance tasks;hybrid DRL method;hierarchical reinforcement learning framework;multiagent navigation;hidden Markov model;learning efficiency;hierarchical navigation reinforcement network;DRL structure;deep reinforcement learning;Hidden Markov models;Task analysis;Navigation;Collision avoidance;Heuristic algorithms;Training;Reinforcement learning}, 
doi={10.1109/ROBIO.2018.8664803}, 
ISSN={}, 
month={Dec},}
@article{levine2018reinforcementReview,
  author    = {Sergey Levine},
  title     = {Reinforcement Learning and Control as Probabilistic Inference: Tutorial
               and Review},
  journal   = {CoRR},
  volume    = {abs/1805.00909},
  year      = {2018},
  url       = {http://arxiv.org/abs/1805.00909},
  archivePrefix = {arXiv},
  eprint    = {1805.00909},
  timestamp = {Mon, 13 Aug 2018 16:47:19 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1805-00909},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{khan2016survey,
  title={A survey on the state-of-the-art machine learning models in the context of NLP},
  author={Khan, Wahab and Daud, Ali and Nasir, Jamal A and Amjad, Tehmina},
  journal={Kuwait journal of Science},
  volume={43},
  number={4},
  year={2016}
}
@article{Hariyanti_2019,
        doi = {10.1088/1742-6596/1235/1/012013},
        url = {https://doi.org/10.1088%2F1742-6596%2F1235%2F1%2F012013},
        year = 2019,
        month = {jun},
        publisher = {{IOP} Publishing},
        volume = {1235},
        pages = {012013},
        author = {Trienani Hariyanti and Saori Aida and Hiroyuki Kameda},
        title = {Samawa Language Part of Speech Tagging with Probabilistic Approach: Comparison of Unigram, {HMM} and {TnT} Models},
        journal = {Journal of Physics: Conference Series}
}
@article{ASHWIN20172,
title = "Comparative secretome analysis of Colletotrichum falcatum identifies a cerato-platanin protein (EPL1) as a potential pathogen-associated molecular pattern (PAMP) inducing systemic resistance in sugarcane",
journal = "Journal of Proteomics",
volume = "169",
pages = "2 - 20",
year = "2017",
note = "2nd World Congress of the International Plant Proteomics Organization",
issn = "1874-3919",
doi = "https://doi.org/10.1016/j.jprot.2017.05.020",
url = "http://www.sciencedirect.com/science/article/pii/S1874391917301872",
author = "N.M.R. Ashwin and Leonard Barnabas and Amalraj Ramesh Sundar and Palaniyandi Malathi and Rasappa Viswanathan and Antonio Masi and Ganesh Kumar Agrawal and Randeep Rakwal",
keywords = ", Sugarcane, Secretome, Pathogen-associated molecular pattern (PAMP), Effector, Systemic resistance"
}
@INPROCEEDINGS{ren2015dna, 
author={S. {Ren} and V. {Sima} and Z. {Al-Ars}}, 
booktitle={2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
title={FPGA acceleration of the pair-HMMs forward algorithm for DNA sequence analysis}, 
year={2015}, 
volume={}, 
number={}, 
pages={1465-1470}, 
keywords={DNA;field programmable gate arrays;genomics;hidden Markov models;FPGA acceleration;pair-HMMs forward algorithm;DNA sequence analysis;next generation sequencing platforms;alignment probability;systolic array design;architectural features;small sequence size;on-chip buffering;Convey supercomputing platform;hidden Markov models;Yttrium;Arrays;DNA;Field programmable gate arrays;NGS;FPGA;pair-HMMs;hardware acceleration}, 
doi={10.1109/BIBM.2015.7359892}, 
ISSN={}, 
month={Nov},}
@ARTICLE{juang1986maximum, 
author={ {Bing-Hwang Juang} and S. {Levinson} and M. {Sondhi}}, 
journal={IEEE Transactions on Information Theory}, 
title={Maximum likelihood estimation for multivariate mixture observations of markov chains (Corresp.)}, 
year={1986}, 
volume={32}, 
number={2}, 
pages={307-309}, 
keywords={Markov processes;maximum-likelihood (ML) estimation}, 
doi={10.1109/TIT.1986.1057145}, 
ISSN={0018-9448}, 
month={March},}
@BOOK{gales2008application, 
author={M. {Gales} and S. {Young}}, 
booktitle={Application of Hidden Markov Models in Speech Recognition}, 
title={Application of Hidden Markov Models in Speech Recognition}, 
year={2008}, 
volume={}, 
number={}, 
pages={}, 
keywords={Electrical and Electronic Engineering;Machine Learning}, 
doi={}, 
ISSN={}, 
publisher={now}, 
isbn={9781601981202}, 
url={https://ieeexplore.ieee.org/document/8187420},}
@ARTICLE{chatterjee2011auditory, 
author={S. {Chatterjee} and W. B. {Kleijn}}, 
journal={IEEE Transactions on Audio, Speech, and Language Processing}, 
title={Auditory Model-Based Design and Optimization of Feature Vectors for Automatic Speech Recognition}, 
year={2011}, 
volume={19}, 
number={6}, 
pages={1813-1825}, 
keywords={feature extraction;speech recognition;feature vector optimization;automatic speech recognition;spectro-temporal auditory models;perturbation-based analysis;human auditory system;feature vector domain;static spectral auditory model;MFCC;dynamic spectro-temporal feature vector;ASR performance;Speech;Distortion measurement;Speech recognition;Computational modeling;Sensitivity;Acoustic distortion;Optimization;Auditory models;mel frequency cepstral coefficient (MFCC);speech recognition}, 
doi={10.1109/TASL.2010.2101597}, 
ISSN={1558-7916}, 
month={Aug},
}
@incollection{buys2018bridging,
title = {Bridging HMMs and RNNs through Architectural Transformations},
author = {Buys, Jan and Bisk, Yonatan and Choi, Yejin},
booktitle = {32nd Conference on Neural Information Processing Systems, IRASL workshop},
pages = {},
publisher={},
year = {2018}
}
@ARTICLE{vik2016rnnHmm,
       author = {{Krakovna}, Viktoriya and {Doshi-Velez}, Finale},
        title = "{Increasing the Interpretability of Recurrent Neural Networks Using Hidden Markov Models}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Machine Learning, Computer Science - Computation and Language, Computer Science - Machine Learning},
         year = "2016",
        month = "Jun",
          eid = {arXiv:1606.05320},
        pages = {arXiv:1606.05320},
archivePrefix = {arXiv},
       eprint = {1606.05320},
 primaryClass = {stat.ML},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016arXiv160605320K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@book{villani2008optimal,
  title={Optimal Transport: Old and New},
  author={Villani, C.},
  isbn={9783540710509},
  lccn={2008932183},
  series={Grundlehren der mathematischen Wissenschaften},
  url={https://books.google.se/books?id=hV8o5R7\_5tkC},
  year={2008},
  publisher={Springer Berlin Heidelberg}
}
@book{monge1781memoire,
  title={M{\'e}moire sur la th{\'e}orie des d{\'e}blais et des remblais},
  author={Monge, G.},
  url={https://books.google.se/books?id=IG7CGwAACAAJ},
  year={1781},
  publisher={De l'Imprimerie Royale}
}
@article{bousquet2017optimal,
    title={From optimal transport to generative modeling: the VEGAN cookbook},
    author={Olivier Bousquet and Sylvain Gelly and Ilya Tolstikhin and Carl-Johann Simon-Gabriel and Bernhard Schoelkopf},
    year={2017},
    journal={arXiv preprint arXiv:1705.07642},
    eprint={1705.07642},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}
@article{genevay2017gan,
    title={GAN and VAE from an Optimal Transport Point of View},
    author={Aude Genevay and Gabriel Peyré and Marco Cuturi},
    year={2017},
    eprint={1706.01807},
    journal={arXiv},
    primaryClass={stat.ML}
}
@article{patrini2018sinkhornVAE,
  author    = {Giorgio Patrini and
               Marcello Carioni and
               Patrick Forr{\'{e}} and
               Samarth Bhargav and
               Max Welling and
               Rianne van den Berg and
               Tim Genewein and
               Frank Nielsen},
  title     = {Sinkhorn AutoEncoders},
  journal   = {CoRR},
  volume    = {abs/1810.01118},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.01118},
  archivePrefix = {arXiv},
  eprint    = {1810.01118},
  timestamp = {Tue, 30 Oct 2018 10:49:09 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-01118.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{marvin2018crf,
  author    = {Marvin T. T. Teichmann and
               Roberto Cipolla},
  title     = {Convolutional CRFs for Semantic Segmentation},
  journal   = {CoRR},
  volume    = {abs/1805.04777},
  year      = {2018},
  url       = {http://arxiv.org/abs/1805.04777},
  archivePrefix = {arXiv},
  eprint    = {1805.04777},
  timestamp = {Mon, 13 Aug 2018 16:47:47 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1805-04777.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{ryan2018waveglow,
  author    = {Ryan Prenger and
               Rafael Valle and
               Bryan Catanzaro},
  title     = {WaveGlow: {A} Flow-based Generative Network for Speech Synthesis},
  journal   = {CoRR},
  volume    = {abs/1811.00002},
  year      = {2018},
  url       = {http://arxiv.org/abs/1811.00002},
  archivePrefix = {arXiv},
  eprint    = {1811.00002},
  timestamp = {Thu, 22 Nov 2018 17:58:30 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1811-00002.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{dilokthanakulmg2016gausian-vae,
  author    = {Nat Dilokthanakul and
               Pedro A. M. Mediano and
               Marta Garnelo and
               Matthew C. H. Lee and
               Hugh Salimbeni and
               Kai Arulkumaran and
               Murray Shanahan},
  title     = {Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders},
  journal   = {CoRR},
  volume    = {abs/1611.02648},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.02648},
  archivePrefix = {arXiv},
  eprint    = {1611.02648},
  timestamp = {Mon, 13 Aug 2018 16:46:56 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/DilokthanakulMG16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{CreswellB2017denoise,
  author    = {Antonia Creswell and
               Anil Anthony Bharath},
  title     = {Denoising Adversarial Autoencoders},
  journal   = {CoRR},
  volume    = {abs/1703.01220},
  year      = {2017},
  url       = {http://arxiv.org/abs/1703.01220},
  archivePrefix = {arXiv},
  eprint    = {1703.01220},
  timestamp = {Mon, 13 Aug 2018 16:48:18 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/CreswellB17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{ambrogioni2018wasserstein,
  title={Wasserstein Variational Inference},
  author={Ambrogioni, Luca and G{\"u}{\c{c}}l{\"u}, Umut and G{\"u}{\c{c}}l{\"u}t{\"u}rk, Ya{\u{g}}mur and Hinne, Max and van Gerven, Marcel AJ and Maris, Eric},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2473--2482},
  year={2018}
}
@article{TomczakW16vae-flow,
  author    = {Jakub M. Tomczak and
               Max Welling},
  title     = {Improving Variational Auto-Encoders using Householder Flow},
  journal   = {CoRR},
  volume    = {abs/1611.09630},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.09630},
  archivePrefix = {arXiv},
  eprint    = {1611.09630},
  timestamp = {Mon, 13 Aug 2018 16:46:13 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/TomczakW16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{ising1925,
author  ={Ernst Ising},
title   ={Beitrag zur Theorie des Ferromagnetismus},
journal ={Zeitschrift fur Physik},
pages={253-258}, 
biburl = {https://doi.org/10.1007/BF02980577},
year = {1925},
doi = {10.1007/BF02980577},
}
@article{PhysRev.65.117,
  title = {Crystal Statistics. I. A Two-Dimensional Model with an Order-Disorder Transition},
  author = {Onsager, Lars},
  journal = {Phys. Rev.},
  volume = {65},
  issue = {3-4},
  pages = {117--149},
  numpages = {0},
  year = {1944},
  month = {Feb},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRev.65.117},
  url = {https://link.aps.org/doi/10.1103/PhysRev.65.117}
}
We propose a fast non-iterative approximate inference method that uses a feedforward network to implement efficient exact sampling from the variational posterior. The model and this inference network are trained jointly by maximizing a variational lower bound on the log-likelihood. Although the naive estimator of the inference network gradient is too high-variance to be useful, we make it practical by applying several straightforward model-independent variance reduction techniques. Applying our approach to training sigmoid belief networks and deep autoregressive networks, we show that it outperforms the wake-sleep algorithm on MNIST an

@InProceedings{mnih14NVIL,
  title = 	 {Neural Variational Inference and Learning in Belief Networks},
  author = 	 {Andriy Mnih and Karol Gregor},
  booktitle = 	 {Proceedings of the 31st International Conference on Machine Learning},
  pages = 	 {1791--1799},
  year = 	 {2014},
  editor = 	 {Eric P. Xing and Tony Jebara},
  volume = 	 {32},
  number =       {2},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Bejing, China},
  month = 	 {22--24 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v32/mnih14.pdf},
  url = 	 {http://proceedings.mlr.press/v32/mnih14.html},
  abstract = 	 {Highly expressive directed latent variable models, such as sigmoid belief networks, are difficult to train on large datasets because exact inference in them is intractable and none of the approximate inference methods that have been applied to them scale well. We propose a fast non-iterative approximate inference method that uses a feedforward network to implement efficient exact sampling from the variational posterior. The model and this inference network are trained jointly by maximizing a variational lower bound on the log-likelihood. Although the naive estimator of the inference network gradient is too high-variance to be useful, we make it practical by applying several straightforward model-independent variance reduction techniques. Applying our approach to training sigmoid belief networks and deep autoregressive networks, we show that it outperforms the wake-sleep algorithm on MNIST and achieves state-of-the-art results on the Reuters RCV1 document dataset.}
}
@inproceedings{Li2020To,
title={To Relieve Your Headache of Training an MRF, Take AdVIL},
author={Chongxuan Li and Chao Du and Kun Xu and Max Welling and Jun Zhu and Bo Zhang},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=Sylgsn4Fvr}
}
@article{ranganath2015hierarchical,
    title={Hierarchical Variational Models},
    author={Rajesh Ranganath and Dustin Tran and David M. Blei},
    year={2015},
    eprint={1511.02386},
    journal={arXiv},
    primaryClass={stat.ML}
}
@article{adler2018banach,
  author    = {Jonas Adler and
               Sebastian Lunz},
  title     = {Banach Wasserstein {GAN}},
  journal   = {CoRR},
  volume    = {abs/1806.06621},
  year      = {2018},
  url       = {http://arxiv.org/abs/1806.06621},
  archivePrefix = {arXiv},
  eprint    = {1806.06621},
  timestamp = {Mon, 13 Aug 2018 16:46:52 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1806-06621.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{gemici2018primaldual,
    title={Primal-Dual Wasserstein GAN},
    author={Mevlana Gemici and Zeynep Akata and Max Welling},
    year={2018},
    eprint={1805.09575},
    journal={arXiv: 1805.09575},
    primaryClass={stat.ML}
}
@phdthesis{yuan2007thesis,
author = {Yuan Qi},
year = {2007},
month = {12},
pages = {},
address = {MIT, MA},
publisher = {Massachusetts Institute of Technology},
title = {Extending expectation propagation for graphical models}
}
@Article{kalman1960,
  author         = {R. E. Kalman},
  title          = {A New Approach to Linear Filtering And Prediction
                   Problems},
  journal        = {ASME Journal of Basic Engineering},
  year           = {1960}
}
@article{rauch1965,
	doi = {10.2514/3.3166},
	year = 1965,
	month = {aug},
	publisher = {American Institute of Aeronautics and Astronautics ({AIAA})},
	volume = {3},
	number = {8},
	pages = {1445--1450},
	author = {H. E. RAUCH and F. TUNG and C. T. STRIEBEL},
	title = {Maximum likelihood estimates of linear dynamic systems},
	journal = {{AIAA} Journal}
}
@incollection{wallach2019hybrid,
title = {Combining Generative and Discriminative Models for Hybrid Inference},
author = {Garcia Satorras, Victor and Akata, Zeynep and Welling, Max},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {13825--13835},
year = {2019},
publisher = {Curran Associates, Inc.}
}
@article{ljung1979,
  author={L. {Ljung}},
  journal={IEEE Transactions on Automatic Control}, 
  title={Asymptotic behavior of the extended Kalman filter as a parameter estimator for linear systems}, 
  year={1979},
  volume={24},
  number={1},
  pages={36-50},
  }
@inproceedings{wan2000unscented,
author={E. A. {Wan} and R. {Van Der Merwe}},
booktitle={Proceedings of the IEEE 2000 Adaptive Systems for Signal Processing, Communications, and Control Symposium (Cat. No.00EX373)},
title={The unscented Kalman filter for nonlinear estimation},
year={2000},
volume={},
number={},
pages={153-158},
}
@ARTICLE{richardson2001,
  author={T. J. {Richardson} and M. A. {Shokrollahi} and R. L. {Urbanke}},
  journal={IEEE Transactions on Information Theory}, 
  title={Design of capacity-approaching irregular low-density parity-check codes}, 
  year={2001},
  volume={47},
  number={2},
  pages={619-637},}
@book{richardson2008,
author = {Richardson, Tom and Urbanke, Ruediger},
title = {Modern Coding Theory},
year = {2008},
isbn = {0521852293},
publisher = {Cambridge University Press},
address = {USA}
}
@INPROCEEDINGS{liu2019alpha,
  author={D. {Liu} and N. N. {Moghadam} and L. K. {Rasmussen} and J. {Huang} and S. {Chatterjee}},
  booktitle={2019 IEEE Global Conference on Signal and Information Processing (GlobalSIP)}, 
  title={α Belief Propagation as Fully Factorized Approximation}, 
  year={2019},
  volume={},
  number={},
  pages={1-5},}
@inproceedings{xionggyr19one-shot,
  author    = {Hao Xiong and
               Yuanzhen Guo and
               Yibo Yang and
               Nicholas Ruozzi},
  editor    = {Amir Globerson and
               Ricardo Silva},
  title     = {One-Shot Inference in Markov Random Fields},
  booktitle = {Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial
               Intelligence, {UAI} 2019, Tel Aviv, Israel, July 22-25, 2019},
  pages     = {19},
  publisher = {{AUAI} Press},
  year      = {2019},
  url       = {http://auai.org/uai2019/proceedings/papers/19.pdf},
  timestamp = {Thu, 12 Mar 2020 11:31:12 +0100},
  biburl    = {https://dblp.org/rec/conf/uai/XiongGYR19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{domke2019provable,
  author    = {Justin Domke},
  title     = {Provable Smoothness Guarantees for Black-Box Variational Inference},
  journal   = {CoRR},
  volume    = {abs/1901.08431},
  year      = {2019},
  url       = {http://arxiv.org/abs/1901.08431},
  archivePrefix = {arXiv},
  eprint    = {1901.08431},
  timestamp = {Sat, 02 Feb 2019 16:56:00 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1901-08431.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@InProceedings{hernandez-lobatob16,
  title = 	 {Black-Box Alpha Divergence Minimization},
  author = 	 {Jose Hernandez-Lobato and Yingzhen Li and Mark Rowland and Thang Bui and Daniel Hernandez-Lobato and Richard Turner},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {1511--1520},
  year = 	 {2016},
  editor = 	 {Maria Florina Balcan and Kilian Q. Weinberger},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/hernandez-lobatob16.pdf},
  url = 	 {http://proceedings.mlr.press/v48/hernandez-lobatob16.html}
}
@article {Krzakala20935,
	author = {Krzakala, Florent and Moore, Cristopher and Mossel, Elchanan and Neeman, Joe and Sly, Allan and Zdeborov{\'a}, Lenka and Zhang, Pan},
	title = {Spectral redemption in clustering sparse networks},
	volume = {110},
	number = {52},
	pages = {20935--20940},
	year = {2013},
	doi = {10.1073/pnas.1312486110},
	publisher = {National Academy of Sciences},
	abstract = {Spectral algorithms are widely applied to data clustering problems, including finding communities or partitions in graphs and networks. We propose a way of encoding sparse data using a {\textquotedblleft}nonbacktracking{\textquotedblright} matrix, and show that the corresponding spectral algorithm performs optimally for some popular generative models, including the stochastic block model. This is in contrast with classical spectral algorithms, based on the adjacency matrix, random walk matrix, and graph Laplacian, which perform poorly in the sparse case, failing significantly above a recently discovered phase transition for the detectability of communities. Further support for the method is provided by experiments on real networks as well as by theoretical arguments and analogies from probability theory, statistical physics, and the theory of random matrices.Spectral algorithms are classic approaches to clustering and community detection in networks. However, for sparse networks the standard versions of these algorithms are suboptimal, in some cases completely failing to detect communities even when other algorithms such as belief propagation can do so. Here, we present a class of spectral algorithms based on a nonbacktracking walk on the directed edges of the graph. The spectrum of this operator is much better-behaved than that of the adjacency matrix or other commonly used matrices, maintaining a strong separation between the bulk eigenvalues and the eigenvalues relevant to community structure even in the sparse case. We show that our algorithm is optimal for graphs generated by the stochastic block model, detecting communities all of the way down to the theoretical limit. We also show the spectrum of the nonbacktracking operator for some real-world networks, illustrating its advantages over traditional spectral clustering.},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/110/52/20935},
	eprint = {https://www.pnas.org/content/110/52/20935.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}
@article{Viana_1985,
	doi = {10.1088/0022-3719/18/15/013},
	url = {https://doi.org/10.1088%2F0022-3719%2F18%2F15%2F013},
	year = 1985,
	month = {may},
	publisher = {{IOP} Publishing},
	volume = {18},
	number = {15},
	pages = {3037--3051},
	author = {L Viana and A J Bray},
	title = {Phase diagrams for dilute spin glasses},
	journal = {Journal of Physics C: Solid State Physics},
	abstract = {A generalised, dilute, infinite-ranged Ising spin-glass model is introduced and studied as a function of the concentration p and temperature T. The phase diagram is investigated and paramagnetic (P), ferromagnetic (F), spin glass (SG) and mixed (M) phases, meeting at a multicritical point (p*,T*), are identified. The P/F and P/SG phase boundaries are derived, and the F/M and M/SG boundaries are calculated close to (p*,T*). The condition for having a re-entrant spin-glass transition is derived. In non-zero magnetic field a p-dependent A-T instability line is obtained. The authors apply their results to the insulator EuxSr1-xS, it is predicted to exhibit re-entrant behaviour.}
}
@article{domke2013learn_inference,
  author={J. {Domke}},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Learning Graphical Model Parameters with Approximate Marginal Inference}, 
  year={2013},
  volume={35},
  number={10},
  pages={2454-2467},
  }
@incollection{NIPS2004_2741,
title = {Validity Estimates for Loopy Belief Propagation on Binary Real-world Networks},
author = {Mooij, Joris M and Hilbert J. Kappen},
booktitle = {Advances in Neural Information Processing Systems 17},
editor = {L. K. Saul and Y. Weiss and L. Bottou},
pages = {945--952},
year = {2005},
publisher = {MIT Press}
}
@article{PhysRevLett.35.1792,
  title = {Solvable Model of a Spin-Glass},
  author = {Sherrington, David and Kirkpatrick, Scott},
  journal = {Phys. Rev. Lett.},
  volume = {35},
  issue = {26},
  pages = {1792--1796},
  numpages = {0},
  year = {1975},
  month = {Dec},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.35.1792},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.35.1792}
}
@incollection{saadeNIPS2014,
title = {Spectral Clustering of graphs with the Bethe Hessian},
author = {Saade, Alaa and Krzakala, Florent and Zdeborov\'{a}, Lenka},
booktitle = {Advances in Neural Information Processing Systems 27},
editor = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
pages = {406--414},
year = {2014},
publisher = {Curran Associates, Inc.}
}
@article{Mooij_2005jsm,
	doi = {10.1088/1742-5468/2005/11/p11012},
	year = 2005,
	month = {nov},
	publisher = {{IOP} Publishing},
	volume = {2005},
	number = {11},
	pages = {P11012--P11012},
	author = {J M Mooij and H J Kappen},
	title = {On the properties of the Bethe approximation and loopy belief propagation on binary networks},
	journal = {Journal of Statistical Mechanics: Theory and Experiment},
	abstract = {We analyse the local stability of the high-temperature fixed point of the loopy belief
propagation (LBP) algorithm and how this relates to the properties of the Bethe free
energy which LBP tries to minimize. We focus on the case of binary networks with pairwise
interactions. In particular, we state sufficient conditions for convergence of LBP to a unique
fixed point and show that these are sharp for purely ferromagnetic interactions. In contrast,
in the purely antiferromagnetic case, the undamped parallel LBP algorithm is
suboptimal in the sense that the stability of the fixed point breaks down much earlier
than for damped or sequential LBP; we observe that the onset of instability for
the latter algorithms is related to the properties of the Bethe free energy. For
spin-glass interactions, damping LBP only helps slightly. We estimate analytically the
temperature at which the high-temperature LBP fixed point becomes unstable
for random graphs with arbitrary degree distributions and random interactions.}
}
@article{pelizzola2005cvm,
author = {Pelizzola, Alessandro},
year = {2005},
month = {09},
pages = {},
title = {Cluster Variation Method in Statistical Physics and Probabilistic Graphical Models},
volume = {38},
journal = {Journal of Physics A: Mathematical and General},
doi = {10.1088/0305-4470/38/33/R01}
}
@article{Zhou_2012,
   title={Region Graph Partition Function Expansion and Approximate Free Energy Landscapes: Theory and Some Numerical Results},
   volume={148},
   ISSN={1572-9613},
   DOI={10.1007/s10955-012-0555-1},
   number={3},
   journal={Journal of Statistical Physics},
   publisher={Springer Science and Business Media LLC},
   author={Zhou, Haijun and Wang, Chuang},
   year={2012},
   month={Aug},
   pages={513–547}
}
@article{Wang_2013,
	doi = {10.1088/1742-6596/473/1/012004},
	year = 2013,
	month = {dec},
	publisher = {{IOP} Publishing},
	volume = {473},
	pages = {012004},
	author = {Chuang Wang and Hai-Jun Zhou},
	title = {Simplifying generalized belief propagation on redundant region graphs},
	journal = {Journal of Physics: Conference Series},
	abstract = {The cluster variation method has been developed into a general theoretical framework for treating short-range correlations in many-body systems after it was first proposed by Kikuchi in 1951. On the numerical side, a message-passing approach called generalized belief propagation (GBP) was proposed by Yedidia, Freeman and Weiss about a decade ago as a way of computing the minimal value of the cluster variational free energy and the marginal distributions of clusters of variables. However the GBP equations are often redundant, and it is quite a non-trivial task to make the GBP iteration converges to a fixed point. These drawbacks hinder the application of the GBP approach to finite-dimensional frustrated and disordered systems.

In this work we report an alternative and simple derivation of the GBP equations starting from the partition function. Based on this derivation we propose a natural and systematic way of removing the redundance of the GBP equations. We apply the simplified generalized belief propagation (SGBP) equations to the two-dimensional and the three-dimensional ferromagnetic Ising model and Edwards-Anderson spin glass model. The numerical results confirm that the SGBP message-passing approach is able to achieve satisfactory performance on these model systems. We also suggest that a subset of the SGBP equations can be neglected in the numerical iteration process without affecting the final results.}
}
@article{Dom_nguez_2011,
   title={Characterizing and improving generalized belief propagation algorithms on the 2D Edwards–Anderson model},
   volume={2011},
   ISSN={1742-5468},
   DOI={10.1088/1742-5468/2011/12/p12007},
   number={12},
   journal={Journal of Statistical Mechanics: Theory and Experiment},
   publisher={IOP Publishing},
   author={Domínguez, Eduardo and Lage-Castellanos, Alejandro and Mulet, Roberto and Ricci-Tersenghi, Federico and Rizzo, Tommaso},
   year={2011},
   month={Dec},
   pages={P12007}
}
@article{Edwards_1975,
	doi = {10.1088/0305-4608/5/5/017},
	year = 1975,
	month = {may},
	publisher = {{IOP} Publishing},
	volume = {5},
	number = {5},
	pages = {965--974},
	author = {S F Edwards and P W Anderson},
	title = {Theory of spin glasses},
	journal = {Journal of Physics F: Metal Physics}	
}
@conference{carreiraperpinan2005contrastive,
  author = {Carreira-Perpinan, Miguel A. and Hinton, Geoffrey E.},
  editor = {Intelligence, Artificial and {Statistics, 2005}, Barbados},
  keywords = {Contrastive Divergence Learning On},
  title = {On Contrastive Divergence Learning},
  year = 2005
}
@article{zhirong2016JMLR,
  author  = {Zhirong Yang and Jukka Corander and Erkki Oja},
  title   = {Low-Rank Doubly Stochastic Matrix Decomposition for Cluster Analysis},
  journal = {Journal of Machine Learning Research},
  year    = {2016},
  volume  = {17},
  number  = {187},
  pages   = {1-25}  
}
@inproceedings{ICML-2012-KulisJ,
	author        = "Brian Kulis and Michael I. Jordan",
	booktitle     = "{Proceedings of the 29th International Conference on Machine Learning}",
	pages         = "148",
	title         = "{Revisiting k-means: New Algorithms via Bayesian Nonparametrics}",
	year          = 2012,
}
@incollection{NIPS2018_8013,
title = {Cluster Variational Approximations for Structure Learning of Continuous-Time Bayesian Networks from Incomplete Data},
author = {Linzner, Dominik and Koeppl, Heinz},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {7880--7890},
year = {2018},
publisher = {Curran Associates, Inc.}
}
@article{Pelizzola_2013,
   title={Variational approximations for stationary states of Ising-like models},
   volume={86},
   ISSN={1434-6036},
   url={http://dx.doi.org/10.1140/epjb/e2013-40031-6},
   DOI={10.1140/epjb/e2013-40031-6},
   number={4},
   journal={The European Physical Journal B},
   publisher={Springer Science and Business Media LLC},
   author={Pelizzola, Alessandro},
   year={2013},
   month={Mar}
}
@article{DBLP:journals/corr/HeZRS15,
  author    = {Kaiming He and
               Xiangyu Zhang and
               Shaoqing Ren and
               Jian Sun},
  title     = {Deep Residual Learning for Image Recognition},
  journal   = {CoRR},
  volume    = {abs/1512.03385},
  year      = {2015},
  url       = {http://arxiv.org/abs/1512.03385},
  archivePrefix = {arXiv},
  eprint    = {1512.03385},
  timestamp = {Wed, 17 Apr 2019 17:23:45 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/HeZRS15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
