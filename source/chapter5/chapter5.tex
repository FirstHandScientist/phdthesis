\chapter{Learning with inference}
In Part~\ref{part:inference}, we have discussed different ways for inference, assuming a probabilistic graphical model is given. As introduced in chapter~\ref{chapter2}, learning graphical models includes structure learning and parameter learning. We restrict our discussion to the parameter learning of graphical models. From this chapter onward, we would focus on answering the question of how to decide the parameters of a probabilistic graphical model.
In this chapter, we mainly discuss the learning of undirected graphical models and leave the learning of directed graphical models for the coming chapters.

As explained in section~\ref{chpt2:sec:learning-principles}, the most essential learning principle is \textit{maximal likelihood} that is derived from the minimization of KL-divergence. Since we do not have access to the true distribution $p^{\ast}(\bm{x})$, practical maximal likelihood learning is via tuning parameter $\bm{\theta}$ of model $p(\bm{x};\bm{\theta})$ using information of samples of $p^{\ast}(\bm{x})$, i.e. $\Dd = \left\{ \bm{x}^1, \bm{x}^2, \cdots, \bm{x}^{M}\right\}$.

We would begin with the explanation of why inference is required in learning, after which MRF learning via RENN is explained. Numerical comparisons with classic message-passing methods w.r.t. learning is demonstrated afterwords. Topics on learning with hidden variables are then discussed.

\section{Why does learning MRF requires inference?}
\label{chpt5:sec:learning-mrf}
Let us continue the discussion on learning in section~\ref{chpt2:sec:learning-principles}. Given a sample $\bm{x}$, the log-likelihood of this evidence is
\begin{align}\label{chpt5:eq:one-sample-likely}
  l(\bm{x};\bm{\theta}) = \log{\tilde{p}(\bm{x}; \bm{\theta})} - \log{Z(\bm{\theta})}.
\end{align}
where $\tilde{p}(\bm{x}; \bm{\theta}) =  \prod_{a\in \Ff} \phi_a(\bm{x}_a; \bm{\theta}_a)$. 
Without loss of generality, computing the gradient w.t.r. $\bm{\theta}_a$ gives
\begin{align}\label{chpt5:eq:one-sample-likely-grad}
  \pd{l(\bm{x};\bm{\theta})}{\bm{\theta}_a} = \pd{\log{{\phi_a}(\bm{x}_a; \bm{\theta}_a)}}{\bm{\theta}_a} - \EE_{p(\bm{x}_a; \bm{\theta})}\left[ \pd{\log{{\phi_a}(\bm{x}_a; \bm{\theta}_a)}}{\bm{\theta}_a} \right].
\end{align}
Applying all sample from dataset $\Dd$, i.e.
\begin{equation}
  \Ll(\bm{\theta}) = \frac{1}{|\Dd |} \sum_{\bm{x} \in \Dd} l(\bm{x}; \bm{\theta}),
\end{equation}
we have
\begin{equation}\label{chpt3:eq:likely-gradient-thetaa}
  \pd{\Ll(\bm{x};\bm{\theta})}{\bm{\theta}_a} = \frac{1}{|\Dd |} \sum_{\bm{x} \in \Dd}\pd{\log{{\phi_a}(\bm{x}_a; \bm{\theta}_a)}}{\bm{\theta}_a} - \EE_{p(\bm{x}_a; \bm{\theta})}\left[ \pd{\log{{\phi_a}(\bm{x}_a; \bm{\theta}_a)}}{\bm{\theta}_a} \right].
\end{equation}
In general, there is no closed-form solution in maximizing this log-likelihood $\Ll(\Dd; \bm{\theta})$. But it is intuitive to observe that the stationary point of ${\Ll(\bm{x};\bm{\theta})}$ is
\begin{equation}\label{chpt5:eq:stationary-point-likely}
  \frac{1}{|\Dd |} \sum_{x \in \Dd}\pd{\log{{\phi_a}(\bm{x}_a; \bm{\theta}_a)}}{\bm{\theta}_a} = \EE_{p(\bm{x}_a; \bm{\theta})}\left[ \pd{\log{{\phi_a}(\bm{x}_a; \bm{\theta}_a)}}{\bm{\theta}_a} \right].
\end{equation}
The left-hand-side of \eqref{chpt5:eq:stationary-point-likely} is empirical expectation w.r.t. to the gradient of potential function $\phi_a$, while on the right-hand-side of \eqref{chpt5:eq:stationary-point-likely} the expectation is computed by support of the marginal distribution $p(\bm{x}_a; \bm{\theta})$. This stationary point is intuitively telling us that the maximum likelihood estimation is trying to enforce the equality of empirical expectation of gradient with each $\bm{\theta}_a$ and the model's expectation of that.
\begin{remark}
  If we instantiate $p(\bm{x};\bm{\theta})$ from exponential family and it is canonically parameterized, its potential $\phi_a(\bm{x}_a; \bm{\theta})$ is log-linear w.r.t. the sufficient statistic. The corresponding equation of \eqref{chpt5:eq:stationary-point-likely} in this case is reduced into \textit{moment matching}.
\end{remark}

Apart from the classical techniques such as iterative proportional fitting \cite{eric10-708ipf}, the wide used approach is gradient decent method by using \eqref{chpt3:eq:likely-gradient-thetaa}, in maximizing $\Ll(\Dd; \bm{\theta})$. Either way, the inference on marginal distribution $\left\{ p(\bm{x}_a; \bm{\theta}), a \in \Ff \right\}$ is inevitable.

Take the gradient decent method as example, we need to use one of the inference approaches introduced in Part~\ref{part:inference} to approximate the marginals $\left\{ p(\bm{x}_a; \bm{\theta}), a \in \Ff \right\}$, then do update of the parameter
\begin{equation}
  \bm{\theta}_a \leftarrow \bm{\theta}_a + r \cdot \pd{\Ll(\bm{x};\bm{\theta})}{\bm{\theta}_a}, \forall~~a~\in~\Ff.
\end{equation}

\begin{remark}
  When the MRF scales up, the number of parameters, i.e. elements of $\bm{\theta}$, could be large. To reduce overfitting, a typical trick is to add a zero-mean Gaussian prior to $\bm{\theta}$ as \textit{regularization}. Then log-likelihood of evidence $\bm{x}$ becomes 
  \begin{equation}
    l(\bm{x};\bm{\theta}) = \log{\tilde{p}(\bm{x}; \bm{\theta})} - \log{Z(\bm{\theta})} - \frac{\norm{\bm{\theta}}^2}{2\sigma^2},
  \end{equation}
  where $\sigma^2$ is variance of the Gaussian prior, which is free to determine. The rest analysis fellows.
\end{remark}


\section{Model Learning with Inference of RENN}
\label{sec:model-learning-with-renn}
In section~\ref{sec:infer-renn}, we explained how to do inference with RENN when parameter $\bm{\theta}$ of $p(\bm{x}; \bm{\theta})$ is assumed to be known. As the continuation, we consider the case of learning parameter $\bm{\theta}$ of $p(\bm{x}; \bm{\theta})$ with inference by RENN here.

The likelihood minimization of \eqref{chpt5:eq:one-sample-likely} can be written as minimization of the negative log-likelihood
\begin{equation}\label{eq:maximizing-likelihood}
  \umin{\bm{\theta}}{ -\log{\tilde{p}(\bm{x}; \bm{\theta})} + \log{Z(\bm{\theta})}}.
\end{equation}
Instead of explicitly formulating gradients as in section~\ref{chpt5:sec:learning-mrf}, we can make use of the autodiff methods in PyTorch or Tensorflow and minimize the above cost directly. Note this does not mean that inference is not required. Instead, we need to deal with the intractability of $\log{Z(\bm{\theta})}$, which is expensive or prohibitive to solve \eqref{eq:maximizing-likelihood} directly. Approximation to $\log{Z(\bm{\theta})}$ requires inference, which can be done by RENN. 

The region-based free energy $F_R(\Bb;\bm{\theta})$ would exactly be negative partition function of $p(\bm{x};\bm{\theta})$, i.e. $-\log{Z(\bm{\theta})}$, if each belief is exactly the corresponding marginalization, $b_R(\bm{x}_R)=p(\bm{x}_{R})$, $\forall~R\in \Rr$. Otherwise, $F_R(\Bb^{\ast};\bm{\theta})$ can always be an approximation of $-\log{Z(\bm{\theta})}$, where $\Bb^{\ast}= \{b_R(\bm{x}_R; \bm{\omega}^{\ast}), R\in \Rr\}$ with $\bm{\omega}^{\ast}$ being the solution to problem \eqref{eq:infer-F-all-belief}.

Combining the model learning and inference, we have
\begin{align}\label{eq:learning-min-max}
  \min_{\bm{\theta}}\max_{\bm{\omega}} -\log{\tilde{p}(\bm{x}; \bm{\theta})} - F_R(\Bb; \bm{\theta}) 
  -\lambda \sum_{R\in \Rr \backslash \Rr_0} \sum_{R_p \in \Pp(R)}\!\!\!\!\!d( b_R, \!\!\!\!\! \sum_{\Ss(R_p)\backslash \Ss(R)}\!\!\!\!\! b_{R_p}(\bm{x}_{R_p}; \bm{\omega})).
\end{align}
Then the difficulty of computing $Z(\bm{\theta})$ is dealt with by joint learning of MRF and inference by RENN in \eqref{eq:learning-min-max}.

Learning MRF with RENN doesn't need iterative message propagation. Additionally, RENN can be implemented with modern toolboxes such as PyTorch or TensorFlow that enjoys the GPU computation capacity. Consequently, learning MRF with RENN can be much faster, which would be shown in the following section.


\section{Numerical Comparisons in MRF Learning}

\begin{table*}[t]
  \caption{NLL of grid graphical models training using different inference methods.}
  \label{tab:nll-training-grid-n5n10}
  \begin{center}
    \begin{small}
      \setlength\tabcolsep{2pt}
      \begin{tabular}{lcccccccc}
        % std=1.0
        \toprule
        $n$ & True & Exact & Mean Field & Loopy BP & Damped BP & GBP & Inference Net & RENN \\
        25  &  9.000  &  9.004  &  9.811  &  {9.139}  &  9.196  &  10.56  &  9.252  &  \textbf{9.048}  \\
        100 &  19.34  &  19.38  &  23.48  &  {19.92}  &  20.02  &  28.61  &  20.29  &  \textbf{19.76} \\
        225 &  63.90  &  63.97  &  69.01  &  66.44    &  66.25  &  92.62  &  68.15  &  \textbf{64.79}  \\
        \bottomrule
      \end{tabular}
      
    \end{small}
  \end{center}
\end{table*}


\begin{table*}[t]
  \caption{NLL of complete graphical models training using different inference methods.}
  \label{tab:nll-training-full-n3n4}
  \begin{center}
    \begin{small}
      \setlength\tabcolsep{2pt}
      \begin{tabular}{lcccccccc}
        \toprule
        % std=1.0
        $n$ & True & Exact & Mean Field & Loopy BP & Damped BP & GBP & Inference Net & RENN \\
        \midrule
        9  &  3.276  &  3.286  &  9.558  &  5.201  &  5.880  &  10.06  &  5.262  & \textbf{3.414}  \\
        16  &  4.883  &  4.934  &  28.74  &  13.64  &  18.95  &  24.45  &  13.77  &  \textbf{5.178}  \\
        
        \bottomrule
      \end{tabular}
      
    \end{small}
  \end{center}
\end{table*}

% \begin{table}[t]
%   \caption{Consumed time per epoch (unit second).}
%   \label{tab:time-training}
%   \begin{center}
%     \begin{small}
%       \begin{sc}
%         \begin{tabular}{lcc}
%           \toprule
%           $n$ & 25 & 100 \\
%           \midrule
%           Mean Field & 8.850 & 24.36 \\
%           Loopy BP &  41.58 & 94.97 \\
%           Damped BP & 35.85 & 156.8 \\
%           GBP &  1.466 & 9.245  \\
%           Inference Net & 1.466 & 5.314 \\
%           RENN &  2.329 & 10.98\\

%           \bottomrule
%         \end{tabular}
%       \end{sc}
%     \end{small}
%   \end{center}
%   \vskip -0.2in
% \end{table}

\begin{table}[h]
  
  \caption{Average consumed time per epoch (unit: second) for two training cases in Table~\ref{tab:nll-training-grid-n5n10} and \ref{tab:nll-training-full-n3n4}.}
  \label{tab:time-training}
  
  \begin{center}
    \begin{small}
      
      \begin{tabular}{lcc}
        \toprule
        {} & \begin{tabular}[x]{@{}c@{}} Grid Graph\\ $n=225$\end{tabular}
           & \begin{tabular}[x]{@{}c@{}} Complete Graph\\ $n=16$\end{tabular}  \\
        \midrule
        Mean Field & 40.09 & 2.499 \\
        Loopy BP &  335.1 & 12.40\\
        Damped BP & 525.1 & 5.431\\
        GBP &   12.37    & 1.387\\
        Inference Net & 19.49 & 0.882 \\
        RENN & 16.03  & 2.262\\

        \bottomrule
      \end{tabular}
      
    \end{small}
  \end{center}
  
\end{table}

To make the comparison more concrete, we do the learning of MRFs, i.e. learning the model parameter $\bm{\theta}$, via different inference methods discussed.


We do learning on two types of MRF graphs, grid (Table~\ref{tab:nll-training-grid-n5n10}) and complete graphs (Table~\ref{tab:nll-training-full-n3n4}). For both cases, we firstly sample the parameter set $\bm{\theta}^{\prime}$, then sample training and testing dataset from $p(\bm{x}; \bm{\theta}^{\prime})$. The true NLL of sampled datasets can be computed by $p(\bm{x}; \bm{\theta}^{\prime})$. We then train a randomly-initialized model with the obtained training dataset by using RENN (section~\ref{sec:model-learning-with-renn}). The trained model by RENN is evaluated with testing dataset w.r.t. NLL value, which is compared with trained models by other methods. We also include the comparison with exact inference where $Z(\bm{\theta})$ is computed exactly.
In the grid graphs, there are $4000$ samples for training and $1000$ for testing. In the complete graph case, there are $2000$ samples for training and $1000$ samples for testing. 

In the cases of grid graphs, the NLLs of most methods are close to the true NLL for small-sized graphs ($n=25,100$), with RENN reaching the lowest NNL. In the case of $n=255$, RENN outperforms all other methods significantly. Additionally, RENN is much faster. As shown in Table~\ref{tab:time-training}, loopy BP needs almost $335$s and damped BP needs about $525$s per epoch iteration, while RENN takes $16$s per epoch.
Neural network based methods parameterize the beliefs or marginal distributions and thus can do new inference estimations much faster when model parameter $\bm{\theta}$ is updated in optimization steps. % When the learning step of $\bm{\theta}$ is small, multiple steps of update of $\bm{\theta}$ can share one inference estimation of marginals without degenerating performance.



In the cases of complete graphs, the advantage of RENN is significant, compared with other methods as shown in Table~\ref{tab:nll-training-full-n3n4}. Other benchmark methods fall behind RENN by a distinct difference, given the size of graphs is relatively small. The results here actually agree with inference experiments shown in Table~\ref{tab:infer-full-n9} and \ref{tab:infer-full-n16}, where partition function estimations of other benchmark methods have much larger errors. As for the average time per epoch, neural network based models still are faster than iterative message-passing methods in general.


\section{Discussion on Learning}

The parameter learning becomes more challenge when variable vector $\bm{x}$ is \textit{partial observed}. Denote $\bm{x} = [\bm{x}_U, \bm{x}_O]$, where $\bm{x}_O$ is the observed part and $\bm{x}_U$ is unobserved part that is also known as latent variable. Then the joint distribution becomes $p(\bm{x}; \bm{\theta}) = p(\bm{x}_U, \bm{x}_O; \bm{\theta})$. Since it is only partially observed, we can not maximize the complete evidence log-likelihood as what we did in section~\ref{chpt5:sec:learning-mrf}. Instead, we marginalize the latent variable $\bm{x}_U$ out first
\begin{equation}\label{chpt2:eq:patial-likelihood}
  l(\bm{x}_O; \bm{\theta}) = \log{\sum_{\bm{x}_U}p(\bm{x}_U, \bm{x}_O; \bm{\theta})} = \log{Z(\bm{x}_O;\bm{\theta})} - \log{Z(\bm{\theta})},
\end{equation}
where $Z(\bm{x}_O;\bm{\theta}) = \sum_{\bm{x}_O}\tilde{p}(\bm{x}; \bm{\theta})$. In general, there are two categories of methods in dealing with learning in partial observed scenarios:
\begin{itemize}
\item Directly optimize $l(\bm{x}_O; \bm{\theta})$ w.r.t. $\bm{\theta}$. We need to be able to compute or estimate $l(\bm{x}_O; \bm{\theta})$. Essentially, it requires the gradient of $l(\bm{x}_O; \bm{\theta})$ which is a function of both $p(\bm{x}_a| \bm{x}_O; \bm{\theta})$ and $p(\bm{x}_a; \bm{\theta})$. Inference methods that we previously discussed, e.g., mean field, BP (and its variants), GBP, and RENN, would do the job.
\item Optimize the a lower bound of $l(\bm{x}_O; \bm{\theta})$. Methods such as (variational) EM, variational Bayes and variational auto-encoder (VAE) \cite{kingma2019vae} belongs to this category.
\end{itemize}

The first track is straightforward since it is to estimate the sub-partition function $Z(\bm{x}_O; \bm{\theta})$ and partition function $Z(\bm{x}; \bm{\theta})$ as done in section~\ref{sec:model-learning-with-renn} via RENN or other inference methods. Then do the maximization step sequentially. Note different from partition function $Z(\bm{\theta})$, the sub-partition function $Z(\bm{x}_O; \bm{\theta})$ is evidence-dependent, i.e. the inference has to be done per given evidence or sample  $\bm{x}_O$. 

Alternative, we may work with a lower bound of $l(\bm{x}_O; \bm{\theta})$ instead. In this track, the most classical framework is EM, which can be well explained by our target of maximizing $l(\bm{x}_O; \bm{\theta})$. The intuition can be obtained by firstly observing that
\begin{align}
  F_V(q(\bm{x}_U|\bm{x}_O)) & = \mathrm{KL}(q(\bm{x}_U|\bm{x}_O) || p(\bm{x}_U|\bm{x}_O; \bm{\theta})) - \log{Z(\bm{x}_O; \bm{\theta})} \nonumber \\
                            &= \EE_{q(\bm{x}_U|\bm{x}_O)}\left[  \log{\frac{q(\bm{x}_U|\bm{x}_O)}{{p}(\bm{x}_U| \bm{x}_O; \bm{\theta})}} \right] - \log{Z(\bm{x}_O;\bm{\theta})} \nonumber \\
                            &= \EE_{q(\bm{x}_U|\bm{x}_O)}\left[ \log{\frac{q(\bm{x}_U|\bm{x}_O)}{\tilde{p}(\bm{x}_U, \bm{x}_O; \bm{\theta})}} \right],
\end{align}
similar to the variational free energy \eqref{chpt2:eq:variational-free-energy}. Here $\bm{x}_O$ is the clamped value of observation. Since
\begin{equation}
  \EE_{q(\bm{x}_U|\bm{x}_O)}\left[ \log{\frac{q(\bm{x}_U|\bm{x}_O)}{\tilde{p}(\bm{x}_U, \bm{x}_O; \bm{\theta})}} \right] \geq - \log{Z(\bm{x}_O; \bm{\theta})},
\end{equation}
we have
\begin{align}
  l(\bm{x}_O; \bm{\theta}) &\geq \EE_{q(\bm{x}_U|\bm{x}_O)}\left[ \log{\frac{\tilde{p}(\bm{x}_U, \bm{x}_O; \bm{\theta})}{q(\bm{x}_U|\bm{x}_O)}} \right] - \log{Z(\bm{\theta})} \nonumber \\
                           & = \EE_{q(\bm{x}_U|\bm{x}_O)}\left[ \log{\frac{{p}(\bm{x}_U, \bm{x}_O; \bm{\theta})}{q(\bm{x}_U|\bm{x}_O)}} \right] \nonumber \\
                           & = \EE_{q(\bm{x}_U|\bm{x}_O)}\left[ \log{{p}(\bm{x}_U, \bm{x}_O; \bm{\theta})} \right] + H({q(\bm{x}_U|\bm{x}_O)}) \nonumber \\
                           & := F(q, \bm{\theta}),
\end{align}
which is exactly the lower bound \eqref{chpt2:eq:llk-lower-bound}. This give the intuition of viewing EM as a coordinate ascent method:
\begin{align}
  \mathrm{E~step:}~~~ q^{(t+1)} &= \uargmax{q}{F(q, \bm{\theta}^{(t)})}, \\
  \mathrm{M~step:}~~~\bm{\theta}^{(t+1)} &= \uargmax{\bm{\theta}}{F(q^{(t+1)}, \bm{\theta})}.
\end{align}

It is interesting to note that VAE maximizes the same lower bound of $l(\bm{x}_O;\bm{\theta})$ as EM and variational Bayes, by slightly rewriting the bound as
\begin{equation}
  F(q, \bm{\theta}) = \EE_{q(\bm{x}_U|\bm{x}_O)}\left[ \log{{p}(\bm{x}_O| \bm{x}_U; \bm{\theta})} + \log{{p}(\bm{x}_U; \bm{\theta})} \right] + H({q(\bm{x}_U|\bm{x}_O)}),
\end{equation}
with encoder $q(\bm{x}_U|\bm{x}_O)$ and decoder ${p}(\bm{x}_O| \bm{x}_U; \bm{\theta})$. The prior on latent variable $\bm{x}_U$, i.e. ${p}(\bm{x}_U; \bm{\theta})$ is usually assumed as a known Gaussian distribution.


\begin{remark}
  The lower bound $F(q, \bm{\theta})$ of $l(\bm{x}_O; \bm{\theta})$ can also be obtained from a importance sampling trick with Jensen's inequality.
  \begin{align}
    l(\bm{x}_O; \bm{\theta}) & = \log{\sum_{\bm{x}_U} {p}(\bm{x}_O, \bm{x}_U; \bm{\theta})} \nonumber \\
                             & = \log{\sum_{\bm{x}_U} {q(\bm{x}_U|\bm{x}_O)}\frac{{p}(\bm{x}_O, \bm{x}_U; \bm{\theta})}{q(\bm{x}_U|\bm{x}_O)} } \label{chpt5:eq:important-sampling-inituition}\\
                             & \geq \EE_{q(\bm{x}_U|\bm{x}_O)}\left[ \log{\frac{{p}(\bm{x}_O, \bm{x}_U; \bm{\theta})}{q(\bm{x}_U|\bm{x}_O)}}\right] \label{chpt5:eq:apply-jensen-ineq},
  \end{align}
  where \eqref{chpt5:eq:important-sampling-inituition} can be seen as an evaluation of importance sampling with proposal distribution $q$ and \eqref{chpt5:eq:apply-jensen-ineq} uses Jensen's inequality $\log{\EE_{p(x)}[f(x)] \geq \EE_{P(x)}[\log{f(x)}]}$.
\end{remark}

\begin{remark}
  Among all the methods discussed in the section, a key piece of the methods is computing or estimating $p(\bm{x}_U| \bm{x}_O; \bm{\theta})$. Apart from direct optimization of $l(\bm{x}_O;\bm{\theta})$ with exact inference methods and EM method that uses exact $q(\bm{x}_U| \bm{x}_O) = p(\bm{x}_U| \bm{x}_O; \bm{\theta})$, the rest employs an approximation of the posterior, i.e. $q(\bm{x}_U| \bm{x}_O) \approx p(\bm{x}_U| \bm{x}_O; \bm{\theta})$.
\end{remark}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../main"
%%% End:
