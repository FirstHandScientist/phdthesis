\chapter{Learning with Inference}
\label{chpt5:undirecteLearning}
In Part~\ref{part:background} and \ref{part:inference}, we have discussed different approaches to perform inference. But these discussions were based on one assumption: \textit{a probabilistic graphical model is known or given}. From this chapter onward, we mainly focus on topics in learning probabilistic graphical models.
As introduced in Chapter~\ref{chapter2}, learning of a graphical model includes structure learning and parameter learning. We restrict our discussion to parameter learning in graphical models. That is to say, answering the question of how to decide the parameters of a probabilistic graphical model.
In this chapter, we mainly discuss the learning of undirected graphical models and leave the learning of directed graphical models for the coming chapters.

As explained in Section~\ref{chpt2:sec:learning-principles}, one of the most essential learning principles is \textit{maximum likelihood} that is derived from the minimization of KL-divergence. Since we do not have access to the true distribution $p^{\ast}(\bm{x})$, practical maximum likelihood learning is done via tuning the parameter $\bm{\theta}$ of a model $p(\bm{x};\bm{\theta})$ using information from samples of $p^{\ast}(\bm{x})$, i.e. $\Dd = \left\{ \bm{x}^1, \bm{x}^2, \cdots, \bm{x}^{M}\right\}$.

We begin with the explanation of why inference is required in learning, and then, discuss MRF learning via RENN. Numerical comparisons with learned MRFs via classic message-passing methods are presented and discussed afterwards. We also include learning with hidden variables, an important topic in graphical model learning, in this chapter.



\section{Why does learning of an MRF require inference?}
\label{chpt5:sec:learning-mrf}
Let us continue the discussion on learning in Section~\ref{chpt2:sec:learning-principles}. Given a sample $\bm{x}$, the log-likelihood of this evidence is
\begin{align}\label{chpt5:eq:one-sample-likely}
  l(\bm{x};\bm{\theta}) = \log{\tilde{p}(\bm{x}; \bm{\theta})} - \log{Z(\bm{\theta})}.
\end{align}
where $\tilde{p}(\bm{x}; \bm{\theta}) =  \prod_{a\in \Ff} \phi_a(\bm{x}_a; \bm{\theta}_a)$. 
Without loss of generality, computing the gradient with regard to $\bm{\theta}_a$ gives
\begin{align}\label{chpt5:eq:one-sample-likely-grad}
  \pd{l(\bm{x};\bm{\theta})}{\bm{\theta}_a} = \pd{\log{{\phi_a}(\bm{x}_a; \bm{\theta}_a)}}{\bm{\theta}_a} - \EE_{p(\bm{x}_a; \bm{\theta})}\left[ \pd{\log{{\phi_a}(\bm{x}_a; \bm{\theta}_a)}}{\bm{\theta}_a} \right].
\end{align}
In \eqref{chpt5:eq:one-sample-likely-grad}, a computation trick is used, i.e., $\pd{f(x)}{x} = f(x) \pd{\log{f(x)}}{x}$ holds for a differentiable function $f(x)$. Applying all sample from dataset $\Dd$ to \eqref{chpt5:eq:one-sample-likely-grad} gives
\begin{equation}\label{chpt4:eq:dataset-likely}
  \Ll(\bm{\theta}) = \frac{1}{|\Dd |} \sum_{\bm{x} \in \Dd} l(\bm{x}; \bm{\theta}).
\end{equation}
We similarly have the gradient of $\Ll$ as
\begin{equation}\label{chpt5:eq:likely-gradient-thetaa}
  \pd{\Ll(\bm{x};\bm{\theta})}{\bm{\theta}_a} = \frac{1}{|\Dd |} \sum_{\bm{x} \in \Dd}\pd{\log{{\phi_a}(\bm{x}_a; \bm{\theta}_a)}}{\bm{\theta}_a} - \EE_{p(\bm{x}_a; \bm{\theta})}\left[ \pd{\log{{\phi_a}(\bm{x}_a; \bm{\theta}_a)}}{\bm{\theta}_a} \right].
\end{equation}
In general, there is no closed-form solution to the maximization problem of the log-likelihood $\Ll(\Dd; \bm{\theta})$. But it is intuitive to observe that the stationary points of ${\Ll(\bm{x};\bm{\theta})}$ satisfy
\begin{equation}\label{chpt5:eq:stationary-point-likely}
  \frac{1}{|\Dd |} \sum_{x \in \Dd}\pd{\log{{\phi_a}(\bm{x}_a; \bm{\theta}_a)}}{\bm{\theta}_a} = \EE_{p(\bm{x}_a; \bm{\theta})}\left[ \pd{\log{{\phi_a}(\bm{x}_a; \bm{\theta}_a)}}{\bm{\theta}_a} \right].
\end{equation}
The left-hand side of \eqref{chpt5:eq:stationary-point-likely} is empirical expectation with regard to the gradient of potential function $\phi_a$, while on the right-hand side of \eqref{chpt5:eq:stationary-point-likely} the expectation is computed by the marginal distribution $p(\bm{x}_a; \bm{\theta})$. This \eqref{chpt5:eq:stationary-point-likely} is intuitively telling us that the maximum likelihood estimation is trying to enforce the equality of empirical expectation of the gradient with regard to each $\bm{\theta}_a$ and the model's expectation of that.
\begin{remark}[Interpretation to learning with inference]
  If we instantiate $p(\bm{x};\bm{\theta})$ from exponential family and it is canonically parameterized, its potential $\phi_a(\bm{x}_a; \bm{\theta})$ is log-linear with regard to the sufficient statistics. The corresponding equation of \eqref{chpt5:eq:stationary-point-likely} in this case is reduced to \textit{moment matching}.

  For log-linear potential function $\phi_a$, $\forall ~a \in \Ff$, the first term of $l(\bm{x}; \bm{\theta})$, $\log{\tilde{p}(\bm{x};\bm{\theta})}$, is linear with regard to $\bm{\theta}$ and the second term, i.e. $-\log{Z(\bm{\theta})}$, is concave. Thus the log-likelihood is concave and there is a unique globally optimal log-likelihood value. But this does not necessarily mean that there is always a unique global optimal solution $\bm{\theta}^{\ast}$, since the parameterization of an MRF can be redundant. That is to say, in the case of over-parameterization of an MRF, there could exit multiple global optimal solutions after maximum likelihood learning.
  
\end{remark}

Apart from the classical techniques such as iterative proportional fitting \cite[Section~6.1]{wainwright2008graphical}\cite{eric10-708ipf}, a widely used approach is maximizing $\Ll(\Dd; \bm{\theta})$ by \eqref{chpt5:eq:likely-gradient-thetaa} via gradient decent method. In either way, the inference on marginal distribution $\left\{ p(\bm{x}_a; \bm{\theta}), a \in \Ff \right\}$ is inevitable.

Take the gradient decent method as an example: we need to use one of the inference approaches introduced in Part~\ref{part:background} and \ref{part:inference} to approximate the marginals $\left\{ p(\bm{x}_a; \bm{\theta}), a \in \Ff \right\}$ and then do the update of the parameters
\begin{equation}
  \bm{\theta}_a \leftarrow \bm{\theta}_a + r \cdot \pd{\Ll(\bm{x};\bm{\theta})}{\bm{\theta}_a}, ~\forall~a \in \Ff.
\end{equation}

\begin{remark}[Learning with a prior]
  When an MRF scales up, the number of parameters, i.e. elements of $\bm{\theta}$, could be large. If training data is limited, it may lead to overfitting. To address this issue, a typical trick is to use prior distributions to regulate the parameters. This principle comes from \textit{Bayesian learning} where priors that stand for domain knowledge and usually are subjective or biased, and likelihoods from observations form the objective function for learning together. Since the formulated joint likelihood of random variable $\bm{x}$ and parameter $\bm{\theta}$ is proportional to the posterior of $\bm{\theta}$ given the evidence, maximization of the joint likelihood is also termed as \textit{maximum a posterior}. Learning with priors offers benefits at least in two ways: i). When the size of training data is small, the biased prior from domain knowledge has a stronger impact on the choice of the model parameter $\bm{\theta}$ than the likelihood from observations. ii). As the number of training samples increases, the prior's impact gets weaker and the likelihood of observations dominates. Therefore, employing a prior can help to avoid overfitting in the case of insufficient training data, while still allowing objective information from observations to affect the choice of model parameters. The latter dominates as there are richer training samples. For a concrete example, let us apply a zero-mean Gaussian prior distribution to $\bm{\theta}$. Then, the log-likelihood of dataset $\Dd$ becomes 
  \begin{equation}
    \Ll(\bm{x};\bm{\theta}) = \sum_{\bm{x}\in \Dd}\left[ \log{\tilde{p}(\bm{x}; \bm{\theta})} - \log{Z(\bm{\theta})}  \right] - \frac{\norm{\bm{\theta}}^2}{2\sigma^2},
  \end{equation}
  where $\sigma^2$ is the variance of the Gaussian prior, which is free to determine and reflects our subjective prior knowledge. The rest of the analysis follows. Intuitively, the presence of the prior reduces the chance for making prediction dominated by a small number of observations. 
Alternatively, Laplacian prior is also commonly used, which brings a $\ell_1$-norm regularization over $\bm{\theta}$ and encourages sparsity.

\end{remark}


\section{MRF Learning with Inference of RENN}
\label{sec:model-learning-with-renn}
In Section~\ref{sec:infer-renn}, we explained how to do inference with RENN when parameter $\bm{\theta}$ of $p(\bm{x}; \bm{\theta})$ is assumed to be known. As the continuation, we consider the case of learning parameter $\bm{\theta}$ of MRF $p(\bm{x}; \bm{\theta})$ with inference by RENN here.

The likelihood maximization of \eqref{chpt5:eq:one-sample-likely} can be written as minimization of the negative log-likelihood (NLL)
\begin{equation}\label{eq:maximizing-likelihood}
  \umin{\bm{\theta}}{ -\log{\tilde{p}(\bm{x}; \bm{\theta})} + \log{Z(\bm{\theta})}}.
\end{equation}
As discussed in Section~\ref{chpt5:sec:learning-mrf}, inference is needed in this MRF learning problem due to the intractable partition function, whose gradient with regard to the model parameter is an expectation with regard to marginal distributions. RENN, similar to other inference methods, is able to infer both the marginal distributions (via beliefs) and partition functions (via region-based free energy) more efficiently and accurately, as explained in Chapter~\ref{chapter4}. Thus there are two alternative ways to employ the inference results of RENN for MRF learning:
\begin{itemize}
\item use belief $\left\{ b_{a}(\bm{x}_a) \right\}$ obtained from a RENN in place of marginal distributions $\left\{ p(\bm{x}_a; \bm{\theta}) \right\}$ to compute gradient of the likelihood $\Ll$ in \eqref{chpt5:eq:likely-gradient-thetaa}. Then employ a gradient-based optimization algorithm to optimize the model parameter $\bm{\theta}$;
  \item use the region-based free energy of a RENN in place of the intractable negative log-partition function $-\log{Z(\bm{\theta})}$. Make use of autodiff methods in morden framework such as PyTorch or Tensorflow for $\bm{\theta}$. Then do alternating optimization routine with respect to the model parameter $\bm{\theta}$ and RENN parameters under the same objective function.
\end{itemize}
The first alternative is straightforward by following the discussion in Section~\ref{chpt5:sec:learning-mrf}. We give a further explanation to the second alternative in this section.

The region-based free energy $F_R(\Bb;\bm{\theta})$ in \eqref{eq:def-region-free-energy} of Definition~\ref{def:region-free-energy} is the exactly negative partition function of $p(\bm{x};\bm{\theta})$, i.e. $-\log{Z(\bm{\theta})}$, if each belief is exactly the corresponding marginalization, $b_R(\bm{x}_R)=p(\bm{x}_{R})$, $\forall~R\in \Rr$. Otherwise, $F_R(\Bb^{\ast};\bm{\theta})$ can always be used as an approximation of $-\log{Z(\bm{\theta})}$, where $\Bb^{\ast}= \{b_R(\bm{x}_R; \bm{\omega}^{\ast}), R\in \Rr\}$ with $\bm{\omega}^{\ast}$ being a solution to problem \eqref{eq:infer-F-all-belief}.

Combining MRF learning and RENN inference, we have
\begin{align}\label{eq:learning-min-max}
  \min_{\bm{\theta}}\max_{\bm{\omega}} -\log{\tilde{p}(\bm{x}; \bm{\theta})} - F_R(\Bb; \bm{\theta}) 
  -\lambda \sum_{R\in \Rr \backslash \Rr_0} \sum_{R_p \in \Pp(R)}\!\!\!\!\!d( b_R, \!\!\!\!\! \sum_{\Ss(R_p)\backslash \Ss(R)}\!\!\!\!\! b_{R_p}(\bm{x}_{R_p}; \bm{\omega})).
\end{align}
Then the difficulty of computing $Z(\bm{\theta})$ is dealt with by joint learning of MRF and inference via RENN in \eqref{eq:learning-min-max}.

Learning MRF with RENN does not need an iterative message propagation. Additionally, RENN can be implemented with modern toolboxes and enjoys the GPU computation capacity. Consequently, learning MRF with RENN can be much faster, which will be shown in the following section.


\section{Numerical Comparisons in MRF Learning}

\begin{table*}[t]
  \caption{NLL of grid graphical models, training using different inference methods.}
  \label{tab:nll-training-grid-n5n10}
  \begin{center}
    \begin{small}
      \setlength\tabcolsep{2pt}
      \begin{tabular}{lcccccccc}
        % std=1.0
        \toprule
        $n$ & True & Exact & Mean Field & Loopy BP & Damped BP & GBP & Inference Net & RENN \\
        \midrule
        25  &  9.000  &  9.004  &  9.811  &  {9.139}  &  9.196  &  10.56  &  9.252  &  \textbf{9.048}  \\
        100 &  19.34  &  19.38  &  23.48  &  {19.92}  &  20.02  &  28.61  &  20.29  &  \textbf{19.76} \\
        225 &  63.90  &  63.97  &  69.01  &  66.44    &  66.25  &  92.62  &  68.15  &  \textbf{64.79}  \\
        \bottomrule
      \end{tabular}
      
    \end{small}
  \end{center}
\end{table*}


\begin{table*}[t]
  \caption{NLL of complete graphical models, training using different inference methods.}
  \label{tab:nll-training-full-n3n4}
  \begin{center}
    \begin{small}
      \setlength\tabcolsep{2pt}
      \begin{tabular}{lcccccccc}
        \toprule
        % std=1.0
        $n$ & True & Exact & Mean Field & Loopy BP & Damped BP & GBP & Inference Net & RENN \\
        \midrule
        9  &  3.276  &  3.286  &  9.558  &  5.201  &  5.880  &  10.06  &  5.262  & \textbf{3.414}  \\
        16  &  4.883  &  4.934  &  28.74  &  13.64  &  18.95  &  24.45  &  13.77  &  \textbf{5.178}  \\
        
        \bottomrule
      \end{tabular}
      
    \end{small}
  \end{center}
\end{table*}

% \begin{table}[t]
%   \caption{Consumed time per epoch (unit second).}
%   \label{tab:time-training}
%   \begin{center}
%     \begin{small}
%       \begin{sc}
%         \begin{tabular}{lcc}
%           \toprule
%           $n$ & 25 & 100 \\
%           \midrule
%           Mean Field & 8.850 & 24.36 \\
%           Loopy BP &  41.58 & 94.97 \\
%           Damped BP & 35.85 & 156.8 \\
%           GBP &  1.466 & 9.245  \\
%           Inference Net & 1.466 & 5.314 \\
%           RENN &  2.329 & 10.98\\

%           \bottomrule
%         \end{tabular}
%       \end{sc}
%     \end{small}
%   \end{center}
%   \vskip -0.2in
% \end{table}

\begin{table}[h]
  
  \caption{Average consumed time per epoch (unit: seconds) for two training cases in Table~\ref{tab:nll-training-grid-n5n10} and \ref{tab:nll-training-full-n3n4}.}
  \label{tab:time-training}
  
  \begin{center}
    \begin{small}
      
      \begin{tabular}{lcc}
        \toprule
        {} & \begin{tabular}[x]{@{}c@{}} Grid Graph\\ $n=225$\end{tabular}
           & \begin{tabular}[x]{@{}c@{}} Complete Graph\\ $n=16$\end{tabular}  \\
        \midrule
        Mean Field & 40.09 & 2.499 \\
        Loopy BP &  335.1 & 12.40\\
        Damped BP & 525.1 & 5.431\\
        GBP &   12.37    & 1.387\\
        Inference Net & 19.49 & 0.882 \\
        RENN & 16.03  & 2.262\\

        \bottomrule
      \end{tabular}
      
    \end{small}
  \end{center}
  
\end{table}

To make the comparison more concrete, we consider learning of MRFs (i.e. learning the model parameter $\bm{\theta}$), via different inference methods discussed in the following.


We perform learning on two types of MRF graphs, grid (Table~\ref{tab:nll-training-grid-n5n10}) and complete graphs (Table~\ref{tab:nll-training-full-n3n4}). For both cases, we firstly sample a parameter set $\bm{\theta}^{\prime}$, then sample training and testing datasets from $p(\bm{x}; \bm{\theta}^{\prime})$. The true NLL of sampled datasets can be computed by $p(\bm{x}; \bm{\theta}^{\prime})$. We then train a randomly-initialized MRF with the obtained training dataset by using RENN (Section~\ref{sec:model-learning-with-renn}). The trained MRF by RENN is evaluated with the testing dataset to compute the NLL value, which is compared with trained models by other methods. We also include the comparison with exact inference where $Z(\bm{\theta})$ is computed precisely in learning.
In the grid graphs, there are $4000$ samples for training and $1000$ for testing. In the complete graph case, there are $2000$ samples for training and $1000$ samples for testing. 

In the cases of grid graphs, the NLLs of most methods are close to the true NLL for small-sized graphs ($n=\left\{ 25, 100 \right\}$), with RENN reaching the lowest NNL. In the case of $n=255$, RENN outperforms all other methods significantly. In addition, RENN is much faster. As shown in Table~\ref{tab:time-training}, loopy BP needs almost $335$s and damped BP needs about $525$s per epoch iteration, while RENN takes $16$s per epoch.
Neural network based methods parameterize the beliefs or marginal distributions and thus can do new inference estimations much faster when the model parameter $\bm{\theta}$ is updated in the optimization steps. 



In the case of complete graphs, the advantage of RENN is significant, compared with other methods as shown in Table~\ref{tab:nll-training-full-n3n4}. The other benchmark methods fall behind RENN by a distinct difference, given the size of graphs is relatively small. The results here actually agree with inference experiments shown in Table~\ref{tab:infer-full-n9} and \ref{tab:infer-full-n16}, where partition function estimations of those methods have much larger errors. In terms of the average time per epoch, neural network based models still are faster than iterative message-passing methods in general.


\section{Further Discussion on Learning}\label{chpt5:sec:futher-dis-learning}

Parameter learning becomes more challenging when the variable vector $\bm{x}$ is only \textit{partially observed}. Denote $\bm{x} = [\bm{x}_U, \bm{x}_O]$, where $\bm{x}_O$ is the observed part and $\bm{x}_U$ is unobserved part that is also known as hidden or latent variable. Then the joint distribution becomes $p(\bm{x}; \bm{\theta}) = p(\bm{x}_U, \bm{x}_O; \bm{\theta})$. Since it is only partially observed, we cannot maximize the complete evidence log-likelihood as we did in Section~\ref{chpt5:sec:learning-mrf}. Instead, we marginalize out the latent variable $\bm{x}_U$ first
\begin{equation}\label{chpt5:eq:patial-likelihood}
  l(\bm{x}_O; \bm{\theta}) = \log{\sum_{\bm{x}_U}p(\bm{x}_U, \bm{x}_O; \bm{\theta})} = \log{Z(\bm{x}_O;\bm{\theta})} - \log{Z(\bm{\theta})},
\end{equation}
where $Z(\bm{x}_O;\bm{\theta}) = \sum_{\bm{x}_U}\tilde{p}(\bm{x}; \bm{\theta})$. In general, there are two categories of methods in dealing with learning in partial observed scenarios:
\begin{itemize}
\item Directly optimize $l(\bm{x}_O; \bm{\theta})$ with regard to $\bm{\theta}$. We need to be able to compute or estimate $l(\bm{x}_O; \bm{\theta})$. Essentially, it requires the gradient of $l(\bm{x}_O; \bm{\theta})$ which is a function of both $p(\bm{x}_a| \bm{x}_O; \bm{\theta})$ and $p(\bm{x}_a; \bm{\theta})$. Inference methods that we previously discussed, e.g., mean field, BP (and its variants), GBP, and RENN, can be used to do the inference on (conditional) marginal distributions.
\item Optimize a lower bound of $l(\bm{x}_O; \bm{\theta})$. Methods such as (variational) EM, variational Bayes and variational auto-encoder (VAE) \cite{kingma2019vae} belongs to this category.
\end{itemize}

The first track is straightforward since it is equivalent to estimating the sub-partition function $Z(\bm{x}_O; \bm{\theta})$ and partition function $Z(\bm{x}; \bm{\theta})$ as done in Section~\ref{sec:model-learning-with-renn} via RENN or other inference methods. Then, do the step of model parameter learning sequentially. Note different from the partition function $Z(\bm{\theta})$, the sub-partition function $Z(\bm{x}_O; \bm{\theta})$ is evidence-dependent, i.e., the inference has to be done per given evidence or sample  $\bm{x}_O$. 

Alternatively, we may work with a lower bound of $l(\bm{x}_O; \bm{\theta})$ instead. In this approach, the most classical framework is EM, which can be well explained by our target of maximizing $l(\bm{x}_O; \bm{\theta})$. Recap the variational free energy $F_V$ in \eqref{chpt2:eq:variational-free-energy} in Section~\ref{chpt2:sec:variational-inference}. The intuition can be obtained by substituting the posterior $q(\bm{x}_U|\bm{x}_O)$ into the variational free energy and observing that
\begin{align}
  F_V(q(\bm{x}_U|\bm{x}_O)) & = \mathrm{KL}(q(\bm{x}_U|\bm{x}_O) || p(\bm{x}_U|\bm{x}_O; \bm{\theta})) - \log{Z(\bm{x}_O; \bm{\theta})} \nonumber \\
                            &= \EE_{q(\bm{x}_U|\bm{x}_O)}\left[  \log{\frac{q(\bm{x}_U|\bm{x}_O)}{{p}(\bm{x}_U| \bm{x}_O; \bm{\theta})}} \right] - \log{Z(\bm{x}_O;\bm{\theta})} \nonumber \\
                            &= \EE_{q(\bm{x}_U|\bm{x}_O)}\left[ \log{\frac{q(\bm{x}_U|\bm{x}_O)}{\tilde{p}(\bm{x}_U, \bm{x}_O; \bm{\theta})}} \right],
\end{align}
where $\bm{x}_O$ is the clamped value of observation. Since KL divergence is non-negative,
\begin{equation}\label{chpt5:eq:partial-obs-ineq}
  \EE_{q(\bm{x}_U|\bm{x}_O)}\left[ \log{\frac{q(\bm{x}_U|\bm{x}_O)}{\tilde{p}(\bm{x}_U, \bm{x}_O; \bm{\theta})}} \right] \geq - \log{Z(\bm{x}_O; \bm{\theta})}.
\end{equation}
Substituting \eqref{chpt5:eq:partial-obs-ineq} into \eqref{chpt5:eq:patial-likelihood}, we have $l(\bm{x}_O; \bm{\theta})$ bounded from bellow
\begin{align}\label{chpt5:eq:lower-bound-F}
  l(\bm{x}_O; \bm{\theta}) &\geq \EE_{q(\bm{x}_U|\bm{x}_O)}\left[ \log{\frac{\tilde{p}(\bm{x}_U, \bm{x}_O; \bm{\theta})}{q(\bm{x}_U|\bm{x}_O)}} \right] - \log{Z(\bm{\theta})} \nonumber \\
                           & = \EE_{q(\bm{x}_U|\bm{x}_O)}\left[ \log{\frac{{p}(\bm{x}_U, \bm{x}_O; \bm{\theta})}{q(\bm{x}_U|\bm{x}_O)}} \right] \nonumber \\
                           & = \EE_{q(\bm{x}_U|\bm{x}_O)}\left[ \log{{p}(\bm{x}_U, \bm{x}_O; \bm{\theta})} \right] + H({q(\bm{x}_U|\bm{x}_O)}) \nonumber \\
                           & := F(q, \bm{\theta}),
\end{align}
which is exactly the lower bound \eqref{chpt2:eq:llk-lower-bound}. This gives the intuition of viewing EM as a coordinate ascent method:
\begin{align}
  \mathrm{E~step:}~~~ q^{(t+1)} &= \uargmax{q}{F(q, \bm{\theta}^{(t)})}, \\
  \mathrm{M~step:}~~~\bm{\theta}^{(t+1)} &= \uargmax{\bm{\theta}}{F(q^{(t+1)}, \bm{\theta})}.
\end{align}

It is interesting to note that VAE maximizes the same lower bound of $l(\bm{x}_O;\bm{\theta})$ as EM and variational Bayes, by slightly rewriting the bound as
\begin{equation}
  F(q, \bm{\theta}) = \EE_{q(\bm{x}_U|\bm{x}_O)}\left[ \log{{p}(\bm{x}_O| \bm{x}_U; \bm{\theta})} + \log{{p}(\bm{x}_U; \bm{\theta})} \right] + H({q(\bm{x}_U|\bm{x}_O)}),
\end{equation}
with encoder $q(\bm{x}_U|\bm{x}_O)$ and decoder ${p}(\bm{x}_O| \bm{x}_U; \bm{\theta})$. The prior on latent variable $\bm{x}_U$, i.e. ${p}(\bm{x}_U; \bm{\theta})$ is usually assumed as a known Gaussian distribution.


\begin{remark}
  The lower bound $F(q, \bm{\theta})$ of $l(\bm{x}_O; \bm{\theta})$ can also be obtained from a importance sampling trick with Jensen's inequality.
  \begin{align}
    l(\bm{x}_O; \bm{\theta}) & = \log{\sum_{\bm{x}_U} {p}(\bm{x}_O, \bm{x}_U; \bm{\theta})} \nonumber \\
                             & = \log{\sum_{\bm{x}_U} {q(\bm{x}_U|\bm{x}_O)}\frac{{p}(\bm{x}_O, \bm{x}_U; \bm{\theta})}{q(\bm{x}_U|\bm{x}_O)} } \label{chpt5:eq:important-sampling-inituition}\\
                             & \geq \EE_{q(\bm{x}_U|\bm{x}_O)}\left[ \log{\frac{{p}(\bm{x}_O, \bm{x}_U; \bm{\theta})}{q(\bm{x}_U|\bm{x}_O)}}\right] \label{chpt5:eq:apply-jensen-ineq},
  \end{align}
  where \eqref{chpt5:eq:important-sampling-inituition} can be seen as an evaluation of importance sampling with proposal distribution $q$ and \eqref{chpt5:eq:apply-jensen-ineq} uses Jensen's inequality $\log{\EE_{p(x)}[f(x)] \geq \EE_{p(x)}[\log{f(x)}]}$.
\end{remark}

\begin{remark}
  Among all the methods discussed in the section, a key piece of the methods is computing or estimating $p(\bm{x}_U| \bm{x}_O; \bm{\theta})$. Apart from direct optimization of $l(\bm{x}_O;\bm{\theta})$ with exact inference methods and EM method that uses exact $q(\bm{x}_U| \bm{x}_O) = p(\bm{x}_U| \bm{x}_O; \bm{\theta})$, the rest employs an approximation of the posterior, i.e. $q(\bm{x}_U| \bm{x}_O) \approx p(\bm{x}_U| \bm{x}_O; \bm{\theta})$.
\end{remark}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../main"
%%% End:
