\chapter{Closing Remarks}
\label{chapter9}

\section{Summary of the Dissertation}
In this dissertation, inference and learning problems in probabilistic graphical models were studied. As introduced in the background in Part~\ref{part:background}, inference and learning in probabilistic graphical models cover a wide range of topics. This dissertation mainly investigated selected topics in the subject that is broadly applicable to various problems and different disciplines. The dissertation started with the big picture of interesting problems in graphical models, and identified the related topics to which this dissertation related in Part~\ref{part:background} (Chapter~\ref{chapter2}).

In Part~\ref{part:inference} (Chapter~\ref{chapter3} and \ref{chapter4}), we focused on inference in Markov random fields (MRFs), i.e., undirected graphical models. Two meta algorithms (or models) were proposed in addressing the inference in MRFs. We firstly studied a $\alpha$-divergence minimization problem in an approximate way. The problem in connection with messages over an MRF or its factor graph showed an interesting intuition on belief propagation. The interesting connection gave rise to the $\alpha$ belief propagation ($\alpha$-BP) algorithm in general graphs. Based on the belief update rule of $\alpha$-BP, sufficient conditions were developed for the convergence of $\alpha$-BP to a fixed point for the binary-state space of each variable. The conditions allow us to check if running $\alpha$-BP for a given problem would be guaranteed to converge without actual implementation and executing the algorithm, which makes a difference in large inference problems. In pursuit of faster and more accurate inference algorithms, Chapter~\ref{chapter4} addressed the generic inference issue on a hyper-graph, i.e., a region graph. In a region graph, a node can be associated with multiple random variables and their potential preferences jointly. This change gives room for performance gains of inference, especially when there strongly conflicting potentials that form loops. Instead of developing an iterative message passing algorithm, a neural network was employed to formulate an optimization problem where the region-based free energy of the region graph was to be minimized, leading to the region-based energy neural network (RENN). RENN has the free energy minimization intuition as typical message passing methods, and also makes use of the efficient optimizers of modern neural network frameworks that offer highly-customizable models. With these advantages, RENN showed competitive performances in inference tasks of MRFs.

Part~\ref{part:learning} moved the focus to learning (parameter learning) of graphical models. In connecting to the inference part in Part~\ref{part:inference}, Part~\ref{part:learning} started with the role that inference plays in general model learning problems in graphic models in Chapter~\ref{chpt5:undirecteLearning}, which is also numerically demonstrated by the performance comparison of MRF learning with difference inference algorithms. The rest part extended the model learning discussion into a more general case, i.e., incomplete observations or the presence of hidden variables with a focus on directed graphical models. Learning with hidden variables was firstly treated within expectation maximization framework. The normalizing flows were brought into the directed models in order to gain the model expressivity. The model learning with hidden variables and normalizing flows was firstly addressed for independently-and-identically-distributed samples in Chapter~\ref{chpt6:em-flow}, and was then extended into a dynamic model (a hidden Markov model) in Chapter~\ref{chpt7:genhmm} for sequential or temporal signals. The applications of these developed models were demonstrated with image and speech data. The last chapter (Chapter~\ref{chapter8}) of this part considered a likelihood-free case where generative models were learned via entropy-regulated optimal transport (EOT) distance instead of maximum likelihood. Advantages and disadvantages of EOT in contrast with maximum likelihood were discussed, e.g., more freedom in choosing a generator for the model learning via EOT but the loss of likelihood tractability, etc.

\section{Open Directions}
Given the extensive work devoted to the study of probabilistic graphic models, there are still open problems that remain to be answered. Take the first topic we discussed, i.e., belief propagation methods and its variants including $\alpha$ belief propagation in this dissertation, the issue of convergence to a stable solution of these deterministic approximate inference methods still remains. Problems to which answers would be interesting include but not limited to: i) How fast a belief propagation method can be in converging to a stable solution (if it does converge)? ii) What is the quality of the converged solution? Although we usually enjoy the efficiency of these deterministic approximate inference methods empirically, theoretical insights would be very helpful in algorithm choice and system designs if the guideline is available (answers to the question i) here). Another critical issue is the quality of the returned solution by a selected belief propagation method. The current answer to this question mainly relies on empirical evaluations. One usually has to manually tune the selected algorithm in practice and try out the configurations. Thus, the error analyses of these deterministic approximate inference methods, though challenging, would be valuable in support of this family of methods. There are some works in attempting problem i) and ii) in restricted cases (see, e.g., Section~\ref{chpt3:sec:literature}), i.e., some special cases of graphical models, which are inspiring. 

The other important direction is the development of automated inference methods. It would be an interesting study to reduce (or avoid if possible) the manual tuning and adjustment, e.g., message update rules and scheduling methods. On this development track, jointly consider making use of modern hardware (e.g., high-capacity GPUs) in algorithm designs for scaled-up applications is also important since we face more high-dimensional data that comes in large volume nowadays. Work in Chapter~\ref{chapter4} made a step in this direction. RENN is able to answer multiple queries with one execution on an MRF. When partial random variables are instantiated as observed evidence, inference on posteriors (condition on evidence) becomes evidence-dependent for RENN and also belief propagation methods, i.e., queries associated with each evidence needs the execution or call of the inference algorithm once (see, e.g., Section~\ref{chpt5:sec:futher-dis-learning}). Modern variational inference such as VAEs (see, e.g., Section~\ref{chpt4:sec:literature}, Section~\ref{chpt8:sec:literature}) circumvents the evidence-dependent issue and presents a very interesting way of modeling by directly learning a posterior distribution of the hidden or latent variables of interests. In this case, answers to certain \textit{predefined} queries (associated with the directly-modeled posterior) can be efficiently computed. Queries falling out of the predefined class have to rely on Monte Carlo estimations or just can not be answered. Therefore, further development in this direction towards automated inference is anticipated.


New issues raise with model learning as well when modern neural networks are adopted into graphical models for higher modeling capacity. Classic statistic models with proper factorization usually allow factor-wise updates from closed-form optimization solutions. This property that factors can be decomposed in updates significantly simplifies the learning of directed graphical models, but is lost when customizable neural network models are incorporated. Meanwhile, the data amount is larger for models to extract useful information to adjust its parameters. The learning becomes more complex with the presence of hidden variables where inference algorithms are required as subroutines (see, e.g., Sectiton~\ref{chpt5:sec:futher-dis-learning}) to generate the information (usually a conditional distribution) missed from the (partial or incomplete) evidence.
Therefore, efficient learning algorithms are needed to incorporate the changes, i.e., the missing information or data, loss of decomposing factors in the objective function, and introduction of non-linear functional forms of modern neural network models. This motivates our consideration in three dimensions: i) modeling parameterization or structures (e.g., how to factorize a joint distribution and how to parameterize each factor); ii) the objective function that we aim to optimize (e.g., likelihood, variational bound of likelihood, pseudolikelihood, likelihood-free objectives, etc); iii) subroutine inference (e.g., exact inference, approximate inference). Although these three decisions affect each other, there is sufficient freedom and independence such that each one can be improved separately. Additionally, a learning algorithm design is also dependent on the model's final application scenarios or tasks. For instance, the difference between discriminative and generative learning (see, e.g., Remark~\ref{chpt7:rmk:generative-discriminative}, applications in Sections~\ref{chpt:7:sec:app-speech-recog} and \ref{chpt7:sec:app-sepsis}).


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../main"
%%% End:
