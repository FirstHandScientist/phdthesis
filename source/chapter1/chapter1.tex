\chapter{Introduction}
\label{chapter1}

\section{Motivations}
\label{section1.1}

Most tasks conducted by a person or an automated system requires a fundamental ability of \textit{reasoning}, which is always about reaching a conclusion based on available information. At times, a conclusion is not enough and it is also required to know how reliable the conclusion is. Take the coronavirus (COVID-19) that started from Wuhan in China at the end of 2019 as example, a doctor needs to check the information about a person to reason if the person is infected by the coronavirus. The relevant information includes symptoms such as fever, cough, breathing difficulties and probably kidney failure in severe cases. Even after the doctor has reached a conclusion of positive or negative infection of coronavirus for the person, a natural question is why and how \textit{confident} the diagnose is made.

Instead of randomly guessing, reasoning is to answer queries with preserving uncertainty, by making best of available information. Two fundamental problems are inevitable before a rational reasoning can be conducted. 
\begin{itemize}
\item How should we specify the relationship between a conclusion and the available information? In the coronavirus example, the counterpart question to answer is how the doctor should relate coronavirus infection with the symptoms. This step is called \textit{modeling} which represents a reasoning problem abstractly by specifying the relationship between known information and unknown parts, in preparation of answer query on it.
\item With the modeled representation, how a conclusion should be made? This process of reaching a answer to the query from an abstracted representation is called \textit{inference}. Assume the doctor knows that the candidate has fever and cough, but not any breathing problem, how likely does the candidate have be infected by coronavirus?
\end{itemize}

In the modeling and inference process, it is not likely that the very beginning assumptions are perfectly right about the truth and can be used for all instances of the same type of queries. On the contrary, we usually begin with simple (sometimes naive) assumptions of representation a problem, and come back to revise it later when more information or evidence is available, which is aligned with our learning process of new knowledge. In fact, this assumption-and-revision procedure can be more compact. Instead of fixing the model representation at the beginning when one might not be sure the correctness of the assumptions about the model, one can assume a set of models with the assumption that the 'correct' model is in this set. A typical strategy is to leave some freedom to the configuration of the model at the beginning. Each instantiating the configuration generates a model representation. By using available observations or information, the model is adjusted to be able to be best compatible with the observations. This procedure adds the following fundamental problem in reasoning.
\begin{itemize}
\item Instead of having a fixed model at the first step, a set of models (or hypothesis) is given. We then need to choose one model that best describes the available observations or information. This phase of choosing a model is called \textit{learning}. 
\end{itemize}
Afterwards, inference can be conducted on the learned model to answer queries.

With all the discussed problems above, modeling, inference and learning, our purpose is to carry out reasoning with being aware of how confident a conclusion or answer is. 
These problems can be treated nicely with probabilistic models via Bayesian logic. Probabilistic model is built on the fundamental calculus of probability theory that is natural to accommodate the \textit{uncertainty}, which is desired in reasoning. In additional, the probabilistic models offers rich space to modeling problems, where inference can be carried on either exactly or approximately. \textbf{More importantly, the modeling or modeling learning part is not necessarily coupled with inference algorithms}. This proper separation allows freedom that a certain family of general inference algorithms can be applies to a broad class probabilistic models. It offers the freedom of trying different model representation of a class without the need of replacing inference algorithm. 

\begin{example}\label{example-corona}
  Consider the coronavirus infection problem. Using probabilistic model, we are able to model the problem in a rigid way. Additionally, we can make query more formally in probabilistic model framework. Assume each symptom among fever, cough and breathing difficulty can take value from $\left\{ \mathrm{True}, \mathrm{False} \right\}$. Also the coronavirus infection is either true or false. One exemplified query can be
  \begin{equation}
    P(\mathrm{Infection} = \mathrm{True} | \mathrm{Fever} = \mathrm{True}, \mathrm{Cough} = \mathrm{False}, \mathrm{Breathing Difficulty}= \mathrm{True}), \nonumber
  \end{equation}
  which is asking how likely the patient is infected by coronavirus if symptoms of both fever and breathing difficulty are observed but no sight of cough symptom.

\end{example}

\begin{figure}[!t]
  % \captionsetup[subfigure]{justification=centering}
  \begin{subfigure}{.4\textwidth}
    \begin{tikzpicture}
      \tikzstyle{cnode} = [thick, draw=black, ellipse, inner sep = 1pt,  align=center]
      \tikzstyle{nnode} = [thick, rectangle, rounded corners = 0pt,draw,inner sep = 2pt]
      \node[cnode] (virus) at (0,-1) {Coronavirus};
      \node[cnode] (fever) at (-3,-3) {Fever};
      \node[cnode] (cough) at (-1,-3) {Cough};
      \node[cnode] (breath) at (2, -3) {Breath Difficulty};
      \draw[->] (virus) -- (fever);
      \draw[->] (virus) -- (cough);
      \draw[->] (virus) -- (breath);
    \end{tikzpicture}
    \caption{A directed graphical model for Example~\ref{example-corona}.}
    \label{fig:dag-coronavirus}
  \end{subfigure}\hspace{2.5cm}
  \begin{subfigure}{0.3\textwidth}
    \begin{tikzpicture}
      \tikzstyle{cnode} = [thick, draw=black, ellipse, inner sep = 2pt,  align=center]
      \node[cnode] (ts) at (0,0) {True Symbol};
      \node[cnode] (rs) at (0.5,-2) {Received Symbol};
      \node[cnode] (env) at (2,-1) {Environment};
      
      \draw[-] (ts) -- (rs);
      \draw[-] (ts) -- (env);
      \draw[-] (rs) -- (env);
      
    \end{tikzpicture}
    \caption{An undirected graphical model.}
    \label{fig:mrf-communication}
  \end{subfigure}
  \caption{Different perspectives on probabilistic graphic models. \ref{fig:dag-coronavirus} A toy Bayesian network. \ref{fig:mrf-communication} A toy Markov random field.}
  \hspace{1cm}
\end{figure}


Given the fact that probabilistic theory offers a rigid foundation to model and study the problems, which is used to answer query that we concern, it soon becomes intractable when dozens or hundreds of relevant attributes are joint considered. This can be exemplified by giving finer levels of each symptom in coronavirus infection, e.g. symptom fever is represented by the actual body temperature in integer instead of true-or-false binary state on the one hand. On the other hand, there could be more directly and indirectly relevant symptoms such as muscle pain and congestion. Together with the symptoms, the recent travel itinerary is also related. Additionally, season flu could also similarly bring up some symptoms listed above. 

Probabilistic graphical model offers a general framework to encode random variable dependency of a complex probabilistic distribution into a structured graph, which is a powerful tool to compactly model relevant attributes and facts of a complex problem or a system. As show in Figure~\ref{fig:dag-coronavirus} that represents the problem of Example~\ref{example-corona} into a directed graphical model (also called Bayesian network or generative model), the nodes (or vertexes) correspond to the variables that represents symptoms and infection state, whereas the edges between nodes correspond how one variable may influence others. In certain scenarios, it is natural to use directed graphical models (generative models) to represent that a observable variable is dependent on a latent variable that generates the observations. For instance, a noisy location sensor of a car keeps measuring the car's true location and reading noisy locations. 

In contract to the directed graphical model, there are more scenarios that the interaction between related random variables is not directional and an undirected edge is used, which leads to the undirected graphical model (or Markov random field, Section~\ref{chpt2:sec:graphical-models}) representation. Undirected graphical models are popularly used in computer vision, computational biology, digital communication, statistical physics, etc. Figure~\ref{fig:mrf-communication} illustrates an exemplified undirected graphical model in digital communication context. On the one hand, a receiver wants to know what is the true symbol by joint considering its communication environment and its received symbol. On the other hand, the symbol received by the receiver is jointly formulated by the true symbol and communication environment. The influence among them is apparently not directional since the impact along an edge can be bidirectional in this example.

Probabilistic graphical model offers a 'scientific language' to do reasoning with uncertainty within framework of probabilistic theory. It is usually a nature representation for a complex system or problem and offers straightforward abstraction. The compact representation of probabilistic graphical model bridges the joint distribution of a complex system, and its graphical abstraction that captures the statistic dependency reflecting our understanding of the system. The advantage of its representation power is one of the reasons that leads to its popularity in difference disciplines.

\begin{figure}[!t]
  \centering
  \begin{tikzpicture}
    \tikzstyle{cnode} = [thick, draw=black, ellipse, inner sep = 2pt,  align=center]
    \tikzstyle{fnode} = [thick, draw=black, ellipse, inner sep = 10pt,  align=center]
    
    \node[cnode] (infn) at (0,0) {Inference};
    \node[cnode] (lern) at (3,0) {Learning};
    
    \node[fnode, fit=(infn)(lern)] (box) {};
    \node[] at (1.4, -0.6) {Probabilistic Graphical Model};
    \draw[->,line width=0.2mm] (infn) to[out=15, in=165] (lern);
    \draw[->,line width=0.2mm] (lern) to[out=195, in=-15] (infn);
  \end{tikzpicture}
  \caption{Two key aspects in practical graphical models.}
  \label{fig:intro-pgm}
  \hspace{1cm}
\end{figure}

Probabilistic graphical model coupled with its underlining distribution is a powerful tool for effective inference, apart from its advantage of representation power. It allows to answer queries with the help of the underlining distribution when practical inference algorithms are provided, which meets our need of reasoning with uncertainty. In addition to inference, probabilistic graphical model also supports learning from data. With certain amount of data available, a probabilistic graphical model can be learned to explain the observed data better in addition to align with our own understanding of a domain. The learned graphical model can serve to do inference with higher confidence in return. A diagram is illustrated in Figure~\ref{fig:intro-pgm}. As would become clear in Part~\ref{part:learning}, the inference may be needed to carry out model learning as well, apart from the above mutual-benefiting interaction.



\section{Scope and Thesis Outline}\label{chpt1:sec:scope-outline}
We gives the intuition and motivation of probabilistic graphical model in last section, and the interaction between inference and learning in this framework. In this section, we would navigate further among the topics within this framework and states the ones that we would cover in the thesis.

Inference in probabilistic graphical model is about answering queries with help of its coupled distribution. These queries can be generally grouped into the following cases:
\begin{itemize}
\item Computing the likelihood of observed data or unobserved random variable.
\item Computing the marginals distribution over a particular subset of random variables.
\item Computing the conditional distribution a subset of variables given the configuration of another subset of variables. 
\item Computing the most likely configuration of (a subset of) variables.
\end{itemize}
\textit{The work of this thesis would be mainly related with the first three cases in inference part.}


Due to either the requirement of efficiency in solving a problem or the structure of the problem's graphical model representation, it is not always that case that the above inference problem can solved exactly. Thus inference methods can be divided into
\begin{itemize}
\item exact inference,
\item and approximate inference.
\end{itemize}
For a limited class of graphs, exact inference such as variable elimination and sum-product algorithm can be used. Some graphs also allow efficient inference after mild graph modification, e.g. junction tree method. However, the above listed inference problems can only be approximately solved in general graphs. The approximate inference family can be further divided into
\begin{itemize}
\item Stochastic Approximation (Particle methods),
\item Deterministic Approximation (Variational methods).
\end{itemize}
Stochastic approximation mainly relies on sample instances to answer queries, where a major challenge lies in how to obtain samples efficiently from a target distribution. Gibbs sampling, importance sampling and Markov Chain Monte Carlo are within this family. On the other hand, deterministic approximations refer to the variational methods, such as mean field approximation, loopy belief propagation, expectation propagation etc. \textit{From the perspectives of methodology, we related work in this thesis locates in the family of variational methods under approximate inference category.}


As for learning in probabilistic graphical models, there are two types of learning problems
\begin{itemize}
\item Structure learning,
\item Parameter learning.
\end{itemize}
The first case refer to determine the structure of a graphical model from observations, which is usually reduced to the problem of whether there should be an edge between a pair of nodes in the graphical model. The parameter learning is about to determine the parameter of a probabilistic graphical model (or its coupled distribution), with its graphical structure known. Structure learning is out of the scope of this thesis. The term \textit{learning} in thesis means the estimation of the parameters of a distribution. This problem is mainly discussed in Part~\ref{part:learning}, where we would touch the topics about learning in both undirected and directed graphical models. 


As for the learning techniques, the learning principles can categorized into
\begin{itemize}
\item Maximal likelihood estimation
\item Maximal conditional likelihood
\item Bayesian estimation
\item Maximal `Margin`
\item Maximum entropy
\end{itemize}
in general. We would touch techniques of the first four cases in Part~\ref{part:learning}.

\subsection{Publications}
The following works ware done during the author's PhD education.
\begin{enumerate}
\item Dong Liu and Lars K Rasmussen. Region-based energy nerual network for approximate
  inference. Under review, 2020. \\
  Code: \href{https://github.com/FirstHandScientist/renn}{https://github.com/FirstHandScientist/renn} (\textcolor{blue}{Private, publish later})
\item Dong Liu, Minh Th\`{a}nh Vu, Li Zuxing, and Lars K Rasmussen. $\alpha$ belief propagation for approximate beyeisian inference. Under review, 2020.\\
  Code: \href{https://github.com/FirstHandScientist/alpha-bp}{https://github.com/FirstHandScientist/alpha-bp}

\item Dong Liu, Antoine Honor{\'e}, Saikat Chatterjee, and Lars K Rasmussen. Powering hidden
  markov model by neural network based generative models. In the 24th European Conference on Artificial Intelligence (ECAI), 2020.\\
  Code: \href{https://github.com/FirstHandScientist/genhmm}{https://github.com/FirstHandScientist/genhmm}

\item Dong Liu, Minh Th\`{a}nh Vu, Saikat Chatterjee, and Lars K Rasmussen. Neural network based explicit mixture models and expectation-maximization based learning. In International Joint Conference on Neural Networks, Glasgow, UK, July 2020. \\
  Code: \href{https://github.com/FirstHandScientist/EM-GM}{https://github.com/FirstHandScientist/EM-GM}

\item Antoine Honor{\'e}, Dong Liu, David Forsberg, Karen Coste, Eric Herlenius, Saikat Chatterjee, and Mikael Skoglund. Hidden markov models for sepsis detection in preterm infants. In 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020.\\
  Code: \href{https://github.com/FirstHandScientist/genhmm}{https://github.com/FirstHandScientist/genhmm}

\item D. Liu, N. N. Moghadam, L. K. Rasmussen, J. Huang, and S. Chatterjee. $\alpha$ belief propagation as fully factorized approximation. In 2019 IEEE Global Conference on Signal and Information Processing (GlobalSIP), pages 1-5, 2019.\\
  Code: \href{https://github.com/FirstHandScientist/amp}{https://github.com/FirstHandScientist/amp}

\item Dong Liu, Minh Th\`{a}nh Vu, Saikat Chatterjee, and Lars K Rasmussen. Entropy-regularized
  optimal transport generative models. In 2019 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP), pages 3532-3536, 2019.\\
  Code: \href{https://github.com/FirstHandScientist/eotgms}{https://github.com/FirstHandScientist/eotgms}

\item Dong Liu, Baptiste Cavarec, Lars K Rasmussen, and Jing Yue. On dominant interference in random networks and communication reliability. In ICC 2019 IEEE International Conference on Communications (ICC), pages 1-7, 2019.
  
\item Dong Liu, Chao Wang, and Lars K Rasmussen. Discontinuous reception for multiple-beam communication. IEEE Access, pages 46931-46946, 2019.
  
\item Dong Liu, Viktoria Fodor, and Lars K Rasmussen. Will scale-free popularity develop scale-free geo-social networks? IEEE Transactions on Network Science and Engineering, 2018.
\end{enumerate}
The material presented in this thesis is based on the author's works which are partially published in 1, 2, 3, 4, 5, 6, 7 listed above. 
\subsection{Outline of Thesis}

\textcolor{blue}{To be written after main content is finished.}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../main"
%%% End:
