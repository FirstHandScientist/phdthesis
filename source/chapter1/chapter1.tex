\chapter{Introduction}
\label{chapter1}

Motivate the research in probabilistic models.

\section{Motivations}
\label{section1.1}

Most tasks conducted by a person or an automated system requires a fundamental ability of \textit{reasoning}, which is always about reaching a conclusion based on available information. At times, a conclusion is not enough and it is also required to know how reliable the conclusion is. Take the coronavirus that started from Wuhan, China at the end of 2019, as example, a doctor needs checks the information about a person to reason if the person is infected by the coronavirus. The relevant information includes symptoms such as fever, cough, breathing difficulties and probably kidney failure in severe cases. (\textcolor{red}{maybe a small figure of coronavirus here}.) Even after the doctor has concluded as positive or negative of ccoronavirus for the person, the natural question is why and how \textit{confident} the diagnose is.

Two problems are inevitable to conduct the reasoning:
\begin{itemize}
\item How should we specify the relationship between a conclusion and the available information? In the coronavirus example, the counterpart question to answer is how the doctor should relate coronavirus infection with the symptoms. This step is called \textit{modeling} which represents a reasoning problem abstractly by specifying the relationship between known information and unknown part, in preparation of answer query on it.
\item With the model, how a conclusion should be made? This process of reaching a answer to the query is called \textit{inference}. \textcolor{red}{something about coronavirus}
\end{itemize}

As times, a model is not totally fixed since one may not be sure the correctness of the assumptions about the model. A typical strategy is to leave some freedom in the configuration of the model at beginning. By using previous observations or information, the model is adjusted to be able to explain the observation in more reasonable way. This adds the following problem in reasoning:
\begin{itemize}
\item Instead of having a fixed model at the first step, a set of model is given. We then need to choose one model based previous observations to do inference in order to make conclusion or answer query. This phase of choosing a model is called \textit{learning}.
\end{itemize}

With all the discussed problems above, modelling, inference and learning, our purpose is to carry out reasoning with being aware of how confident a conclusion or answer is. 
These problems can be treated nicely with probabilistic models. Probabilistic models is built on the fundamental calculus of probability theory that is natural to accommodate the \textit{uncertainty}, which is desired in reasoning. In additional, the probabilistic models offers rich space to modeling problems, where inference can be carried on either exactly or approximately. \textbf{More importantly, the modeling or modeling learning part is not necessarily coupled with inference algorithm.} This proper separation allows free that a certain family of general inference algorithms can be applies to a broad class probabilistic models. It offers the freedom of trying different models of a class without the need of replacing inference algorithm. 

\begin{example}\label{example-corona}
  Consider the coronavirus infection problem. Using probabilistic model, we are able to model the problem in a rigid way. Additionally, we can make query more formally in probabilistic model framework. Assume each symptom among fever, cough and breathing difficulty can take value from $\left\{ \mathrm{True}, \mathrm{False} \right\}$. Also the coronavirus infection is either true or false. One exemplified query can be
  \begin{equation}
    P(\mathrm{Infection} = \mathrm{True} | \mathrm{Fever} = \mathrm{True}, \mathrm{Cough} = \mathrm{False}, \mathrm{Breathing Difficulty}= \mathrm{True}), \nonumber
  \end{equation}
  which is asking how likely the patient is infected by coronavirus if symptoms of both fever and breathing difficulty are observed but no sight of cough.

\end{example}

\begin{figure}[!t]
  % \captionsetup[subfigure]{justification=centering}
  \begin{subfigure}{.4\textwidth}
    \begin{tikzpicture}
      \tikzstyle{cnode} = [thick, draw=black, ellipse, inner sep = 1pt,  align=center]
      \tikzstyle{nnode} = [thick, rectangle, rounded corners = 0pt,draw,inner sep = 2pt]
      \node[cnode] (virus) at (0,-1) {Coronavirus};
      \node[cnode] (fever) at (-3,-3) {Fever};
      \node[cnode] (cough) at (-1,-3) {Cough};
      \node[cnode] (breath) at (2, -3) {Breath Difficulty};
      \draw[->] (virus) -- (fever);
      \draw[->] (virus) -- (cough);
      \draw[->] (virus) -- (breath);
    \end{tikzpicture}
    \caption{A directed graphical model for Example~\ref{example-corona}.}
    \label{fig:dag-coronavirus}
  \end{subfigure}\hspace{2.5cm}
  \begin{subfigure}{0.3\textwidth}
    \begin{tikzpicture}
      \tikzstyle{cnode} = [thick, draw=black, ellipse, inner sep = 2pt,  align=center]
      \node[cnode] (ts) at (0,0) {True Symbol};
      \node[cnode] (rs) at (0.5,-2) {Received Symbol};
      \node[cnode] (env) at (2,-1) {Environment};
      
      \draw[-] (ts) -- (rs);
      \draw[-] (ts) -- (env);
      \draw[-] (rs) -- (env);
      
    \end{tikzpicture}
    \caption{An undirected graphical model.}
    \label{fig:mrf-communication}
  \end{subfigure}
  \caption{Different perspectives on probabilistic graphic models. \ref{fig:dag-coronavirus} A toy Bayesian network. \ref{fig:mrf-communication} A toy Markov random field.}
  \hspace{1cm}
\end{figure}


Given the fact that probabilistic theory offers a rigid foundation to model and study the problems, which is used to answer query that we concerns, it soon becomes intractable when dozens or hundreds of relevant attributes are joint considered. This can be exemplified by giving finer levels of each symptom in coronavirus infection, e.g. symptom fever is represented by the actual body temperature in integer instead of true-or-false binary state on the one hand. On the other hand, there could be more directly and indirectly relevant symptoms such as muscle pain and congestion. Together with the symptoms, the recent travel itinerary is also related. Additionally, season flu could also similarly bring up some symptoms listed above. 

Probabilistic graphical model offers a general framework to encode random variable dependency of a complex probabilistic distribution into a structured graph, which is a powerful tool to compactly model relevant attributes and facts of a complex problem. As show in Figure~\ref{fig:dag-coronavirus} that represents the problem of Example~\ref{example-corona} into a directed graphical model (or Bayesian network), the nodes (or vertexes) corresponds to the variables that represents symptoms and infection state, whereas the edges between nodes correspond how one variable may influence others. In contract to the directed graphical model, there are more scenarios that the influence or interaction between related random variables is not directional, an undirected edge is used, which leads the undirected graphical model (or Markov random field, Section~\ref{sec:background-graphial-reppresentation}) representation. The undirected graphical models are popular used in computer vision \cite{}, computational biology \cite{}, digital communication \cite{}, statistical physics, etc. Figure~\ref{fig:mrf-communication} illustrates an exemplified undirected graphical model in digital communication context, where the receiver wants to guess what is the true symbol by joint considering the communication environment and the received symbol, whereas the symbol received by the receiver is jointly formulated by the true symbol and environment.

Probabilistic graphical model offers a 'scientific language' to do reasoning with uncertainty within framework of probabilistic theory. It is usually very nature to represent a complex system or problem by a probabilistic graphical model. The compact representation of probabilistic graphical model bridges the joint distribution of a complex system, and its graphical abstraction that captures the statistic dependency reflecting our understanding of the system. The advantage of its representation power is one of its popular application in difference disciplines.

\begin{figure}[!t]
  \centering
  \begin{tikzpicture}
    \tikzstyle{cnode} = [thick, draw=black, ellipse, inner sep = 2pt,  align=center]
    \tikzstyle{fnode} = [thick, draw=black, ellipse, inner sep = 10pt,  align=center]
    
    \node[cnode] (infn) at (0,0) {Inference};
    \node[cnode] (lern) at (3,0) {Learning};
    
    \node[fnode, fit=(infn)(lern)] (box) {};
    \node[] at (1.4, -0.6) {Probabilistic Graphical Model};
    \draw[->,line width=0.2mm] (infn) to[out=15, in=165] (lern);
    \draw[->,line width=0.2mm] (lern) to[out=195, in=-15] (infn);
  \end{tikzpicture}
  \caption{Two key aspects in practical graphical models.}
  \label{fig:intro-pgm}
  \hspace{1cm}
\end{figure}

Probabilistic graphical model coupled with its underlining distribution is also a powerful tool for effective inference, apart from its advantage of representation power. It allows to answer queries with regarding to the underlining distribution when practical inference algorithms are provided, which meets our need of reasoning with uncertainty. In addition to inference, probabilistic graphical model also supports learning from data. With certain amount of data available, a probabilistic graphical model can be learned to explain the observed data better in addition to align with our own understanding of a domain. The learned graphical model can serve to do inference with higher confidence in return. A diagram is illustrated in Figure~\ref{fig:intro-pgm}. As would become clear in Part~\ref{part:learning}, the inference may be needed to carry out model learning as well, apart from the above mutual-benefiting interaction.



\section{Scope and Thesis Outline}
We gives the intuition and motivation of probabilistic graphical model in last section, and the interaction between inference and learning in this framework. We would explore a bit further  and state what topics within this framework we would cover in this thesis.

Inference in probabilistic graphical model is about the answer queries with regarding to its  coupled distribution. These queries can be generally grouped into the following cases:
\begin{itemize}
\item Computing the likelihood of observed data or unobserved random variable.
\item Computing the marginals distribution over a particular subset of nodes.
\item Computing the conditional distribution a subset of nodes given the configuration of another subset of nodes. 
\item Computing the most likely configuration of (a subset of) nodes.
\end{itemize}
\textit{The work of this thesis would be mainly related with the first three cases in inference part.}


Due to either the requirement of efficiency in solving a problem or the graphical structure of the problem's representation, it is not always that case that the above inference problem can solved exactly. Thus inference methods can be divided into
\begin{itemize}
\item Exact inference,
\item Approximate inference.
\end{itemize}
For a limited class of graphs, exact inference such as variable elimination and sum-product algorithm can be used. Some graphs also allow efficient inference after mild modification, e.g. junction tree method. However, the above listed inference problems can only be approximately solved in general graphs. The approximation inference family can be further broken into
\begin{itemize}
\item Stochastic Approximation (Particle methods),
\item Deterministic Approximation (Variational methods).
\end{itemize}
Stochastic approximation mainly relies on samples to answer queries. Gibbs sampling, importance sampling and Markov Chain Monte Carlo are within this family. On the other hand, deterministic approximations refer to the variational methods, such as mean field approximation, loopy belief propagation, expectation propagation etc. \textit{From the perspectives of methodology, we related work in this thesis locates in the family of variational methods under approximate inference category.}


As for learning in probabilistic graphical models, there are two types of learning problems
\begin{itemize}
\item Structure learning,
\item Parameter learning.
\end{itemize}
The first case refer to determine the structure of a graphical model from observation of data, which is usually reduced to the problem of whether there should be an edge between a pair of nodes in the graphical model. The parameter learning is about to determine the parameter of a probabilistic graphical model (or its coupled distribution), with its graphical structure known. Structure learning is out of the scope of this thesis. The term \textit{learning} in thesis means the estimation of the parameters of a distribution. This problem is mainly discussed in Part~\ref{part:learning}, where we would touch the learning of both undirected and directed graphical models.


As for the learning techniques, the learning principles can categorized into
\begin{itemize}
\item Maximal likelihood estimation
\item Maximal conditional likelihood
\item Bayesian estimation
\item Maximal `Margin`
\item Maximum entropy
\end{itemize}
in general. We would touch and use techniques of the first four cases in Part~\ref{part:learning}.


We organizing the general Bayesian problems and/or methods as the follows, based on which we would discuss the part that this thesis would cover.

\begin{itemize}
\item Inference
  \begin{itemize}
  \item Exact Inference
    \begin{itemize}
    \item Variable elimination
    \item Belief probation
    \item Junction tree method
    \item \ldots
    \end{itemize}
    
  \item Approximate Inference
    \begin{itemize}
    \item Stochastic Approximation (Particle methods)
      \begin{itemize}
      \item Gibbs Samples
      \item Importance sampling
      \item Markov Chain Monte Carlo
      \item \ldots
      \end{itemize}
      
    \item \textbf{Deterministic Approximation (Variational methods)}
      \begin{itemize}
      \item Mean field
      \item Loopy belief propagation
      \item Expectation Propagation
      \item \ldots
      \end{itemize}
    \end{itemize}
  \end{itemize}
  
\item Learning
  \begin{itemize}
  \item Structure Learning
    \begin{itemize}
    \item Directed graphical models
    \item Undirected graphical models
    \end{itemize}
    
  \item \textbf{Parameter Learning}
    \begin{itemize}
    \item Directed graphical models
    \item Undirected graphical models
    \end{itemize}
  \end{itemize}
\end{itemize}

The main work of this thesis would about the deterministic approximate inference methods and parameter learning of graphical models, although the rest topics or issues would be referred in  context in the above category.






\subsection{Summary of Contributions}
\cite{liu2019dominant}

\subsection{Publications}

Tools (code) developed:


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../main"
%%% End:
