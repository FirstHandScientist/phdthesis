\chapter{Introduction}
\label{chapter1}

\section{Motivations}
\label{section1.1}

Most tasks conducted by a person or an automated system require a fundamental ability of \textit{reasoning}, which is always about reaching a conclusion based on available information. At times, a conclusion is not enough, and it is also required to know how reliable the conclusion is. Take the coronavirus (COVID-19) that started at the end of 2019 as an example: A doctor needs to check the information about a person to reason if the person is infected by the coronavirus. The relevant information includes symptoms such as fever, cough, breathing difficulties, and probably kidney failure in severe cases. Even after the doctor has reached a conclusion of positive or negative infection of coronavirus for the person, a natural question is why and how \textit{confidently} the diagnose is made.

Instead of randomly guessing, reasoning aims at answering queries with preserving uncertainty by making the best of the available information. Two fundamental problems are inevitable before rational reasoning can be conducted. 
\begin{itemize}
\item How should we specify the relationship between a conclusion and the available information? In the coronavirus example, the corresponding question to answer is how the doctor should relate the coronavirus infection to the symptoms. This step is called \textit{modeling} which represents a reasoning problem abstractly by specifying the relationship between known information and unknown parts, in preparation for answering queries.
\item With the modeled representation, how should a conclusion be drawn? This process of reaching an answer to the query from an abstracted representation is called \textit{inference}. Assume the doctor knows that the candidate has fever and cough, but not any breathing problem. Then, how likely is it that the candidate is infected with the coronavirus?
\end{itemize}

In the modeling and inference process, it is not likely that very early assumptions are perfectly right about the truth and can be used for all instances of the same type of queries. On the contrary, we usually begin with simple (sometimes naive) assumptions on the representation of a problem, and come back to revise them later when more information or evidence is available. This is aligned with our process of learning new knowledge. In fact, this assumption-and-revision procedure can be more compact. Instead of fixing the model representation at the beginning when one might not be sure about the correctness of the assumptions of the model, one can consider a set of models under the assumption that the 'correct' model is in this set. A typical strategy is to leave some freedom to the configuration of the model at the beginning. Each instantiation of the configuration generates a model representation. By using available observations or information, the model is adjusted to best match with the observations. This procedure adds the following fundamental problem in reasoning.
\begin{itemize}
\item Instead of having a fixed model at the first step, a set of models (or hypotheses) is given. We then need to choose one model that best describes the available observations or information. This phase of choosing a model is called \textit{learning}. 
\end{itemize}
Afterwards, inference can be conducted on the learned model to answer queries.

With all the discussed problems above, i.e., modeling, inference, and learning, our goal is to carry out reasoning while being aware of how confident a conclusion or answer is. 
These problems can be treated nicely with probabilistic models via Bayesian logic. A probabilistic model is built on the fundamental calculus of probability theory that provides a convenient framework for addressing \textit{uncertainty}, which is desired in reasoning. In addition, probabilistic models offer rich space for modeling problems, where inference can be carried out either exactly or approximately. \textbf{More importantly, the modeling or model learning part is not necessarily coupled with inference algorithms}. This proper separation provides the freedom that a certain family of general inference algorithms can be applied to a broad class of probabilistic models. It offers the freedom of trying different model representations of a class without the need for replacing the inference algorithm.

\begin{example}\label{example-corona}
  Consider the coronavirus infection problem. Using a probabilistic model, we are able to model the problem in a rigid way. Additionally, we can formalize a query in a probabilistic model framework. Assume each symptom among fever, cough, and breathing difficulty can take a value from $\left\{ \mathrm{True}, \mathrm{False} \right\}$. Also, the coronavirus infection is either true or false. One exemplified query can be
  \begin{equation}
    P(\mathrm{Infection} = \mathrm{True} | \mathrm{Fever} = \mathrm{True}, \mathrm{Cough} = \mathrm{False}, \mathrm{Breathing Difficulty}= \mathrm{True}), \nonumber
  \end{equation}
  which is asking how likely the patient is to be infected by the coronavirus if symptoms of both fever and breathing difficulty are observed but no cough symptoms.

\end{example}

\begin{figure}[!t]
  % \captionsetup[subfigure]{justification=centering}
  \begin{subfigure}{.4\textwidth}
    \begin{tikzpicture}
      \tikzstyle{cnode} = [thick, draw=black, ellipse, inner sep = 1pt,  align=center]
      \tikzstyle{nnode} = [thick, rectangle, rounded corners = 0pt,draw,inner sep = 2pt]
      \node[cnode] (virus) at (0,-1) {Coronavirus};
      \node[cnode] (fever) at (-3,-3) {Fever};
      \node[cnode] (cough) at (-1,-3) {Cough};
      \node[cnode] (breath) at (2, -3) {Breath Difficulty};
      \draw[->] (virus) -- (fever);
      \draw[->] (virus) -- (cough);
      \draw[->] (virus) -- (breath);
    \end{tikzpicture}
    \caption{}
    \label{fig:dag-coronavirus}
  \end{subfigure}\hspace{2.5cm}
  \begin{subfigure}{0.3\textwidth}
    \begin{tikzpicture}
      \tikzstyle{cnode} = [thick, draw=black, ellipse, inner sep = 2pt,  align=center]
      \node[cnode] (ts) at (0,0) {True Symbol};
      \node[cnode] (rs) at (0.5,-2) {Received Symbol};
      \node[cnode] (env) at (2,-1) {Environment};
      
      \draw[-] (ts) -- (rs);
      \draw[-] (ts) -- (env);
      \draw[-] (rs) -- (env);
      
    \end{tikzpicture}
    \caption{}
    \label{fig:mrf-communication}
  \end{subfigure}
  \caption{Different perspectives on probabilistic graphic models. \ref{fig:dag-coronavirus} A directed graphical model (Bayesian network) for Example~\ref{example-corona}. \ref{fig:mrf-communication} A toy undirected graphical model (Markov random field).}
\end{figure}


We have argued that probabilistic theory offers a rigid foundation to model and study the problems, which can be used to answer queries. Unfortunately, it soon becomes intractable when dozens or hundreds of relevant attributes are jointly considered. This can be exemplified by giving finer levels of each symptom in the coronavirus infection, e.g., symptom fever is represented by the actual body temperature in integer instead of true-or-false binary state on the one hand. On the other hand, there could be more directly and indirectly relevant symptoms such as muscle pain and congestion. Together with these symptoms, a recent travel itinerary may also be relevant. Additionally, season flu could also similarly bring up some symptoms listed above. 

A probabilistic graphical model offers a general framework to encode random variable dependency of a complex probabilistic distribution into a structured graph, which is a powerful tool to compactly model relevant attributes and facts of a complex problem or a system. As shown in Figure~\ref{fig:dag-coronavirus} that represents the problem of Example~\ref{example-corona} into a directed graphical model (also called Bayesian network or generative model), the nodes (or vertexes) correspond to the variables that represent symptoms and infection state, whereas the edges between nodes correspond how one variable may influence others. In certain scenarios, it is natural to use directed graphical models (generative models) to represent that an observable variable is depending on a latent variable that generates the observations. For instance, a noisy location sensor of a car keeps measuring the car's true location and reading noisy locations. 

In contrast to directed graphical models, there are scenarios for which the interaction between related random variables is not directional and an undirected edge is used, which leads to the undirected graphical model representation (or Markov random field, see Section~\ref{chpt2:sec:graphical-models}). Undirected graphical models are popularly used in computer vision, computational biology, digital communication, statistical physics, etc. Figure~\ref{fig:mrf-communication} illustrates an exemplified undirected graphical model in digital communication context. On the one hand, a receiver wants to know what is the true symbol by jointly considering its communication environment and its received symbol. On the other hand, the symbol received by the receiver is jointly formulated by the true symbol and communication environment. The influence among them is apparently not directional since the impact along an edge can be bidirectional in this example, e.g., in a sensor network where the environment triggers the sensor to send a message at the same time it affects the communication channel.

A probabilistic graphical model offers a 'scientific language' to do reasoning with uncertainty within the framework of probabilistic theory. It is usually a natural representation of a complex system or problem and offers straightforward abstraction. A probabilistic graphical model bridges the graphical representation of the complex system and the statistic dependency of attributes of the system.
% The compact representation of probabilistic graphical model bridges the joint distribution of a complex system, and its graphical abstraction that captures the statistic dependency reflecting our understanding of the system.
The advantage of its representation power is one of the reasons that lead to its popularity in different disciplines.

\begin{figure}[!t]
  \centering
  \begin{tikzpicture}
    \tikzstyle{cnode} = [thick, draw=black, ellipse, inner sep = 2pt,  align=center]
    \tikzstyle{fnode} = [thick, draw=black, ellipse, inner sep = 10pt,  align=center]
    
    \node[cnode] (infn) at (0,0) {Inference};
    \node[cnode] (lern) at (3,0) {Learning};
    
    \node[fnode, fit=(infn)(lern)] (box) {};
    \node[] at (1.4, -0.6) {Probabilistic Graphical Model};
    \draw[->,line width=0.2mm] (infn) to[out=15, in=165] (lern);
    \draw[->,line width=0.2mm] (lern) to[out=195, in=-15] (infn);
  \end{tikzpicture}
  \caption{Two key aspects in practical graphical models.}
  \label{fig:intro-pgm}
\end{figure}

A probabilistic graphical model coupled with its underlying distribution is a powerful tool for effective inference, apart from its advantage of representation power. It allows us to answer queries with the help of the underlining distribution when practical inference algorithms are provided, which meets our need for reasoning with uncertainty. In addition to inference, probabilistic graphical models also support learning from data. With a certain amount of data available, a probabilistic graphical model can be learned to explain the observed data better in addition to align with our subjective understanding of a domain, e.g., our prior knowledge. The learned graphical model can serve to do inference with higher confidence in return. The interplay between inference and learning in probabilistic graphical models is illustrated in Figure~\ref{fig:intro-pgm}. As would become clear in Part~\ref{part:learning}, the inference may be needed to carry out model learning as well, apart from the above mutual-benefiting interaction.



\section{Scope and the Dissertation Outline}\label{chpt1:sec:scope-outline}
We have given the intuition and motivation of probabilistic graphical models in the last section, and pointed out the interaction between inference and learning in this framework. In this section, we would categorize the main topics within the framework firstly. Then, we would outline the dissertation content.

Inference in probabilistic graphical models is about answering queries with help of its coupled distribution. These queries can be generally grouped into the following cases:
\begin{itemize}
\item Computing the likelihood of observed data or unobserved random variables.
\item Computing the marginal distribution over a particular subset of random variables.
\item Computing the conditional distribution of a subset of variables given the configuration of another subset of variables. 
\item Computing the most likely configuration of (a subset of) variables.
\end{itemize}
\textit{The work of this dissertation is mainly related to the first three cases mentioned above.}


Due to either the requirement of efficiency in solving a problem or the structure of the problem's graphical model representation, it is not always the case that the above inference problem can be solved exactly. Thus inference methods can be divided into
\begin{itemize}
\item Exact inference,
\item Approximate inference.
\end{itemize}
For a limited class of graphs, exact inference such as variable elimination and sum-product algorithm can be used. Some graphs also allow efficient inference after mild graph modification, e.g., junction tree method. However, the above-listed inference problems can only be approximately solved in general graphs. The approximate inference family can be further divided into
\begin{itemize}
\item Stochastic Approximation (Particle methods),
\item Deterministic Approximation (Variational methods).
\end{itemize}
Stochastic approximation mainly relies on sample instances to answer queries, where a major challenge lies in how to obtain samples efficiently from a target distribution. Gibbs sampling, importance sampling, and Markov Chain Monte Carlo are within this family. On the other hand, deterministic approximations refer to the variational methods, such as mean field approximation, loopy belief propagation, expectation propagation, etc. \textit{From the perspective of methodology, this dissertation is mainly related with approximate inference under the variational method category.}


As for learning in probabilistic graphical models, there are two types of learning problems
\begin{itemize}
\item Structure learning,
\item Parameter learning.
\end{itemize}
The first case refers to determining the structure of a graphical model from observations, which is usually reduced to the problem of whether there should be an edge between a pair of nodes in the graphical model. The parameter learning is about to determine the parameter of a probabilistic graphical model (or its coupled distribution), with its graphical structure known. Structure learning is out of the scope of this dissertation. The term \textit{learning} in the dissertation means the estimation of the parameters of the underlying distribution of a graphical model. This problem is mainly discussed in Part~\ref{part:learning}, where we would address learning in both undirected and directed graphical models. 


As for the learning techniques, the learning principles can be categorized into
\begin{itemize}
\item Maximum likelihood estimation
\item Maximum conditional likelihood
\item Bayesian estimation
\item Pseudolikelihood or maximum-margin
\item Maximum entropy
\end{itemize}
in general. \textit{Maximum likelihood} is one of the most fundamental principles in model learning. As a variant of maximum likelihood, maximum conditional likelihood is a useful objective for particular applications. Although the full Bayesian learning is usually too complex in practices, using Bayesian priors to regulate the learning of probabilistic graphical models can avoid some issues encountered by pure maximum likelihood learning.
Our work is related to the first three cases in Part~\ref{part:learning}. At times, likelihood based learning of certain classes of graphical models is too expensive, which may motivate the choice of pseudolikelihood or maximum-margin that formulate learning objective as a function of (conditional) marginals. Maximum entropy has a close relation to the representation of graphical models in exponential family form. But it is out of the scope of this dissertation. Nevertheless, at the end of Part~\ref{part:learning}, an alternative learning metric to maximum likelihood is introduced.

\subsection{Publications}
The following works ware done during the author's PhD education.
\begin{enumerate}
\item \label{pub-renn} \textbf{Dong Liu}, Ragnar Thobaben, and Lars K. Rasmussen. Region-based energy neural network for approximate inference. Preprint, 2020. \\
  Code: \href{https://github.com/FirstHandScientist/renn}{https://github.com/FirstHandScientist/renn}
\item \label{pub-alphabp-convergence}\textbf{Dong Liu}, Minh Th\`{a}nh Vu, Li Zuxing, and Lars K. Rasmussen. $\alpha$ belief propagation for approximate Bayesian inference. Preprint, 2020.\\
  Code: \href{https://github.com/FirstHandScientist/alpha-bp}{https://github.com/FirstHandScientist/alpha-bp}

\item Anubhab Ghosh, Antoine Honor{\'e}, \textbf{Dong Liu}, Gustav Eje Henter, and Saikat Chatterjee. Robust Classification using Hidden Markov Models and Mixtures of Normalizing Flows. Prepint, 2020.

\item Zuxing Li, Gyorgy Dan, and \textbf{Dong Liu}. A Game Theoretic Analysis of LQG Control under Adversarial Attack. Preprint, 2020.

\item Andrea Scotti, Nima N. Moghadam, \textbf{Dong Liu}, Karl Gafvert and Jinliang Huang. Graph Neural Networks for Massive MIMO Detection and Higher-Order QAM. Preprint, 2020.
  
\item \label{pub-hmm-flow} \textbf{Dong Liu}, Antoine Honor{\'e}, Saikat Chatterjee, and Lars K. Rasmussen. Powering hidden
  markov model by neural network based generative models. In the 24th European Conference on Artificial Intelligence (ECAI), 2020.\\
  Code: \href{https://github.com/FirstHandScientist/genhmm}{https://github.com/FirstHandScientist/genhmm}

\item \label{pub-em-flow} \textbf{Dong Liu}, Minh Th\`{a}nh Vu, Saikat Chatterjee, and Lars K. Rasmussen. Neural network based explicit mixture models and expectation-maximization based learning. In International Joint Conference on Neural Networks, Glasgow, UK, July 2020. \\
  Code: \href{https://github.com/FirstHandScientist/EM-GM}{https://github.com/FirstHandScientist/EM-GM}

\item \label{pub-hmm-sepsis} Antoine Honor{\'e}, \textbf{Dong Liu}, David Forsberg, Karen Coste, Eric Herlenius, Saikat Chatterjee, and Mikael Skoglund. Hidden markov models for sepsis detection in preterm infants. In 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020.\\
  Code: \href{https://github.com/FirstHandScientist/genhmm}{https://github.com/FirstHandScientist/genhmm}

\item \label{pub-abp} \textbf{Dong Liu}, Nima N. Moghadam, Lars K. Rasmussen, Jinliang Huang, and Saikat Chatterjee. $\alpha$ belief propagation as fully factorized approximation. In 2019 IEEE Global Conference on Signal and Information Processing (GlobalSIP), pages 1-5, 2019.\\
  Code: \href{https://github.com/FirstHandScientist/amp}{https://github.com/FirstHandScientist/amp}

\item \label{pub-eot-gm} \textbf{Dong Liu}, Minh Th\`{a}nh Vu, Saikat Chatterjee, and Lars K. Rasmussen. Entropy-regularized
  optimal transport generative models. In 2019 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP), pages 3532-3536, 2019.\\
  Code: \href{https://github.com/FirstHandScientist/eotgms}{https://github.com/FirstHandScientist/eotgms}

\item \textbf{Dong Liu}, Baptiste Cavarec, Lars K. Rasmussen, and Jing Yue. On dominant interference in random networks and communication reliability. In ICC 2019 IEEE International Conference on Communications (ICC), pages 1-7, 2019.
  
\item \textbf{Dong Liu}, Chao Wang, and Lars K. Rasmussen. Discontinuous reception for multiple-beam communication. IEEE Access, pages 46931-46946, 2019.
  
\item \textbf{Dong Liu}, Viktoria Fodor, and Lars K. Rasmussen. Will scale-free popularity develop scale-free geo-social networks? IEEE Transactions on Network Science and Engineering, 2018.
\end{enumerate}


The material presented in this dissertation is based on the author's works which are partially published in \ref{pub-renn}, \ref{pub-alphabp-convergence}, \ref{pub-hmm-flow}, \ref{pub-em-flow}, \ref{pub-hmm-sepsis},\ref{pub-abp}, \ref{pub-eot-gm} listed above.
\subsection{Outline of the Dissertation}

A general outline of the rest of the dissertation is as follows. Part~\ref{part:background} introduces the background knowledge and notations in support of the successive chapters. In what follows, the main ideas and techniques are present with two parts. Part~\ref{part:inference} consisting of Chapter~\ref{chapter3} and \ref{chapter4} focuses on inference techniques. Part~\ref{part:learning} consisting of Chapter~\ref{chpt5:undirecteLearning}, \ref{chpt6:em-flow}, \ref{chpt7:genhmm}, and \ref{chapter8}, covers mainly the learning topics. Chapter~\ref{chapter9} discusses some future directions along this line of research and closes the dissertation. We summarize the content more specifically as follows.

The background part (Part~\ref{part:background}) first reviews the basics of probabilistic graphical models e.g., directed and undirected graphical models (also known as Bayesian network and Markov random fields), that are the fundamental frameworks for the content in this dissertation. Then, the inference tasks and basic message-passing algorithms (e.g., mean field and belief propagation) are reviewed. To give more intuition on approximate inference, the connection to variational free energy is discussed. The background part ends with the discussion on the essential learning principles of probabilistic graphical models.

Part~\ref{part:inference} focuses on the development of inference in Markov random fields. Firstly, an alternative view of belief propagation is developed in contract to free energy minimization in Chapter~\ref{chapter3}. A variant belief propagation algorithm is developed based on the alternative understanding. The convergence conditions of the proposed algorithm are studied and offered in the binary-state case of each variable node in pairwise Markov random fields. Secondly, in Chapter~\ref{chapter4}, this part then takes the path of addressing inference approximately by optimizing the region-based free energy which generalizes the Bethe free energy corresponding to the belief propagation algorithm. The developed method incorporates deep neural networks in solving the optimization problem of region-based free energy.

Part~\ref{part:learning} moves the focus to learning of probabilistic graphical models. This part begins with the explanation of the role of inference in addressing learning problems of probabilistic graphical models in Chapter~\ref{chpt5:undirecteLearning} and shows numerically the impact of different inference algorithms on the learning of Markov random fields. The rest content of this part is mainly on model learning with partial observations, i.e., there is the presence of hidden variables, which is addressed in expectation maximization framework. Chapter~\ref{chpt6:em-flow} explores the more expressive probabilistic graphical models, i.e., finite mixtures of models with normalizing flows that are likelihood-tractable, and learning of these models. Chapter~\ref{chpt7:genhmm} extends the idea of graphical models with flows of Chapter~\ref{chpt6:em-flow} into a special case of dynamic Bayesian network, i.e., the hidden Markov model. The model learning is addressed with expectation maximization and applications of this model is demonstrated. With the idea that more general non-linear mapping can induce more expressive probabilistic distributions, Chapter~\ref{chapter8} studies the learning of likelihood-free generative models with a different metric, i.e., optimal transport. The learning of this kind of model is developed in the way of both direct optimal transport distance minimization and generative adversarial training.

Chapter~\ref{chapter9} concludes the dissertation with closing remarks.
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../main"
%%% End:
