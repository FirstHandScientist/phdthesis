\chapter{An Implicit Probabilistic Generative Model}
\label{chapter8}
\graphicspath{{source/chapter8/}}
In previous chapters, we discussed learning in both undirected (Chapter~\ref{chpt5:undirecteLearning}) and directed (Chapter~\ref{chpt6:em-flow} and \ref{chpt7:genhmm}) graphical models. The model learning of previous discussions has been mainly under the principle of maximum likelihood, which includes the cases of MRF learning with approximate inference and the likelihood variational lower bound such as \eqref{chpt5:eq:lower-bound-F}. With maximum likelihood learning, if exactly likelihood is not available, its variational or approximate value is used as the objective function.

In this chapter, we introduce a way of learning a generative model that does not use maximum likelihood. Different from distributions induced by normalizing flows in previous chapters, we do not require the induced distribution $p(\bm{x};\bm{\theta})$ by a generator in the generative model to be tractable here. In another word, the generator-induced distribution $p(\bm{x};\bm{\theta})$ is \textit{implicit} and likelihood-intractable. This relaxation indeed gives us larger freedom of defining the generators since we do not track the Jacobian computation anymore, and thus enlarges the hypothesis space of the model distribution $p(\bm{x};\bm{\theta})$. But it also brings the question of how to learn such a model without the tractable density function. Perhaps, a question before the \textit{how} is the \textit{why}, i.e., why do we need such kind of models?

The motivation for studying these generative models lies in the applications where sampling from implicit distributions is more important than likelihood tractability. Due to the curse of dimensionality, deep generative models that defined with neural network based generators and induce implicit distributions, demonstrate their advantage in the efficiency of modeling complex distributions and sampling in high-dimensional space, in comparison with explicit statistic models. This allows the representation and manipulation of implicitly high-dimensional distributions via the generative models. For instance, deep generative models have been used in image super-resolution, image-to-image translation, etc. Besides, implicit generative models can be naturally incorporated in reinforcement learning where typically an agent tries to learn to finish some tasks via interacting with a dynamic environment. For instance, in model-based methods, using a generative model to represent the environment and generating states for the agent to learn with, offers the way of training agents in simulations; A policy, essentially a conditional distribution of actions given an environment state, may be modeled by an implicit generative model since we mainly need to get the action in the present environment.

In this chapter, we introduce the learning of implicit probabilistic generative models by employing \textit{optimal transport} from transportation theory that does not need the tractability of likelihood. As long as we can simple for a generative model efficiently, learning of the generative model can be formed as the minimization of an optimal transport distance between the induced implicit distribution and an empirical distribution.

\section{Optimal Transport}\label{chpt8:sec:ot}
Optimal transport (OT) is a geometric tool for comparison of probability distributions, which has a rich history. Its work traces back as early as $18$th century in Monge's work \cite{monge1781memoire}. OT has been widely used in different fields, such as economic problems, logistics, production planning, etc. Along with its development in history, OT has been given different names, such as earth mover distance, Kantovich distance, and Wasserstein distance. Abstractly, OT distance measures the minimum transportation cost of from the mass of a distribution to another distribution \cite{villani2003topics}. Due to its close relation to the comparison of probability distributions and optimization, OT and its
variants have also been popularly used in machine learning and gained fast development in both theory and applications \cite{2013arXiv1310.4375C, 2013arXiv1306.0895C, 2016arXiv161006519S, ClaiciCS18}.

The high-level idea of this chapter is to use the OT distance to guide us to find the distribution $p(\bm{x}; \bm{\theta})$ of a generative model to approximate the true distribution $p^{\ast}(\bm{x})$. We begin with the introduction to OT itself and make connections to some popular generative models.

We denote the space $(\bm{\Xx},\|\cdot\|_2)$ be our working space, where $\bm{\Xx} = \prod_{x=1}^{N} \Xx_i$ as defined in Chapter~\ref{chapter2}. $\|\cdot\|_2$ is the Euclidean distance. Assume that $\bm{\Xx}_1$, $\bm{X}_2$ are finite sample subsets of $\bm{X}$. Let $p^{\ast}(\bm{x})$ be a distribution on $\bm{\Xx}_1$ and $p(\bm{x};\bm{\theta})$ be a distribution on $\bm{\Xx}_2$.
With mild abuse of notation, we denote the OT distance between $p^{\ast}(\bm{x})$ and $p(\bm{x}; \bm{\theta})$ by $T(p^{\ast},p)$. According to Kantovich's transportation problem\cite{villani2003topics}, we have $T(p^{\ast},p)$ given by
\begin{equation}\label{chpt8:eq:ot}
  T(p^{\ast}, p) = \min_{\pi\in\Pi(p^{\ast}, p)}\langle\pi,\bm{M}\rangle,
\end{equation}
where $\dotp{\cdot}{\cdot}$ stands for the inner product of
two matrices, and $\Pi(p^{\ast},p)$ is a set of joint distribution $\pi$ on
the sample sets $\bm{\Xx}_1\times\bm{\Xx}_2$ such that $\pi$ has
marginal distributions $p^{\ast}(\bm{x})$ and $p(\bm{x}; \bm{\theta})$. 
The cost matrix $\bm{M}$ has elements $M_{i,j} = d(\bm{x}_1^{i}, \bm{x}_2^{j})$ with $d(\cdot, \cdot)$ as a distance (Euclidean distance is used in our implementation), where $\bm{x}_1^{i}\in\bm{\Xx}_1$ and $\bm{x}_2^{j}\in \bm{\Xx}_2$ are $i$th and $j$th samples from $p^{\ast}(\bm{x})$ and $p(\bm{x}; \bm{\theta})$, respectively. 


\begin{remark}[Connection to the vanilla generative adversarial network]
  In the model learning of previous chapters, the information on how the model should adjust (change its parameter) is given by the likelihood value, stemming from KL divergence. In this chapter, we introduce the OT distance to offer such information in modeling learning. There are other ways to achieve this goal. A popular one is the generative adversarial network (GAN) \cite{NIPS2014_5423, 2017arXiv170100160G}, which casts the generative model learning into an adversarial game between the generator $\bm{g}$ and a discriminator $f$.
The {discriminator} $f$ gets input from either output of $\bm{g}$ or from
$p^{\ast}$. The generator $\bm{g}$ and discriminator ${f}$ play against each other
regarding a min-max objective:
\begin{equation}\label{chpt8:eq:eq-gan}
  \min_{\bm{g}} \max_f \EE_{p^{\ast}(\bm{x})}\left[ \log(f(\bm{x})) \right] + \EE_{p(\bm{z})}\left[ \log(1-f(\bm{g}(\bm{z})) \right],
\end{equation}
where $p(\bm{z})$ is a fixed distribution and easy to sample from. The model distribution $p(\bm{x}; \bm{\theta})$ is an implicit distribution and is induced by $\bm{x} = \bm{g}(\bm{z})$.
The probability model learning relies on if min-max game finds
its equilibrium where distribution $p(\bm{x}; \bm{\theta})$ is the same as $p^{\ast}(\bm{x})$ and
$d$ is optimal. In optimal case of the discriminator, objective of
\eqref{chpt8:eq:eq-gan} is to minimize the Jensen-Shannon divergence between $p(\bm{x}; \bm{\theta})$ and $p^{\ast}(\bm{x})$.
\end{remark}


\begin{remark}[Connection to Wassertein GAN]\label{chpt8:rmk:wgan}
  It is interesting to note that the OT minimization can be connected to GANs via its duality form. According to Kantorovich-Rubinstein duality \cite[section~2.4]{2018arXiv180300567P}\cite{villani2008optimal},
  \begin{equation}\label{chpt8:eq:kr-duality}
     T(p^{\ast},p) = \sup_{\|f\|_{L} \leq 1} \EE_{p^{\ast}(\bm{x})}\left[ f(\bm{x}) \right] -
  \EE_{p(\bm{x};\bm{\theta})}\left[ f(\bm{x}) \right],
\end{equation}
$\|f\|_L$ is a constraint for $f$ such that $f$ belong to the 1-Lipschitz function
family. That is to say, $f$ in \eqref{chpt8:eq:kr-duality} can be any function from the 1-Lipschitz function
family (which is different from the binary discriminator in \eqref{chpt8:eq:eq-gan}). Then minimization of the OT distance is formulated as a min-max problem \cite{2017arXiv170107875A}, termed as Wasserstein GAN (WGAN),
\begin{equation}\label{chpt8:eq:eq-wgan}
  \min_{\bm{g}} \max_{\|f\|_{L} \leq 1} \EE_{\bm{x} \sim p^{\ast}(\bm{x})}\left[ f(\bm{x}) \right] -
  \EE_{\bm{z}\sim p(\bm{z})}\left[ f(\bm{g}(\bm{z})) \right],
\end{equation}
where $p(\bm{x};\bm{\theta}) = p(\bm{g}(\bm{z}); \bm{\theta})$ with $\bm{\theta}$ parameterizing $\bm{g}$ and $\bm{z}\sim p(\bm{z})$. The  Lipschitz constraint enforcing is not trivial in general, especially for the deep generative models where $\bm{g}$ is implemented by neural networks. Nevertheless, the vanilla GAN in \eqref{chpt8:eq:eq-gan} is similar to the \eqref{chpt8:eq:eq-wgan} from minimization of OT distance.
\end{remark}

\begin{remark}[Connection to auto-encoders]
It is known that the variational lower bound of the likelihood function as in \eqref{chpt5:eq:lower-bound-F} is used to train variational auto-encoders, where an encoder and a decoder adjust their parameters to maximize the variational lower bound in the maximum likelihood principle. Since OT offers an alternative tool for modeling learning, it is also employed to train auto-encoders in likelihood-free settings, which shares similarities with the variational auto-encoders. See \cite{patrini2018sinkhornVAE, genevay2017gan,bousquet2017optimal, ambrogioni2018wasserstein} for detailed discussions.
\end{remark}

\section{EOT based Generative Models}

The optimal transport problem in \eqref{chpt8:eq:ot} is highly intractable. Therefore, approximation has to been used for efficient solutions. In Remark~\ref{chpt8:rmk:wgan}, Kantorovich-Rubinstein duality offers one way to solve the problem in its dual form if the Lipschitz constraint can be properly enforced. An alternative way can be using entropy regulation to seek for \textit{soft} solution.  In this section, we introduce the entropy-regularized optimal transport (EOT) cost and then propose generative models accordingly.

\subsection{Entropy-regularized OT} 

OT calculates the minimum cost of transporting distribution $p^{\ast}(\bm{x})$ to $p(\bm{x}; \bm{\theta})$. We use $W(p^{\ast},p)$ to denote entropy-regularized OT (EOT) cost as follows:
\begin{equation}\label{eq-entropic-wsd}
  W(p^{\ast},p)=\min_{ \pi \in \Pi(p^{\ast}, p)} \dotp{\pi}{\bm{M}} - \la H(\pi),
\end{equation}
where $H(\pi)$ is the entropy of joint distribution $\pi$, i.e. $H(\pi) = \sum_{i,j} -\pi_{i,j}
\log(\pi_{i,j})$ and $\la \in \RR_{+}$ is the regularization
parameter. Here $\RR_{+}$ denotes positive scalars. The entropy regularization in \eqref{eq-entropic-wsd}  translates 
to a requirement that the joint distribution $\pi$ has a high entropy. 
 The duality of EOT cost in \eqref{eq-entropic-wsd} is
\begin{equation}\label{eq-dual-wsd}
  W(p^{\ast}, p)  =  \max_{\bm{\al}, \bm{\be} \in \mathbb{R}^{N}} \bm{\al}^{\intercal}p^{\ast} + \bm{\beta}^{\intercal}p \! - \!
  \sum_{i,j} \lambda e^{ \frac{{\left( \al_i + \beta_j - M_{i,j} \right)}}{\la} },\vspace{-2pt}
\end{equation}
where $\bm{\alpha},\bm{\beta}$ are dual variables, $(\cdot)^{\intercal}$ means transpose, $\alpha_i$ is the $i$th element of $\bm{\alpha}$, and $\beta_i$ is the $j$th element of $\bm{\beta}$

One benefit of having the dual variables is that the optimal dual vector $\bm{\beta}^{\ast}$
of \eqref{eq-dual-wsd} is a subgradient of $W(p^{\ast},p)$ with respect to $p$. Thus, if we can efficiently obtain the optimal dual vector $\bm{\beta}^{\ast}$, we can learning our model distribution $p(\bm{x}; \bm{\theta})$ via the subgradient $\bm{\beta}^{\ast}$.

There is a computationally efficient algorithm called Sinkhorn
algorithm\cite{2013arXiv1306.0895C, 2013arXiv1310.4375C} to
solve~\eqref{eq-dual-wsd}, which alternatively scales the rows and columns of matrix $e^{-\frac{\bm{M}}{\lambda}}$. This alternative computation gives a pair of vectors $(\bm{u}, \bm{v}) \in \RR^N_{+} \times \RR^N_{+}$ that defines the optimal primary and dual variavles \cite[Proposition~2]{2013arXiv1310.4375C} 
\begin{align}\label{eq-dual-opt}
  \pi^{\ast} &=
               \mathrm{diag}(\bm{u}) \, e^{\frac{-\bm{M}}{\la}} \, \mathrm{diag}(\bm{v}),  \nonumber \\
  \bm{\beta}^{\ast} &= \frac{\log(\bm{u}^{\intercal})\mathds{1}_N}{N\la} \mathds{1}_N -\frac{\log(\bm{u})}{\la}.
\end{align}
where $\mathrm{diag}(\bm{u})$ is a matrix with diagonal entries from vector $\bm{u}$ and $\mathds{1}_N$ is a column vector with ones.


\subsection{EOT based Generative Models}

In this subsection, we propose two generative models.
We first develop an EOT based generative model handling signals/data directly. This models is referred to as EOT generative model (EOTGM). 
In our second model, we use a
representation mapping where EOT cost is used to optimize the
generative model and representation mapping jointly. The second model is referred as EOT based GAN (EOTGAN).
% In our EOT based GAN, we have a generator, but do not have an explicit discriminator. Instead we use a representation mapping where EOT cost is used to optimize the generator function and representation function jointly. We first develop an EOT based algorithm for generative model learning that can handle signals/data directly. Then we develop second algorithm that can handle representation features. The second algorithm is referred to as EOT based GAN (EOTGAN). 

\subsubsection{EOT based generative model (EOTGM)}\label{subsec-gmeot}
{Although the true probability distribution $p^{\ast}(\bm{x})$ is not available but its empirical distribution is available, i.e. the dataset. We carry on using the notation $p^{\ast}(\bm{x})$ as the target distribution. $p(\bm{x};\bm{\theta})$ is the probability distribution of our model induced by a generator $\bm{g}: \bm{\Zz} \rightarrow \bm{\Xx}$, where $\bm{\Zz}$ is the support set of latent distribution $p(\bm{z})$. The generator $\bm{g}$ is implemented by a neural network and maps latent signal $\bm{z} \in \bm{\Zz}$ to signal in $\bm{\Xx}$, i.e., $\bm{g}(\bm{z}) \in \bm{\Xx}$. The latent distribution is assumed to be known. The mapped signal $\bm{g}(\bm{z}) \sim p(\bm{x};\bm{\theta})$ since $\bm{g}$ induces the model distribution. Actually, the parameter of $p(\bm{x};\bm{\theta})$ is the parameter of the generator $\bm{g}$. Applying EOT cost to learn $p(\bm{x}; \bm{\theta})$ is equivalent to minimizing $W(p^{\ast}, p)$ w.r.t. the generator $g$}
\begin{equation}\label{eq-entropic-model}
  \underset{\bm{g}:\bm{\mathcal{Z}}\to \bm{\Xx}}{\argmin}\, W(p^{\ast}, p) = \underset{\bm{\theta}}{\argmin}\, W(p^{\ast}, p).
\end{equation}
Since $\bm{\beta}^{\ast}$ in \eqref{eq-dual-opt} is subgradient of $W(p^{\ast}, p)$ w.r.t. $p(\bm{x};\bm{\theta})$, we are able to
optimize the generator $\bm{g}$ such that the induced distribution $p(\bm{x};\bm{\theta})$ approximates $p^{\ast}(\bm{x})$, using gradient chain rule gives
\begin{equation}
  \nabla_{\bm{\theta}}W(p^{\ast}, p) = \left(\nabla_{\bm{\theta}}p\right)^{T} \bm{\beta}^{\ast}.
\end{equation}
Alternatively the optimization problem \eqref{eq-entropic-model} can be addressed by solving $\argmin_{g}\dotp{\pi^{\ast}}{\bm{M}} $ iteratively using auto-gradient functions in PyTorch\cite{pytorch} or
TensorFlow\cite{tensorflow}, where $\pi^{\ast}$ is primary optimal
variable to \eqref{eq-entropic-wsd} given by \eqref{eq-dual-opt}. We
propose Algorithm~\ref{algo-E-WL} to learn distribution $p^{\ast}$
via minimizing the EOT loss w.r.t parameter $\bm{\th}$ of generator function $\bm{g}$.
\begin{algorithm}[!t]
  \caption{EOT based Generative Model (EOTGM)}\label{algo-E-WL}
  \begin{algorithmic}[1]
    \STATE{$l$: the update rate at each iteration. $N$: the batch
      size. $\bm{\th}_{0}$: the initial parameter for $\bm{g}$.}
    \WHILE{$\bm{\theta}$ has not converged}
    \STATE Sample  a batch from a real dataset, $\left\{ \bm{x}^{i} \right\}_{i=1}^{N} \sim p^{\ast}$. 
    \STATE Sample $\left\{ \bm{z}^{i} \right\}_{i=1}^{N} \sim p(\bm{z})$, a batch of latent samples.
    \STATE Passing $\left\{ \bm{z}^{i} \right\}_{i=1}^{N}$ through the generator $\bm{g}$.
    \STATE Calculate the cost matrix $M$.
    \STATE $\pi^{\ast}$, $\bm{\beta}^{\ast} \gets$ primary and dual
    solutions of $W(\left\{ \bm{x}^{i} \right\}_{i=1}^{N}, \left\{
      \bm{g}(\bm{z}^{i})\right\}_{i=1}^{N})$ according \eqref{eq-dual-opt}.
    \STATE $\bm{\theta} \gets \bm{\theta} - l \left(\nabla_{\bm{\th}}{p}\right)^{\intercal}
    \bm{\beta}^{\ast}$. (Or back propagate using loss $\dotp{\pi^{\ast}}{\bm{M}}$)
    \ENDWHILE
  \end{algorithmic}
\end{algorithm}

\subsubsection{EOT based GAN (EOTGAN)}

In this section, we consider representation learning (feature
learning) with which the usage of EOT is more meaningful than that
directly in signal space.
It is well-known that Euclidean distance is not well suited to compare
two multimedia signals. For example, Euclidean distance between an
image and its rotated version can be large, but they are visually same. In Algorithm \ref{algo-E-WL} we construct cost matrix $\bm{M}$ in EOT using 
Euclidean distance between real signals and generated signals. Our new proposal is to transform signal through a representation mapping 
$\bm{f}: \bm{\Xx} \rightarrow \bm{\Mm}$, $\bm{\mathcal{M}}\subset\mathbb{R}^{m}$ and we compare features in the representation space via EOT. We assume that Euclidean distance
between features in the representation space is more semantically
meaningful. An element of the cost matrix $\bm{M_f}$ in representation domain (feature domain) is
\begin{equation}\label{def-similarity}
  d_{\bm{f}}(\bm{x}, \bm{y}) = \|\bm{f}(\bm{x})-\bm{f}(\bm{y})\|_{2}.
\end{equation}

Our new objective is joint learning of generator $\bm{g}$ and representation $\bm{f}$. A natural question is how to construct $\bm{f}$ function? {Inspired by the triplet loss in \cite{7298682} aiming at larger distance between distinct classes than in-class distance, we may consider two virtual classes labeled by $p^{\ast}(\bm{x})$ and $p(\bm{x};\bm{\theta})$.
% Then samples from the same class share more similarity and thus should have smaller distance in $\Mm$.
This means that the representation function $\bm{f}$ should have the algebraic property: $ d_{\bm{f}}(\bm{x}_1, \tilde{\bm{x}}_1) + \gamma \leq d_{\bm{f}}(\bm{x}_1, \bm{x}_2) $ for $\gamma > 0$, where $\bm{x}_1, \tilde{\bm{x}}_1 \in \bm{\Xx}_1$ are two samples from distribution $p^{\ast}$ and $\bm{x}_2$ is a generated signal from distribution $p(\bm{x};\bm{\theta})$, i.e., an output of $\bm{g}$. Meanwhile, $\bm{g}$ tries to mitigate this distinction.}

Following the above idea, let us denote the mapped distribution via $\bm{f}$ by $p^{\ast}_{\bm{f}}$ and $p_{\bm{f}}$, respectively. Let $\bm{M}_{\bm{f}}$ be the cost matrix in representation domain and its elements $M_{\bm{f},{i,j}} = d_{\bm{f}}(\bm{x}^{i}_1, \bm{x}^{j}_2)$, $\bm{x}^{i} \sim p^{\ast}(\bm{x}), \bm{x}^{j}\sim p(\bm{x}; \bm{\theta})$. Then we learn $\bm{f}$ and $\bm{g}$ using alternative optimization, as follows. 
\begin{itemize}
\item Learning of representation $f$ is minimizing EOT cost
  \begin{equation}\label{eq-sim-in}
    W(p^{\ast}_{\bm{f}},p^{\ast}_{\bm{f}}) = \min_{\widetilde{\pi} \in \Pi(p^{\ast}_{\bm{f}}, p^{\ast}_{\bm{f}}) } \dotp{\widetilde{\pi}}{\widetilde{\bm{M}}_{\bm{f}}} - \la H(\widetilde{\pi}),
  \end{equation}
  where $\widetilde{M}_{\bm{f}, {i,j}} = d_{\bm{f}}(\bm{x}^{i}, \tilde{\bm{x}}^{j})$, $\bm{x}^{i}, \tilde{\bm{x}}^{j} \sim p^{\ast}(\bm{x})$, and maximizing EOT cost
  \begin{equation}\label{eq-sim-ex}
    W(p^{\ast}_{\bm{f}}, p_{\bm{f}}) = \min_{{\pi} \in \Pi(p^{\ast}_{\bm{f}}, p_{\bm{f}}) } \dotp{{\pi}}{\bm{M_f}} - \la H({\pi}).\vspace{-8pt}
  \end{equation}
\item Learning of generator $\bm{g}$ is minimizing EOT cost
  \begin{equation}
    W(p^{\ast}_{\bm{f}}, p_{\bm{f}}).
  \end{equation}
\end{itemize} 
Both $W(p^{\ast}_{\bm{f}}, p^{\ast}_{\bm{f}})$and $W(p^{\ast}_{\bm{f}}, p_{\bm{f}})$ have lower bounds, but no upper bounds. 
We combine the step 1 in above using a hinge loss and define the following costs.
\begin{equation}
  \begin{array}{rl}
    &\Ll_{\bm{f}}(p^{\ast}_{\bm{f}}, p_{\bm{f}}) := \max\left(0, W(p^{\ast}_{\bm{f}}, p^{\ast}_{\bm{f}})- W(p^{\ast}_{\bm{f}}, p_{\bm{f}}) \hspace{-2pt}+\hspace{-2pt} \gamma \right),\\
    &\Ll_{\bm{g}}(p^{\ast}_{\bm{f}}, p_{\bm{f}}) := W(p^{\ast}_{\bm{f}}, p_{\bm{f}}),
  \end{array}
\end{equation}
where $\gamma>0$. Hinge loss helps to balance the adversarial training of the $\bm{f}$ and $\bm{g}$.
Note the our hinge adversarial loss shares similarity only in form to the self-attention GAN\cite{2018arXiv180508318Z} and geometric GAN\cite{2017arXiv170502894L} but is motivated differently and defined in different metric. We used neural networks for constructing $\bm{f}$ and $\bm{g}$ functions. Let us assume that the parameters of $\bm{f}$ and $\bm{g}$ are $\bm{\omega}$ and $\bm{\theta}$, respectively. Then the adversarial training between representation $\bm{f}$ and
generator $\bm{g}$ is the following alternative optimization problem
\begin{equation}
  \begin{array}{rl}
    &\min_{\bm{f}} \Ll_{\bm{f}}(p^{\ast}_{\bm{f}}, p_{\bm{f}}) = \min_{\bm{\omega}} \Ll_{\bm{f}}(p^{\ast}_{\bm{f}}, p_{\bm{f}}), \\ 
    &\min_{\bm{g}}\Ll_{\bm{g}}(p^{\ast}_{\bm{f}}, p_{\bm{f}}) = \min_{\bm{\th}}\Ll_{\bm{g}}(p^{\ast}_{\bm{f}}, p_{\bm{f}}).
  \end{array}\vspace{-2pt}
\end{equation}
The algorithm steps of EOTGAN are shown in Algorithm \ref{algo-eWGAN}.

\begin{algorithm}[!t]
  \caption{EOT based GAN (EOTGAN)}\label{algo-eWGAN}
  \begin{algorithmic}[1]
    \STATE {$l$: the update rate at each iteration. $N$: the batch
      size. $\bm{\th}_{0}$, $\bm{\omega}_0$: the initial parameters for $\bm{g}$ and $\bm{f}$.}
    \WHILE{$\bm{\theta}$ has not converged}
    \STATE Sample two batches of data $ \left\{ \bm{x}^{i}
    \right\}_{i=1}^{N}, \left\{ \tilde{\bm{x}}^{i} \right\}_{i=1}^{N}  $,
    and latent samples $\left\{ \bm{z}^{i} \right\}_{i=1}^{N} $,
    $\bm{x}^{i},\tilde{\bm{x}}^{i} \sim p^{\ast}(\bm{x}), \bm{z}\sim p(\bm{z})$.
    \STATE Passing
    $\left\{ \bm{z}^{i} \right\}_{i=1}^{N}$ through $\bm{g}$.
    \STATE $\widetilde{\pi}^{\ast} \gets$ solving $ W_{\bm{f}}\left( \left\{ \bm{f}(\bm{x}^{i})
      \right\}_{i=1}^{N} , \left\{ \bm{f}(\tilde{\bm{x}}^{i})
      \right\}_{i=1}^{N}\right)$
    \STATE ${\pi}^{\ast} \gets$ sovling $W_{\bm{f}}\left( \left\{ \bm{f}(\bm{x}^{i})
      \right\}_{i=1}^{N}, \left\{ \bm{f}(\bm{g}(\bm{z}^i))
      \right\}_{i=1}^{N} \right)$
    \STATE $\partial{\bm{f}} \gets \nabla_{\bm{\om}} \max\left(0,  \dotp{\widetilde{\pi}^{\ast}}{\widetilde{\bm{M}}}-\dotp{{\pi^{\ast}}}{\bm{M}}+ \gamma\right)$
    \STATE $\bm{\om} \gets \bm{\om} - l \cdot \partial{\bm{f}}$
    \STATE Sample $\left\{ \bm{z}^{i} \right\}_{i=1}^{N}$
    and get $ \left\{ \bm{g}(\bm{z}^i)\right\}_{i=1}^{N}$ via $\bm{g}$.
    \STATE $\pi^{\ast} \gets$ sovling $W_{\bm{f}}\left( \left\{ \bm{f}(\bm{x}^{i})
      \right\}_{i=1}^{N}, \left\{ \bm{f}(\bm{g}(\bm{z}^{i}))
      \right\}_{i=1}^{N} \right)$
    \STATE $\partial{\bm{g}} \gets \nabla_{\bm{\th}} \dotp{\pi^{\ast}}{\bm{M}}$
    \STATE $\bm{\th} \gets \bm{\th} -l \cdot \partial{\bm{g}}$
    \ENDWHILE
  \end{algorithmic}
\end{algorithm}

\subsubsection{Advantage of EOT against OT}

Usage of entropy regularization in EOT avoids the need for Kantorovich-Rubinstein duality
of OT, thus is free from Lipschitz constraint. In literature, several methods endeavor to
satisfy Lipschitz constraint, for example, projecting neural network
parameters into a space fulfilling Lipschitz constraint via weight
clipping \cite{2017arXiv170107875A}, spectrum
normalization \cite{2018arXiv180205957M}, or adding gradient 
penalty into GAN's cost function \cite{2017arXiv170400028G}. Projecting
approaches bring the problem of neural network capacity underuse and
limit its ability to learn complex mapping. Gradient penalty approach takes
gradients of each layer's weight parameters of a neural network into
GAN's cost, thus computation complexity grows fast as the neural
network goes deeper. 
EOT avoids the above-mentioned problems and also has the benefit of lower
computation complexity. With entropy-regularization and Sinkhorn
algorithm, the computation complexity scales as $\mathcal{O}(N^2)$ \cite{2013arXiv1306.0895C}. 
On the other hand, solving OT cost using interior-point methods has a computational requirement as 
$\mathcal{O}(N^3\log{N})$. 


\section{Experimental Results}
We perform experiments to verify our arguments on loss choice and algorithms. We evaluate our
generative models on a toy synthetic dataset of Gaussian-mixture distribution and real image dataset MNIST.

\subsection{Evaluation Metrics}\label{subsec-metric}
Similar to Section~\ref{chpt6:sec:eval-metrics}, the Inception Score (IS) \cite{NIPS2016_6125} and Frechet Inception Distance (FID)\cite{2017arXiv170608500H} are used as metrics as evaluation of our models. IS is defined as $
  IS(q) = \exp\bigl[ \EE_{\bm{x} \sim q} \mathrm{KL}\bigl( p(c | \bm{x})\| p(c) \bigr)  \bigr]$,
where $\bm{x}\sim q$ indicates synthetic sample from distribution $q$ under testing, $p(c|\bm{x})$ is the conditional distribution of class $c$, and $p(c) = \int_{\bm{x}}p(c|\bm{x})q(\bm{x}) \,d \bm{x} $ is the marginal class distribution computed with $q$. Large IS score means generated samples contain clear objects. Generative models with high IS can output high diversity of samples. Apart from KL-based metric, an alternative common metric is Frechet Inception Distance (FID)\cite{2017arXiv170608500H}. FID measures the OT distance of two probability distribution by assuming the two distributions are Gaussian, in which case, the closed-form solution is available. Smaller FID means the generated samples are more similar to empirical samples. In short, high IS and low FID are better.


\subsection{Evauation of EOTGM using toy dataset}\label{subsec-mg}

{We firstly evaluate our proposed EOTGM on a toy dataset sampled from a known probability distribution: two-dimensional four-mixture Gaussian. This mixture Gaussian is our target distribution to learn, i.e., $p^{\ast}(\bm{x})$. The generator $\bm{g}$ uses a neural network with structure: Input $\rightarrow$ Dense $256$ $\rightarrow$ ReLU $\rightarrow$ Dense $256$ $\rightarrow$ ReLU $\rightarrow$ Dense $256$ $\rightarrow$ ReLU$\rightarrow$Dense $2$. The parameter $\bm{\theta} $ of $\bm{g}$ here is the set of parameters of this neural network. Latent distribution $p(\bm{z})$ used here is standard Gaussian: $\mathsf{N}\left(\bigl(\begin{smallmatrix}& 0\\
    &0\end{smallmatrix}\bigr) ,\bigl( \begin{smallmatrix}1 & 0\\ 0 &
    1\end{smallmatrix}  \bigr)\right)$.
The toy dataset is used by Algorithm~\autoref{algo-E-WL} (EOTGM) to train $\bm{g}$. In Figure~\ref{fig-toy}, we plot the empirical samples from our toy dataset and the synthetic samples generated by $\bm{g}$. The corresponding contours are also plotted. It shows that the induced distribution by $\bm{g}$ approaches the mixture Gaussian distribution well without missing any mode.}          

\begin{figure*}[!t]
  \captionsetup[subfigure]{justification=centering}
  \centering
  \begin{subfigure}[b]{0.44\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{images/toy/gauss4/frame8.jpg}
    \caption{}
    \label{fig-toy}
  \end{subfigure}
  \centering
  \begin{subfigure}[b]{0.44\textwidth}
    \centering
    \includegraphics[width=0.85\linewidth]{images/mnist/fake/eot_18500_crop.png}\vspace{5pt}
    \caption{}
    \label{fig-fake-wgan}
  \end{subfigure}
  \caption{Demonstration of sampling. (a) Toy distribution learning ($4$-mixture Gaussians) using EOTGM. Real samples (red '+') and contour
    (red curve), versus generated samples (blue 'o') and contour (blue curve) by $\bm{g}$. (b) Generated samples by EOTGAN for MNIST dataset.}
    \vspace{0.4cm}
\end{figure*}


\subsection{Evaluation of generative models using MNIST}
In this section, we evaluate both the generative models using MNIST dataset.
The representation mapping $\bm{f}$ in EOTGAN adapts two converlutional layers
appended with fully connected layers\footnote{$32$ Conv2d $5 \times5$
  $\rightarrow$ PReLU $\rightarrow$ MaxPool $2\times2$ $\rightarrow$
  $64$ Conv2d $5\times5$ $\rightarrow$ PReLU $\rightarrow$ MaxPool
  $2\times2$ $\rightarrow$ Dense $256$ $\rightarrow$ PReLU
  $\rightarrow$ Dense $256$ $\rightarrow$ PReLU $\rightarrow$ Dense
  $2$}
similar to \cite{1467314}\cite{1640964}. Generator $\bm{g}$ uses the same
setting as that of DCGAN and WGAN. Noise $p(\bm{z})$ is $100$-dimensional Guassian.
We report IS and FID scores of EOTGAN in comparison with DCGAN and WGAN. Since EOTGAN is trained with representation mapping $\bm{f}$ that
acts as feature mapping, it is not fair to use this representation mapping $\bm{f}$ to do the
evaluation and make comparisons since it would give EOTGAN
advantages. Similar to \cite{2018arXiv180607755X}, we train a 34-layer
ResNet on MNIST to perform feature extraction for metric measurements
of IS and FID. In addition, we put EOTGM (Algorithm~\autoref{algo-E-WL}) in comparison as well.

\begin{figure*}[!t]
  \centering
  \begin{subfigure}[b]{0.44\textwidth}
    \centering
    \includegraphics[width=1.1\linewidth]{images/mnist/tra_score/IS_29.pdf}\vspace{-3pt}
    \caption{}
    \label{fig-tra-is}
  \end{subfigure}
  \begin{subfigure}[b]{0.44\textwidth}
    \centering
    \includegraphics[width=1.1\linewidth]{images/mnist/tra_score/FID_29.pdf}\vspace{-3pt}
    \caption{}
    \label{fig-tra-fid}
  \end{subfigure}
  \caption{Comparison of IS and FID (on MNIST) versus mixing ratio $r$. (For each model at a certain mixture ratio, $5$ experiments are independently performed. Each solid curve with markers plots the mean of $5$ experiments with shaded areas denoting the range of corresponding results.}
  \label{fig-tra-score}
  \vspace{0.4cm}
\end{figure*}


Data for evaluations is constructed by mixing empirical samples and
synthetic samples generated by $\bm{g}$. We draw the set $\mathcal{S}_{\mathrm{em}}$ of 2000 empirical samples from MNIST dataset. To generate a set $\mathcal{S}_{\mathrm{syn}}$ of synthetic
samples we draw $2000r$ samples from the generator network $\bm{g}$ where
$r\in[0,1]$ while rest $2000(1-r)$ are sampled directly from
MNIST. A demonstration of generated samples from EOTGAN is shown in Figure~\ref{fig-fake-wgan}.
All the following experiments are on
$\mathcal{S}_{\mathrm{em}}$ and $\mathcal{S}_{\mathrm{syn}}$. 
The way of mixing empirical data and generated data helps
us to identify if a metric is intuitively helpful. Among the chosen metrics IS at $r=0$ serves as an upper bound for the test while the FID at $r=0$ serves as a lower bound for the corresponding tests. 

IS measures how certain a classifier assigns a class label to a given
generated sample. The larger IS is, the better the generative model
is. We plot IS versus $r$ for different models in
Figure~\ref{fig-tra-is}. IS scores of all four tested models drop with
increasing portion of synthetic samples in
$\mathcal{S}_{\mathrm{syn}}$, which is consistent with intuition. IS
of EOTGAN drops at the slowest rate among the four models as more
synthetic samples, for larger $r$, are mixed into test data. It shows
that EOTGAN outperforms WGAN and DCGAN in this test. EOTGM is found to provide the lowest IS.
This may be attributed to the setup that 
EOT optimization with cost measured by Euclidean distance of signals
fails to capture semantic similarity.

In Figure~\ref{fig-tra-fid} the performances of different models are
compared using the FID metric. The smaller the FID of a generative model is, the more similar the
generated samples are to the empirical samples. EOTGAN is the least affected model among all the four, as the ratio $r$ increases, i.e. the generated samples by EOTGAN are more similar to the empirical
ones in the feature space regarding FID. FID of WGAN is larger than that of EOTGAN. As
more generated samples are mixed the FIDs of DCGAN and EOTGM grow
even faster, which means the samples generated by these two models are
less similar to the empirical samples.

\section{Summary}
In this chapter, we took a different path from previous chapters and considered the learning of an implement probabilistic generative model. An implicit model offers larger freedom in choosing a generator that induces the model distribution, but brings the challenges of model learning due to the loss of tractability of likelihood. Thus optimal transport was introduced and employed to formulate the cost. Optimal transport distance used in learning the model also has interesting connections with generative adversarial networks and auto-encoder-like models. With an entropy-regularization, optimal transport cost can be computed more efficiently. Two learning algorithms based on the entropy-regularized optimal transport cost were then discussed and demonstrated. 

Different from graphical models discussed in previous chapters, an implicit model has its advantage of flexible generator choice and efficient sampling (or generating sampling), sampling from which is usually a feed-forward pass of the generator once learned. Thus, an implicit model has its application domains where the above-mentioned properties are merited more than explicit form and likelihood tractability of the model's distribution. 


\section{Related Literature}

Deep generative models that use the non-linear mappings of neural networks have received a lot of attentions as a branch of graphical models. The modeling capacity of deep generative models is impressive in high-dimensional space (e.g., image super-resolution \cite{ledig2017photo}, image de-noising \cite{CreswellB2017denoise, dilokthanakulmg2016gausian-vae}, image segmentation \cite{marvin2018crf}, speech generating \cite{ryan2018waveglow} etc.), but the learning of these models is not trivial. The learning difficulty lies in modeling challenge within the high-dimensionality itself or our trading more complex generators (for expressivity) with distribution tractability. Thus, works in pursuit of better understanding of deep generative models have been carried out, and also different model learning methods or costs have been proposed in relieving the learning difficulty.

One leading family of deep generative models are the variational auto-encoders \cite{kingma2019vae}\cite{}, which use special choices of parameterization (e.g., \cite{DBLP:journals/corr/KingmaW13}) or generator structures to preserve the likelihood computation (e.g., the variational inference by normalizing flows, see \ref{rezende2015variational, kingma2016IVF, TomczakW16vae-flow}). A pre-defined inference task can still be carried out and sampling is also efficient in variational auto-encoders. In learning, these models are sensitive to the Monte Carlo estimation of the gradients, \textcolor{blue}{add variance issue analysis}. If the restriction on special p HIM Hierarchical Implicit Models and Likelihood-Free Variational Inference

A prominent example is generative adversarial network (GAN) \cite{NIPS2014_5423}, which comes with an implicit underlining distribution of the model.
In the vanilla GAN, a generator produces synthetic samples and a discriminator endeavors to distinguish between real samples and synthetic samples. Generators and discriminators are both implemented using (deep) neural networks, and play an adversary game against each other using a \textit{min-max} optimization to learn parameters of neural networks. More intuitively speaking, for the generator, the game turns out be minimizing Jensen-Shannon divergence between target distribution (the induced distribution by the generator) when the discriminator is optimal.
This interesting game of casting generative model learning into a adversary game triggered more research in generator and learning method design
\cite{2015arXiv151106434R, 2015arXiv151106434R, 2018arXiv180205957M, 2018arXiv180508318Z, 2018arXiv180600880K, bang2018icml, DBLP:journals/corr/GhoshKNTD17, hoang2018mgan}, and also explanation exploration \cite{2017arXiv170104862A, 2017arXiv170107875A, 2018arXiv180607755X, 2017arXiv170104722M, NIPS2016_6399, li2018graphical}.

Apart from the reasons mentioned above on learning difficulty of deep generative models, new issues raise from the deep generative models themselves. Let us take the learning of the vanilla GAN as an exmaple, which has limitation due to the Jensen-Shannon divergence at optimal case of its discriminator \cite{2017arXiv170104862A}. Firstly, the limitation is that the gradient of the Jensen-Shannon divergence cost w.r.t. the generator vanishes as discriminator approaches optimal, which stops generator from further learning. Secondly, the limitation is due to high sensitivity of Jensen-Shannon divergence to slight perturbations, which can be large between a distribution $p(\bm{x})$ and a distribution $p(\bm{x}+\bm{\epsilon})$ where $\epsilon$ is perturbation. These difficulties were relieved in the followed up model Wasserstein GAN (WGAN) \cite{2017arXiv170107875A}, WGAN-GP \textcolor{blue}{more references}. Wasserstein distance stems from optimal transport (OT) distance as explained in Section~\ref{chpt8:sec:ot}. The WGAN formulation does not require an explicit discriminator and it does not has the vanishing-gradient issue due to the cost function. Wasserstein distance or OT is upper bounded by the standard deviation of perturbation $\epsilon$, addressing the second limitation. The story did not end there since duality of Wasserstein distance brought new challenges as briefed in Remark~\ref{chpt8:rmk:wgan}. \textcolor{blue}{reference following up WGAN, gradient penalty\cite{2017arXiv170400028G} and spectrum normalization\cite{2018arXiv180205957M}.
}



OT based cost in WGAN brings a strict constraint to follow in its optimization.
Kantorovich-Rubinstein duality used in WGAN requires a supremum over
infinite set of all Lipschitz functions with Lipschitz constant equal
to one. Various sub-optimal techniques are proposed to enforce the
Lipschitz property. An example is weight clipping
\cite{2017arXiv170107875A} where neural network parameters (weights)
are updated first without Lipschitz constraint and then projected to
satisfy Lipschitz constraint in each iteration. Other approaches are
gradient penalty\cite{2017arXiv170400028G} and spectrum normalization\cite{2018arXiv180205957M}.

In this article, our main contribution is to explore use of Entropy-regularized OT (EOT) cost for generative models. 
The EOT was studied earlier for efficient comparison between two probability distributions \cite{2013arXiv1306.0895C}. 
The major advantage of EOT is
that corresponding dual problem is free from Lipschitz constraint. Use of EOT improves
analytical tractability allows us to develop two generative models. Our
first model considers EOT cost directly on distribution of signals (in our case,
on the image pixels). This model uses an one-shot optimization problem, \emph{i.e.} no use of adversarial game in iterations. The second model
considers EOT on feature distribution instead of direct signal
distribution. In this case we also need to learn a representation
mapping, which is implemented as a neural network. 
This requires alternative optimization of representation mapping and
generator. In addition to the above advantage, duality of EOT can be
effectively solved and straightforwardly extended to parallel computation. 





%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../main"
%%% End:
